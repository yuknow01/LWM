{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeacb7fd",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ec90d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "#LWM을 하기위한 라이브러리 가져오기\n",
    "import DeepMIMOv3\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "plt . rcParams [ 'figure.figsize' ]  =  [ 12 ,  8 ]  # 기본 플롯 크기 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e17e0b",
   "metadata": {},
   "source": [
    "## GPU설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47baa533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# GPU 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a6fcece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n",
      "90100\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)                   # 설치된 CUDA 버전 (예: '11.7')\n",
    "print(torch.backends.cudnn.version())       # cuDNN 버전 (예: 8200)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())  # True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9e064d",
   "metadata": {},
   "source": [
    "# DeepMIMOv3 다운"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23623e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install DeepMIMOv3 umap-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19540670",
   "metadata": {},
   "source": [
    "## 파라미터 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce5ac681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OFDM': {'RX_filter': 0,\n",
      "          'bandwidth': 0.05,\n",
      "          'selected_subcarriers': array([0]),\n",
      "          'subcarriers': 512},\n",
      " 'OFDM_channels': 1,\n",
      " 'active_BS': array([1]),\n",
      " 'bs_antenna': {'FoV': array([360, 180]),\n",
      "                'radiation_pattern': 'isotropic',\n",
      "                'rotation': array([0, 0, 0]),\n",
      "                'shape': array([8, 4]),\n",
      "                'spacing': 0.5},\n",
      " 'dataset_folder': './Raytracing_scenarios',\n",
      " 'dynamic_scenario_scenes': array([1]),\n",
      " 'enable_BS2BS': 1,\n",
      " 'enable_doppler': 0,\n",
      " 'enable_dual_polar': 0,\n",
      " 'num_paths': 5,\n",
      " 'scenario': 'O1_60',\n",
      " 'ue_antenna': {'FoV': array([360, 180]),\n",
      "                'radiation_pattern': 'isotropic',\n",
      "                'rotation': array([0, 0, 0]),\n",
      "                'shape': array([4, 2]),\n",
      "                'spacing': 0.5},\n",
      " 'user_rows': array([1]),\n",
      " 'user_subsampling': 1}\n"
     ]
    }
   ],
   "source": [
    "## Load and print the default parameters\n",
    "# bandwith: 0.05GHz(50MHz 대역폭 사용)\n",
    "parameters = DeepMIMOv3.default_params()\n",
    "pprint(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f0642c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change parameters for the setup\n",
    "# Scenario O1_60 extracted at the dataset_folder\n",
    "#LWM 동적 시나리오 불러오기\n",
    "#자신의 LWM 파일 위치 경로 작성\n",
    "# parameters['dataset_folder'] = r'/content/drive/MyDrive/Colab Notebooks/LWM'\n",
    "scene = 15 # 장면 수\n",
    "parameters['dataset_folder'] = r'C:\\Users\\dlghd\\졸업프로젝트\\LWM'\n",
    "\n",
    "# scnario = 02_dyn_3p5 <- 다운받은 파일(동적시나리오)\n",
    "parameters['scenario'] = 'O2_dyn_3p5'\n",
    "parameters['dynamic_scenario_scenes'] = np.arange(scene) #scene 0~9\n",
    "\n",
    "# 각 사용자-기지국 채널에 대해 최대 10개 멀티패스 경로 사용\n",
    "parameters['num_paths'] = 10\n",
    "\n",
    "# User rows 1-100\n",
    "parameters['user_rows'] = np.arange(100)\n",
    "# User 축소하기\n",
    "parameters['user_subsampling'] = 0.01\n",
    "\n",
    "# Activate only the first basestation\n",
    "parameters['active_BS'] = np.array([1])\n",
    "\n",
    "parameters['activate_OFDM'] = 1\n",
    "\n",
    "parameters['OFDM']['bandwidth'] = 0.05 # 50 MHz\n",
    "parameters['OFDM']['subcarriers'] = 512 # OFDM with 512 subcarriers\n",
    "parameters['OFDM']['selected_subcarriers'] = np.arange(0, 64, 1)\n",
    "#parameters['OFDM']['subcarriers_limit'] = 64 # Keep only first 64 subcarriers\n",
    "\n",
    "parameters['ue_antenna']['shape'] = np.array([1, 1]) # Single antenna\n",
    "parameters['bs_antenna']['shape'] = np.array([1, 32]) # ULA of 32 elements\n",
    "#parameters['bs_antenna']['rotation'] = np.array([0, 30, 90]) # ULA of 32 elements\n",
    "#parameters['ue_antenna']['rotation'] = np.array([[0, 30], [30, 60], [60, 90]]) # ULA of 32 elements\n",
    "#parameters['ue_antenna']['radiation_pattern'] = 'isotropic'\n",
    "#parameters['bs_antenna']['radiation_pattern'] = 'halfwave-dipole'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30aa4ae2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_folder': 'C:\\\\Users\\\\dlghd\\\\졸업프로젝트\\\\LWM', 'scenario': 'O2_dyn_3p5', 'dynamic_scenario_scenes': array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]), 'num_paths': 10, 'active_BS': array([1]), 'user_rows': array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
      "       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]), 'user_subsampling': 0.01, 'bs_antenna': {'shape': array([ 1, 32]), 'spacing': 0.5, 'rotation': array([0, 0, 0]), 'FoV': array([360, 180]), 'radiation_pattern': 'isotropic'}, 'ue_antenna': {'shape': array([1, 1]), 'spacing': 0.5, 'rotation': array([0, 0, 0]), 'FoV': array([360, 180]), 'radiation_pattern': 'isotropic'}, 'enable_doppler': 0, 'enable_dual_polar': 0, 'enable_BS2BS': 1, 'OFDM_channels': 1, 'OFDM': {'subcarriers': 512, 'selected_subcarriers': array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]), 'bandwidth': 0.05, 'RX_filter': 0}, 'activate_OFDM': 1}\n"
     ]
    }
   ],
   "source": [
    "print(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3326b22d",
   "metadata": {},
   "source": [
    "## dataset 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfe26bf-722b-4800-b03f-42bbf9d3cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset 구축 (chunked on‑the‑fly generation)\n",
    "import time, gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 0~999 씬 인덱스, 한 번에 50개씩 처리\n",
    "scene_indices = np.arange(scene)\n",
    "chunk_size   = 5\n",
    "all_data     = []\n",
    "\n",
    "# 씬 묶음(chunk)마다 generate_data 호출\n",
    "for i in tqdm(range(0, len(scene_indices), chunk_size)):\n",
    "    chunk = scene_indices[i : i+chunk_size].tolist()\n",
    "    parameters['dynamic_scenario_scenes'] = chunk\n",
    "\n",
    "    start = time.time()\n",
    "    data_chunk = DeepMIMOv3.generate_data(parameters)\n",
    "    print(f\"Scenes {chunk[0]}–{chunk[-1]} generation time: {time.time() - start:.2f}s\")\n",
    "\n",
    "    # 바로 all_data에 합치거나, 디스크에 저장해도 OK\n",
    "    all_data.extend(data_chunk)\n",
    "\n",
    "    # 메모리 해제\n",
    "    del data_chunk\n",
    "    gc.collect()\n",
    "\n",
    "# 마지막에 하나의 리스트로 합친 데이터셋\n",
    "dataset = all_data\n",
    "\n",
    "\n",
    "print(parameters['user_rows'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ee486e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_folder': 'C:\\\\Users\\\\dlghd\\\\졸업프로젝트\\\\LWM', 'scenario': 'O2_dyn_3p5', 'dynamic_scenario_scenes': [10, 11, 12, 13, 14], 'num_paths': 10, 'active_BS': array([1]), 'user_rows': array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
      "       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]), 'user_subsampling': 0.01, 'bs_antenna': {'shape': array([ 1, 32]), 'spacing': 0.5, 'rotation': array([0, 0, 0]), 'FoV': array([360, 180]), 'radiation_pattern': 'isotropic'}, 'ue_antenna': {'shape': array([1, 1]), 'spacing': 0.5, 'rotation': array([0, 0, 0]), 'FoV': array([360, 180]), 'radiation_pattern': 'isotropic'}, 'enable_doppler': 0, 'enable_dual_polar': 0, 'enable_BS2BS': 1, 'OFDM_channels': 1, 'OFDM': {'subcarriers': 512, 'selected_subcarriers': array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]), 'bandwidth': 0.05, 'RX_filter': 0}, 'activate_OFDM': 1}\n"
     ]
    }
   ],
   "source": [
    "print(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4e72d4",
   "metadata": {},
   "source": [
    "# 사용자 접근 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57828d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['paths', 'LoS', 'location', 'distance', 'pathloss', 'channel'])\n"
     ]
    }
   ],
   "source": [
    "user_data = dataset[0][0]['user']\n",
    "print(user_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5a3b0c",
   "metadata": {},
   "source": [
    "# 사용자 채널 정보 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23b2884b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(727, 1, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "# subcarries = 나눈 각각의 주파수 채널\n",
    "# Channel = H <- 채널 벡터\n",
    "# 채널 형태\n",
    "# (user, UE antenna, Bs antenna, subcarrier)\n",
    "channel = dataset[0][0]['user']['channel']\n",
    "print(channel.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf443dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 8.57045598e-06+5.5781261e-06j  8.89099283e-06+5.0515800e-06j\n",
      "    9.17921989e-06+4.5066768e-06j ... -1.02173499e-05-4.1711201e-07j\n",
      "   -1.02239183e-05+1.9928974e-07j -1.01933329e-05+8.1496728e-07j]\n",
      "  [ 1.02161603e-05+4.4529790e-07j  1.02244285e-05-1.7108337e-07j\n",
      "    1.01955429e-05-7.8684292e-07j ... -9.00999748e-06+4.8361312e-06j\n",
      "   -8.70222630e-06+5.3702393e-06j -8.36283198e-06+5.8848323e-06j]\n",
      "  [ 9.02330430e-06-4.8112561e-06j  8.71700831e-06-5.3462113e-06j\n",
      "    8.37903553e-06-5.8617388e-06j ... -5.29921817e-06+8.7456565e-06j\n",
      "   -4.76262221e-06+9.0490685e-06j -4.20871947e-06+9.3195977e-06j]\n",
      "  ...\n",
      "  [-7.00710962e-06-7.4477266e-06j -7.44313866e-06-7.0119827e-06j\n",
      "   -7.85211978e-06-6.5507575e-06j ...  9.82847632e-06+2.8229874e-06j\n",
      "    9.98071755e-06+2.2256456e-06j  1.00966881e-05+1.6202162e-06j]\n",
      "  [-9.82065103e-06-2.8500913e-06j -9.97453935e-06-2.2531719e-06j\n",
      "   -1.00921798e-05-1.6480645e-06j ...  9.89848286e-06-2.5667589e-06j\n",
      "    9.72583803e-06-3.1585257e-06j  9.51785023e-06-3.7388147e-06j]\n",
      "  [-9.90552599e-06+2.5394413e-06j -9.73451461e-06+3.1316822e-06j\n",
      "   -9.52812843e-06+3.7125428e-06j ...  7.21819652e-06-7.2433313e-06j\n",
      "    6.76863647e-06-7.6651013e-06j  6.29447914e-06-8.0590162e-06j]]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][0]['user']['channel'][100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b079846",
   "metadata": {},
   "source": [
    "# 사용자 위치 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "84337e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(727, 3)\n",
      "[[-71.03330231 -15.57629967   1.        ]\n",
      " [-68.63330078 -15.57629967   1.        ]\n",
      " [-52.83330154 -15.57629967   1.        ]\n",
      " [-31.23329926 -15.57629967   1.        ]]\n"
     ]
    }
   ],
   "source": [
    "location = dataset[0][0]['user']['location']\n",
    "print(location.shape)      # (사용자 수, 3)\n",
    "print(location[0:4])         # 첫 번째 사용자의 (x, y, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e9a9b6",
   "metadata": {},
   "source": [
    "# 경로정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "79cde8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "727\n",
      "{'num_paths': 2, 'DoD_phi': array([-160.941, -160.941], dtype=float32), 'DoD_theta': array([93.6525, 94.7439], dtype=float32), 'DoA_phi': array([19.0585, 19.0585], dtype=float32), 'DoA_theta': array([86.3475, 94.7439], dtype=float32), 'phase': array([ 143.357, -137.611], dtype=float32), 'ToA': array([2.61886e-07, 2.62253e-07], dtype=float32), 'LoS': array([1., 0.], dtype=float32), 'power': array([7.5363324e-09, 3.2098095e-09], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "paths = dataset[0][0]['user']['paths']\n",
    "#사용자 수\n",
    "print(len(paths))\n",
    "# 첫 번째 사용자 경로 정보\n",
    "print(paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2242869",
   "metadata": {},
   "source": [
    "# 기지국 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2900e481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['paths', 'LoS', 'location', 'distance', 'pathloss', 'channel'])\n"
     ]
    }
   ],
   "source": [
    "bs_data = dataset[0][0]['basestation']\n",
    "print(bs_data.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c063ea95",
   "metadata": {},
   "source": [
    "# Scene 및 사용자 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a20df92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene 0: 727 users\n"
     ]
    }
   ],
   "source": [
    "for i, scene in enumerate(dataset[0]):\n",
    "    user_locs = scene['user']['location']\n",
    "    print(f\"Scene {i}: {len(user_locs)} users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38f71e1",
   "metadata": {},
   "source": [
    "# 채널 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cbbee52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "727"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[0][0]['user']['channel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d474195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_paths': 2, 'DoD_phi': array([-160.941, -160.941], dtype=float32), 'DoD_theta': array([93.6525, 94.7439], dtype=float32), 'DoA_phi': array([19.0585, 19.0585], dtype=float32), 'DoA_theta': array([86.3475, 94.7439], dtype=float32), 'phase': array([ 143.357, -137.611], dtype=float32), 'ToA': array([2.61886e-07, 2.62253e-07], dtype=float32), 'LoS': array([1., 0.], dtype=float32), 'power': array([7.5363324e-09, 3.2098095e-09], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][0]['user']['paths'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "73df9912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "scene = dataset[0][0] # scene 0\n",
    "ue_idx = 0 # 첫 번째 사용자\n",
    "channel = scene['user']['channel'][ue_idx]\n",
    "print(channel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373ffb59",
   "metadata": {},
   "source": [
    "# channel CIR mat 정보 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ec4461cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'CIR_array_full'])\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "\n",
    "file_path = r'C:\\Users\\dlghd\\졸업프로젝트\\LWM\\O2_dyn_3p5\\scene_0\\O2_dyn_3p5.1.CIR.mat'\n",
    "mat_data = sio.loadmat(file_path)\n",
    "\n",
    "# 파일 안의 key 확인\n",
    "print(mat_data.keys())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "662cb441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'MATLAB 5.0 MAT-file, Platform: PCWIN64, Created on: Wed Jun 30 11:33:01 2021'\n"
     ]
    }
   ],
   "source": [
    "# 일반적으로 CIR key는 'CIR' 또는 'cir' 같은 이름일 가능성 높음\n",
    "H_cir = mat_data['__header__']  \n",
    "print(H_cir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f3d7c7",
   "metadata": {},
   "source": [
    "# Time-Prediction 시작\n",
    "## Time Series 형태로 변환\n",
    "### 단일사용자 채널 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4f36971f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0이 존재하는 채널 개수 0\n"
     ]
    }
   ],
   "source": [
    "# print(dataset[0][0]['user']['channel'][150][0][3])\n",
    "\n",
    "count = 0\n",
    "for h in dataset[0][0]['user']['channel'][100][0]:\n",
    "#     h = h.squeeze(0)\n",
    "    h_real = h.real\n",
    "    h_imag = h.imag\n",
    "    if np.sum(np.abs(h_real)) ==0:\n",
    "        count+=1\n",
    "    elif np.sum(np.abs(h_imag)) == 0:\n",
    "        count+=1\n",
    "\n",
    "print(\"0이 존재하는 채널 개수\",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7575de2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antenna #3 subcarriers: [ 5.3233252e-06-8.73100362e-06j  4.7875683e-06-9.03589535e-06j\n",
      "  4.2344141e-06-9.30795068e-06j  3.6658719e-06-9.54618190e-06j\n",
      "  3.0840083e-06-9.74972318e-06j  2.4909377e-06-9.91783418e-06j\n",
      "  1.8888149e-06-1.00499046e-05j  1.2798286e-06-1.01454543e-05j\n",
      "  6.6619128e-07-1.02041367e-05j  5.0133124e-08-1.02257372e-05j\n",
      " -5.6610725e-07-1.02101776e-05j -1.1802904e-06-1.01575160e-05j\n",
      " -1.7901845e-06-1.00679417e-05j -2.3935731e-06-9.94178117e-06j\n",
      " -2.9882635e-06-9.77949367e-06j -3.5720950e-06-9.58166765e-06j\n",
      " -4.1429457e-06-9.34902255e-06j -4.6987411e-06-9.08240327e-06j\n",
      " -5.2374617e-06-8.78277933e-06j -5.7571492e-06-8.45123941e-06j\n",
      " -6.2559161e-06-8.08898767e-06j -6.7319493e-06-7.69734197e-06j\n",
      " -7.1835188e-06-7.27772431e-06j -7.6089841e-06-6.83165945e-06j\n",
      " -8.0067985e-06-6.36076902e-06j -8.3755176e-06-5.86676424e-06j\n",
      " -8.7137996e-06-5.35144000e-06j -9.0204167e-06-4.81666848e-06j\n",
      " -9.2942537e-06-4.26439374e-06j -9.5343166e-06-3.69662257e-06j\n",
      " -9.7397324e-06-3.11541817e-06j -9.9097542e-06-2.52289237e-06j\n",
      " -1.0043765e-05-1.92119842e-06j -1.0141277e-05-1.31252318e-06j\n",
      " -1.0201937e-05-6.99078271e-07j -1.0225523e-05-8.30929494e-08j\n",
      " -1.0211949e-05+5.33194338e-07j -1.0161268e-05+1.14754403e-06j\n",
      " -1.0073660e-05+1.75772368e-06j -9.9494455e-06+2.36151573e-06j\n",
      " -9.7890743e-06+2.95672635e-06j -9.5931318e-06+3.54119220e-06j\n",
      " -9.3623275e-06+4.11278961e-06j -9.0975009e-06+4.66944175e-06j\n",
      " -8.7996150e-06+5.20912499e-06j -8.4697522e-06+5.72987892e-06j\n",
      " -8.1091102e-06+6.22981088e-06j -7.7190007e-06+6.70710369e-06j\n",
      " -7.3008405e-06+7.16002387e-06j -6.8561499e-06+7.58692431e-06j\n",
      " -6.3865441e-06+7.98625479e-06j -5.8937303e-06+8.35656374e-06j\n",
      " -5.3794988e-06+8.69650557e-06j -4.8457186e-06+9.00484429e-06j\n",
      " -4.2943293e-06+9.28046029e-06j -3.7273348e-06+9.52235223e-06j\n",
      " -3.1467955e-06+9.72963971e-06j -2.5548209e-06+9.90157059e-06j\n",
      " -1.9535621e-06+1.00375200e-05j -1.3452043e-06+1.01369933e-05j\n",
      " -7.3195804e-07+1.01996302e-05j -1.1605191e-07+1.02252015e-05j\n",
      "  5.0027592e-07+1.02136155e-05j  1.1147858e-06+1.01649139e-05j]\n",
      "0+0j인 서브캐리어 개수: 0\n",
      "완전 0+0j 안테나 포트 개수: 0\n",
      "0이 아닌 서브캐리어 개수: 2048\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1) (user, ue_port, bs_ant, subc) → (bs_ant, subc) 로 squeeze\n",
    "H = dataset[0][0]['user']['channel'][100, 0]   # shape: (32, 64), complex\n",
    "\n",
    "# 2) BS 안테나 인덱스 3의 서브캐리어 벡터 (64,)\n",
    "print(\"Antenna #3 subcarriers:\", H[3])\n",
    "\n",
    "# 3) 전체 서브캐리어(32×64) 중 값이 정확히 0인 요소 개수\n",
    "zero_elements = np.sum(H == 0)\n",
    "print(\"0+0j인 서브캐리어 개수:\", zero_elements)\n",
    "\n",
    "# 4) 서브캐리어 전부가 0인 안테나 포트(행) 개수\n",
    "zero_ports = np.sum(np.all(H == 0, axis=1))\n",
    "print(\"완전 0+0j 안테나 포트 개수:\", zero_ports)\n",
    "\n",
    "# 5) 만약 “값이 하나도 0이 아닌” 서브캐리어 요소 개수를 보고 싶다면\n",
    "nonzero_elements = np.sum(np.abs(H) > 0)\n",
    "print(\"0이 아닌 서브캐리어 개수:\", nonzero_elements)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20a73af",
   "metadata": {},
   "source": [
    "## 결측치 제거 및 dataload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "267c0446-af32-420e-a812-ea8f16a3b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# ❶ IterableDataset: 모든 유저·서브캐리어를 스트리밍\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class ChannelSeqDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    • seq_len 개의 과거 채널 벡터(real 64 + imag 64 → 128) → 다음 시점 벡터 예측\n",
    "    • 벡터는 평균전력 1 로 power‑normalize 후 반환\n",
    "    \"\"\"\n",
    "    def __init__(self, scenes, seq_len: int = 5, eps: float = 1e-9):\n",
    "        super().__init__()\n",
    "        self.scenes   = scenes\n",
    "        self.seq_len  = seq_len\n",
    "        self.eps      = eps                        # 0 division 방지용 신호세기의 크기 \n",
    "        ch0           = scenes[0][0]['user']['channel']\n",
    "        self.U        = ch0.shape[0]               # 사용자 수\n",
    "        self.A        = ch0.shape[2]               # 안테나 32\n",
    "        self.S        = ch0.shape[3]               # 서브캐리어 64\n",
    "        self.vec_len  = 2 * self.A                 # 64 real + imag\n",
    "        0\n",
    "    def _vec(self, scene, u: int, sc: int) -> torch.Tensor:\n",
    "        \"\"\"(32,) complex → (64,) float32  +  power norm\"\"\"\n",
    "        h = scene[0]['user']['channel'][u, 0, :, sc]          # (32,)\n",
    "        v = np.concatenate([h.real, h.imag]).astype(np.float32)\n",
    "        p = np.mean(v * v) + self.eps                         # 평균 전력: 채널 벡터 h의 각 성분의 진폭 제곱을 합산\n",
    "        v /= np.sqrt(p)                                       # 정규화\n",
    "        return torch.from_numpy(v)                            # (64,)\n",
    "\n",
    "    def __iter__(self):\n",
    "        T = len(self.scenes)\n",
    "        for t in range(self.seq_len, T):  # 타깃 시점\n",
    "            past_scenes = self.scenes[t - self.seq_len : t]\n",
    "            tgt_scene   = self.scenes[t]\n",
    "            for u in range(self.U):\n",
    "                for s in range(self.S):\n",
    "                    seq = torch.stack([self._vec(ps, u, s) for ps in past_scenes])\n",
    "                    if not torch.any(seq):        # 전부 0이면 skip\n",
    "                        continue\n",
    "                    target = self._vec(tgt_scene, u, s)\n",
    "                    if not torch.any(target):     # target이 0이면 skip\n",
    "                        continue\n",
    "                    yield seq, target             # ← 여기서 masked_pos 제거\n",
    "                # shapes: (5,64) / (1,) / (64,)\n",
    "    \n",
    "    def __len__(self):\n",
    "         return (len(self.scenes) - self.seq_len) * self.U * self.S\n",
    "# ─────────────────────────────────────────────\n",
    "# ❷ 학습·검증 DataLoader train : val = 6 : 4\n",
    "seq_len      = 5\n",
    "split_ratio  = 0.6\n",
    "split_idx    = int(len(dataset) * split_ratio)\n",
    "\n",
    "train_ds = ChannelSeqDataset(dataset[:split_idx], seq_len=seq_len)\n",
    "val_ds   = ChannelSeqDataset(dataset[split_idx:], seq_len=seq_len)\n",
    "\n",
    "# train_ds 순회하면서 feature/target min, max 계산\n",
    "\n",
    "\n",
    "batch_size   = 32\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "# ─────────────────────────────────────────────\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c982d28d-40ee-40c0-8a04-2bbd9aa05f4c",
   "metadata": {},
   "source": [
    "## LWM 코드와 다름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6f9dc1d8-db2c-4816-b754-6e85e4cd350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data      import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 1) NumPy 배열로 추출\n",
    "X_list, y_list = [], []\n",
    "for seq, tgt in ChannelSeqDataset(dataset, seq_len=seq_len):\n",
    "    X_list.append(seq.numpy())\n",
    "    y_list.append(tgt.numpy())\n",
    "\n",
    "X = np.stack(X_list, axis=0)  # (N, L, D)\n",
    "y = np.stack(y_list, axis=0)  # (N, D)\n",
    "\n",
    "# 2) Train/Val split\n",
    "split_idx = int(len(X) * split_ratio)\n",
    "X_tr, X_va = X[:split_idx], X[split_idx:]\n",
    "y_tr, y_va = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# 3) 스케일러 준비 및 fit (0–1 사이로 스케일링)\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "Ntr, L, D = X_tr.shape\n",
    "\n",
    "scaler_x.fit(X_tr.reshape(-1, D))\n",
    "scaler_y.fit(y_tr)\n",
    "\n",
    "# 4) Transform 후 원래 shape 복원\n",
    "X_tr_s = scaler_x.transform(X_tr.reshape(-1, D)).reshape(Ntr, L, D)\n",
    "\n",
    "Nva = X_va.shape[0]\n",
    "X_va_s = scaler_x.transform(X_va.reshape(-1, D)).reshape(Nva, L, D)\n",
    "\n",
    "y_tr_s = scaler_y.transform(y_tr)\n",
    "y_va_s = scaler_y.transform(y_va)\n",
    "\n",
    "# 5) Dataset & DataLoader\n",
    "train_ds    = TensorDataset(\n",
    "    torch.from_numpy(X_tr_s).float(),\n",
    "    torch.from_numpy(y_tr_s).float()\n",
    ")\n",
    "val_ds      = TensorDataset(\n",
    "    torch.from_numpy(X_va_s).float(),\n",
    "    torch.from_numpy(y_va_s).float()\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 6) 첫 배치 언패킹\n",
    "seqs, tgts = next(iter(train_loader))  # seqs: (B, L, D), tgts: (B, D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a95b39ac-9ca8-441b-8e2f-c4ccd4e2ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "# from sklearn.preprocessing import MinMaxScaler          # 또는 StandardScaler\n",
    "\n",
    "\n",
    "# # ───────────────────────────────────────────────\n",
    "# # 0)  원본 scene 리스트\n",
    "# #     all_data = [...]  로 모아 둔 결과를 그대로 사용\n",
    "# raw_scenes = all_data                  # (T,) list  ←✱  variable 이름만 변경\n",
    "\n",
    "# seq_len   = 5                          # 과거 프레임 수\n",
    "# eps       = 1e-9                       # 0-division 방지\n",
    "\n",
    "# # ───────────────────────────────────────────────\n",
    "# # 1)  (X, Y) 시퀀스 추출\n",
    "# Xs_list, Ys_list = [], []\n",
    "\n",
    "# T = len(raw_scenes)\n",
    "# assert T >= seq_len + 1, \"scenes 개수가 seq_len 보다 많아야 합니다.\"\n",
    "\n",
    "# # 안테나(A)·서브캐리어(S)·사용자(U) 크기는 한 번만 읽어도 됨\n",
    "# sample_ch = raw_scenes[0][0]['user']['channel']     # (U, 1, A, S)\n",
    "# U, _, A, S = sample_ch.shape\n",
    "# feat_dim   = 2 * A                                 # real+imag  → 64\n",
    "\n",
    "# for t in range(seq_len, T):                        # 타깃 시점\n",
    "#     past_scenes = raw_scenes[t - seq_len : t]\n",
    "#     tgt_scene   = raw_scenes[t]\n",
    "\n",
    "#     for u in range(U):\n",
    "#         for sc in range(S):\n",
    "#             # ── 입력 시퀀스 (seq_len, feat_dim) ──────────────\n",
    "#             seq = []\n",
    "#             for ps in past_scenes:\n",
    "#                 h = ps[0]['user']['channel'][u, 0, :, sc]   # (A,) complex\n",
    "#                 v = np.concatenate([h.real, h.imag]).astype(np.float32)\n",
    "#                 # (선택) power-normalize\n",
    "#                 pwr = np.mean(v * v) + eps\n",
    "#                 v  /= np.sqrt(pwr)\n",
    "#                 seq.append(v)\n",
    "#             seq = np.stack(seq, axis=0)                     # (seq_len, 64)\n",
    "\n",
    "#             # ── 타깃 벡터 (feat_dim,) ───────────────────────\n",
    "#             h_tgt = tgt_scene[0]['user']['channel'][u, 0, :, sc]\n",
    "#             y     = np.concatenate([h_tgt.real, h_tgt.imag]).astype(np.float32)\n",
    "#             pwr   = np.mean(y * y) + eps\n",
    "#             y    /= np.sqrt(pwr)                            # 동일 정규화\n",
    "\n",
    "#             Xs_list.append(seq)\n",
    "#             Ys_list.append(y)\n",
    "\n",
    "# print(f\"총 샘플 수: {len(Xs_list):,}\")\n",
    "\n",
    "# # ───────────────────────────────────────────────\n",
    "# # 2)  numpy 배열로 변환\n",
    "# Xs = np.stack(Xs_list, axis=0).astype(np.float32)     # (N, seq_len, 64)\n",
    "# Ys = np.stack(Ys_list, axis=0).astype(np.float32)     # (N, 64)\n",
    "\n",
    "# N, seq_len, feat_dim = Xs.shape\n",
    "# print(\"Xs:\", Xs.shape, \"Ys:\", Ys.shape)\n",
    "\n",
    "# # ───────────────────────────────────────────────\n",
    "# # 3)  스케일링 (선택) — 입력만 스케일링 권장\n",
    "# scaler_x = MinMaxScaler().fit(Xs.reshape(-1, feat_dim))\n",
    "# Xs_s     = scaler_x.transform(Xs.reshape(-1, feat_dim)).reshape(Xs.shape)\n",
    "\n",
    "# # y도 스케일링하려면 ↓ 두 줄 활성화\n",
    "# scaler_y = MinMaxScaler().fit(Ys)\n",
    "# Ys_s     = scaler_y.transform(Ys)\n",
    "# # 그렇지 않으면 원스케일 유지\n",
    "# # scaler_y = None\n",
    "# # Ys_s     = Ys\n",
    "\n",
    "# # ───────────────────────────────────────────────\n",
    "# # 4)  TensorDataset / DataLoader\n",
    "# tensor_ds = TensorDataset(torch.from_numpy(Xs_s), torch.from_numpy(Ys_s))\n",
    "\n",
    "# train_len         = int(0.6 * len(tensor_ds))\n",
    "# train_ds, val_ds  = random_split(tensor_ds, [train_len, len(tensor_ds) - train_len])\n",
    "\n",
    "# train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  drop_last=True)\n",
    "# val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0aaadd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200563"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds) #4x727x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "90f5b39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133709"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_ds) #1x727x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fe8a8a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x00000232921051F0>\n",
      "batch_size: 32\n",
      "dataset: <torch.utils.data.dataset.TensorDataset object at 0x00000232F9908200>\n",
      "total samples: 200563\n",
      "total batches: 6268\n",
      "seqs.shape: torch.Size([32, 5, 64])\n",
      "tgts.shape: torch.Size([32, 64])\n"
     ]
    }
   ],
   "source": [
    "# 1) DataLoader 설정 확인\n",
    "print(train_loader)                # DataLoader 정보 전체\n",
    "print(\"batch_size:\", train_loader.batch_size)\n",
    "print(\"dataset:\",   train_loader.dataset)\n",
    "\n",
    "# 총 샘플 수\n",
    "print(\"total samples:\", len(train_loader.dataset))\n",
    "# → (len(scenes) - seq_len) * U * S 와 동일한 값\n",
    "\n",
    "# 총 배치 수\n",
    "print(\"total batches:\", len(train_loader))\n",
    "# → ceil(total_samples / batch_size)\n",
    "\n",
    "\n",
    "# 3) 첫 번째 배치 내용 확인\n",
    "first_batch = next(iter(train_loader))\n",
    "seqs, tgts = first_batch\n",
    "print(\"seqs.shape:\",   seqs.shape)    # (B, seq_len, vec_len)\n",
    "print(\"tgts.shape:\",   tgts.shape)    # (B, vec_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3348886",
   "metadata": {},
   "source": [
    "## 이론적 input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ff85e07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ total samples (input_size): 200563\n",
      "→ batch size: 32\n",
      "→ total batches: 6268\n",
      "→ train samples: 200563\n",
      "→ val   samples: 133709\n"
     ]
    }
   ],
   "source": [
    "# ──────────────────────────────────────────────────\n",
    "# input_size 계산 및 출력 (수정된 버전)\n",
    "# ──────────────────────────────────────────────────\n",
    "\n",
    "# 1) 전체 샘플 수\n",
    "input_size = len(train_ds)\n",
    "\n",
    "# 2) 배치 크기와 배치 수\n",
    "batch_size = 32   # 이미 설정된 값\n",
    "n_batches  = len(train_loader)\n",
    "\n",
    "print(f\"→ total samples (input_size): {input_size}\")\n",
    "print(f\"→ batch size: {batch_size}\")\n",
    "print(f\"→ total batches: {n_batches}\")\n",
    "\n",
    "# 3) train/val 분할 비율 확인 (선택)\n",
    "print(f\"→ train samples: {len(train_ds)}\")\n",
    "print(f\"→ val   samples: {len(val_ds)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf6bac8",
   "metadata": {},
   "source": [
    "# 아래 코드 구조\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│ input_ids  (B, seq_len, element_length)  ─┐                 │\n",
    "│ masked_pos (B, num_mask)                  ├─>  LWM backbone │\n",
    "│                                           │    (12-층 트랜스포머)  \n",
    "└────────────────────────────────────────────┘         │\n",
    "            logits_lm  (B, num_mask, element_length)  │   enc_output (B, seq_len, d_model)\n",
    "                                                      ▼\n",
    "                        ┌─[풀링]───────────────┐      ←── feat (B, d_model)\n",
    "                        │ 첫 토큰(0번) 선택    │\n",
    "                        │   or 평균/최대 풀링 │\n",
    "                        └──────────────────────┘\n",
    "                                      ▼\n",
    "                       FC 헤드  (d_model → hidden_dim → out_dim)\n",
    "                                      ▼\n",
    "                                out (B, out_dim)\n",
    "\n",
    "# 시각적비유\n",
    "\n",
    "[패치 프로젝터]──▶[Transformer ×12]──▶[LayerNorm]──┐\n",
    "                                                  ├─▶ 64-차 벡터 (CLS 또는 풀링) ─▶ MLP ─▶ out                                                \n",
    "[Positional Embedding]─────────────────────────────┘\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5beb3297-472f-45b8-a464-1650fca7c8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LWM 모델과 같이 transformer ecoder 층 수 12개로 \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 1) GRUWithHead — GRU 백본 + FC-헤드\n",
    "# ────────────────────────────────────────────────\n",
    "class GRUWithHead(nn.Module):\n",
    "    def __init__(self,\n",
    "                 feat_dim: int,\n",
    "                 hidden_size: int = 256,\n",
    "                 num_layers: int = 12,\n",
    "                 bidirectional: bool = False,\n",
    "                 dropout: float = 0.2,\n",
    "                 hidden_dim: int = 256,\n",
    "                 out_dim: int    = 64,\n",
    "                 freeze_backbone: bool = False):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.GRU(\n",
    "            input_size   = feat_dim,\n",
    "            hidden_size  = hidden_size,\n",
    "            num_layers   = num_layers,\n",
    "            batch_first  = True,\n",
    "            bidirectional= bidirectional,\n",
    "            dropout      = dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        gru_out_dim = hidden_size * (2 if bidirectional else 1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(gru_out_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.backbone(x)         # (B, seq_len, H)\n",
    "        feat   = out[:, -1, :]            # 마지막 타임스텝\n",
    "        return self.head(feat)            # (B, out_dim)\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 2) TransformerWithHead — 소형 Transformer 백본 + FC-헤드\n",
    "# ────────────────────────────────────────────────\n",
    "class TransformerWithHead(nn.Module):\n",
    "    def __init__(self,\n",
    "                 feat_dim: int,\n",
    "                 n_heads: int   = 4,\n",
    "                 dim_ff: int    = 256,\n",
    "                 n_layers: int  = 12, # layer층 lwm과 같이 12개로 설정\n",
    "                 # dropout: float = 0.1, # dropout 제거 \n",
    "                 hidden_dim: int = 256,\n",
    "                 out_dim: int    = 64,\n",
    "                 freeze_backbone: bool = False):\n",
    "        super().__init__()\n",
    "        layer = TransformerEncoderLayer(\n",
    "            d_model = feat_dim,\n",
    "            nhead   = n_heads,\n",
    "            dim_feedforward = dim_ff,\n",
    "            dropout = dropout\n",
    "        )\n",
    "        self.backbone = TransformerEncoder(layer, num_layers=n_layers)\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(feat_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout(dropout), dropout 제거\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z    = self.backbone(x.transpose(0,1))  # (T, B, F)\n",
    "        feat = z[-1]                             # 마지막 토큰\n",
    "        return self.head(feat)                  # (B, out_dim)\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 3) NoPrediction — 마지막 타임스텝 그대로 리턴\n",
    "# ────────────────────────────────────────────────\n",
    "# ────────────────────────────────────────────────\n",
    "# 6) SequentialRNN — RNN 백본 + Linear\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 7) SequentialLSTM — LSTM 백본 + Linear\n",
    "# ────────────────────────────────────────────────\n",
    "class SequentialLSTM(nn.Module):\n",
    "    def __init__(self, feat_dim, hidden=128):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(feat_dim, hidden, batch_first=True)\n",
    "        self.head = nn.Linear(hidden, feat_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.head(out[:, -1])  # (B, feat_dim)\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 8) ParallelTransformer — 병렬 Transformer 백본 + Linear 헤드\n",
    "# ────────────────────────────────────────────────\n",
    "class ParallelTransformer(nn.Module):\n",
    "    def __init__(self, feat_dim, heads: int = 4, dim_ff: int = 256, layers: int = 2):\n",
    "        super().__init__()\n",
    "        layer = TransformerEncoderLayer(\n",
    "            d_model = feat_dim,\n",
    "            nhead   = heads,\n",
    "            dim_feedforward = dim_ff\n",
    "        )\n",
    "        self.enc  = TransformerEncoder(layer, num_layers=layers)\n",
    "        self.head = nn.Linear(feat_dim, feat_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, seq_len, feat_dim)\n",
    "        z    = self.enc(x.transpose(0,1))  # (T, B, F)\n",
    "        feat = z[-1]                        # 마지막 토큰\n",
    "        return self.head(feat)             # (B, feat_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3cec5b",
   "metadata": {},
   "source": [
    "## input_size\n",
    "-input_size = (scene - seq_len) * U * S -> (10-5)+69040*64 = 22092800  \n",
    "-batch_size = 32  \n",
    "-배치 수 = input_size / batch_size = 690400배치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "727c7474-1953-4357-a817-9eb458e4522a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟢 gru initialized with {'feat_dim': 64, 'hidden_size': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2, 'hidden_dim': 256, 'out_dim': 64, 'freeze_backbone': False}\n",
      "🟢 gru initialized with {'feat_dim': 64, 'hidden_size': 128, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.2, 'hidden_dim': 256, 'out_dim': 64, 'freeze_backbone': False}\n"
     ]
    }
   ],
   "source": [
    "# 백본 동결 False값\n",
    "from torch.optim import Adam\n",
    "\n",
    "batch_size, seq_len, D = X_tr_s.shape  # D = 64\n",
    "hidden_dim = 256\n",
    "\n",
    "# ── 2) 모델 카탈로그 ─────────────────────────\n",
    "MODEL_CATALOG = {\n",
    "    \"gru\":                  GRUWithHead,\n",
    "    \"transformer\":          TransformerWithHead,\n",
    "    \"nopred\":               NoPrediction,\n",
    "    \"pad\":                  SequentialPAD,\n",
    "    \"pvec\":                 SequentialPVEC,\n",
    "    \"seqrnn\":               SequentialRNN,\n",
    "    \"seqlstm\":              SequentialLSTM,\n",
    "    \"parallel_transformer\": ParallelTransformer,\n",
    "}\n",
    "# ── 2.1) 모델별 파라미터 맵 ────────────────────────\n",
    "MODEL_PARAMS = {\n",
    "    \"gru\": {\n",
    "        \"feat_dim\":        feat_dim,\n",
    "        \"hidden_size\":     128,\n",
    "        \"num_layers\":      12,\n",
    "        \"bidirectional\":   False,\n",
    "        # \"dropout\":         0.2,\n",
    "        \"hidden_dim\":      hidden_dim,\n",
    "        \"out_dim\":         out_dim,\n",
    "        \"freeze_backbone\": False,\n",
    "    },\n",
    "    \"transformer\": {\n",
    "        \"feat_dim\":        feat_dim,\n",
    "        \"n_heads\":         4,\n",
    "        \"dim_ff\":          256,\n",
    "        \"n_layers\":        12,\n",
    "        # \"dropout\":         0.1,\n",
    "        \"hidden_dim\":      hidden_dim,\n",
    "        \"out_dim\":         out_dim,\n",
    "        \"freeze_backbone\": False,\n",
    "    },\n",
    "    \"nopred\": {},\n",
    "    \"pad\":    {\"eps\": 1e-6},\n",
    "    \"pvec\":   {},\n",
    "    \"seqrnn\": {\"feat_dim\": feat_dim, \"hidden\": 128},\n",
    "    \"seqlstm\":{\"feat_dim\": feat_dim, \"hidden\": 128},\n",
    "    \"parallel_transformer\": {\n",
    "        \"feat_dim\": feat_dim,\n",
    "        \"heads\":    4,\n",
    "        \"dim_ff\":   256,\n",
    "        \"layers\":   2,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# ── 3) 실험할 모델 선택 ─────────────────────\n",
    "model_name = \"gru\"  # 원하는 모델 이름\n",
    "ModelCls   = MODEL_CATALOG[model_name]\n",
    "\n",
    "# ── 3) 실험할 모델 선택 ─────────────────────\n",
    "model_name = \"gru\"  # 원하는 모델 이름\n",
    "ModelCls   = MODEL_CATALOG[model_name]\n",
    "args       = MODEL_PARAMS[model_name]   # ★ 여기가 수정된 부분\n",
    "\n",
    "# ── 4) 인스턴스화 & 옵티마이저 ───────────────\n",
    "model     = ModelCls(**args).to(device)\n",
    "print(f\"🟢 {model_name} initialized with\", args)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "\n",
    "# ── 5) 인스턴스화 & 옵티마이저 ───────────────\n",
    "model     = ModelCls(**args).to(device)\n",
    "print(f\"🟢 {model_name} initialized with\", args)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b37383cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.optim import Adam\n",
    "\n",
    "# # ── 0) 디바이스 ──────────────────────────────\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"device =\", device)\n",
    "\n",
    "# # ── 1) train_loader에서 시퀀스 길이와 피처 차원 추출 ─────────\n",
    "# first_batch = next(iter(train_loader))\n",
    "# seqs, _     = first_batch            # (B, seq_len, feat_dim), (B, feat_dim)\n",
    "# _, seq_len, feat_dim = seqs.shape\n",
    "# out_dim     = feat_dim\n",
    "# hidden_dim  = 256\n",
    "\n",
    "# print(f\"seq_len = {seq_len}, feat_dim = {feat_dim}\")\n",
    "\n",
    "# # ── 2) 모델 카탈로그 ─────────────────────────\n",
    "# MODEL_CATALOG = {\n",
    "#     \"gru\":                  GRUWithHead,\n",
    "#     \"transformer\":          TransformerWithHead,\n",
    "#     \"nopred\":               NoPrediction,\n",
    "#     \"pad\":                  SequentialPAD,\n",
    "#     \"pvec\":                 SequentialPVEC,\n",
    "#     \"seqrnn\":               SequentialRNN,\n",
    "#     \"seqlstm\":              SequentialLSTM,\n",
    "#     \"parallel_transformer\": ParallelTransformer,\n",
    "# }\n",
    "# # ── 2.1) 모델별 파라미터 맵 ────────────────────────\n",
    "# MODEL_PARAMS = {\n",
    "#     \"gru\": {\n",
    "#         \"feat_dim\":        feat_dim,\n",
    "#         \"hidden_size\":     128,\n",
    "#         \"num_layers\":      2,\n",
    "#         \"bidirectional\":   False,\n",
    "#         \"dropout\":         0.2,\n",
    "#         \"hidden_dim\":      hidden_dim,\n",
    "#         \"out_dim\":         out_dim,\n",
    "#         \"freeze_backbone\": False,\n",
    "#     },\n",
    "#     \"transformer\": {\n",
    "#         \"feat_dim\":        feat_dim,\n",
    "#         \"n_heads\":         4,\n",
    "#         \"dim_ff\":          256,\n",
    "#         \"n_layers\":        2,\n",
    "#         \"dropout\":         0.1,\n",
    "#         \"hidden_dim\":      hidden_dim,\n",
    "#         \"out_dim\":         out_dim,\n",
    "#         \"freeze_backbone\": False,\n",
    "#     },\n",
    "#     \"nopred\": {},\n",
    "#     \"pad\":    {\"eps\": 1e-6},\n",
    "#     \"pvec\":   {},\n",
    "#     \"seqrnn\": {\"feat_dim\": feat_dim, \"hidden\": 128},\n",
    "#     \"seqlstm\":{\"feat_dim\": feat_dim, \"hidden\": 128},\n",
    "#     \"parallel_transformer\": {\n",
    "#         \"feat_dim\": feat_dim,\n",
    "#         \"heads\":    4,\n",
    "#         \"dim_ff\":   256,\n",
    "#         \"layers\":   2,\n",
    "#     },\n",
    "# }\n",
    "\n",
    "\n",
    "# # ── 3) 실험할 모델 선택 ─────────────────────\n",
    "# model_name = \"gru\"  # 원하는 모델 이름\n",
    "# ModelCls   = MODEL_CATALOG[model_name]\n",
    "\n",
    "# # ── 3) 실험할 모델 선택 ─────────────────────\n",
    "# model_name = \"gru\"  # 원하는 모델 이름\n",
    "# ModelCls   = MODEL_CATALOG[model_name]\n",
    "# args       = MODEL_PARAMS[model_name]   # ★ 여기가 수정된 부분\n",
    "\n",
    "# # ── 4) 인스턴스화 & 옵티마이저 ───────────────\n",
    "# model     = ModelCls(**args).to(device)\n",
    "# print(f\"🟢 {model_name} initialized with\", args)\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "\n",
    "# # ── 5) 인스턴스화 & 옵티마이저 ───────────────\n",
    "# model     = ModelCls(**args).to(device)\n",
    "# print(f\"🟢 {model_name} initialized with\", args)\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7f7e4ba2-a72d-48fc-89c8-acb7a92f984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def call_model(model: nn.Module, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    모든 모델을 한 줄로 호출하기 위한 헬퍼.\n",
    "    · 모든 모델의 forward(x) 형태로 통일되어 있습니다.\n",
    "    \"\"\"\n",
    "    return model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3e8931ae-5238-415b-bea6-26c65ab15b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install umap-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a6a78d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)               # call_model 대신 직접 호출\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running += loss.item() * xb.size(0)\n",
    "    return running / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    tot_rmse = tot_nmse = tot_n = 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        pred = model(xb)               # 동일하게 직접 호출\n",
    "        bs   = xb.size(0)\n",
    "\n",
    "        tot_rmse += rmse(pred, yb).item() * bs\n",
    "        tot_nmse += nmse(pred, yb).item() * bs\n",
    "        tot_n    += bs\n",
    "\n",
    "    return tot_rmse / tot_n, tot_nmse / tot_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c2ed0412-306c-440a-b301-48116024e204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "def rmse(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Root-Mean-Squared Error (batch mean)\n",
    "    \"\"\"\n",
    "    return torch.sqrt(F.mse_loss(pred, target, reduction=\"mean\"))\n",
    "\n",
    "def nmse(pred: torch.Tensor, target: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Normalized MSE (linear):\n",
    "      E[‖pred – target‖²] / E[‖target‖²]\n",
    "    returns scalar (batch mean)\n",
    "    \"\"\"\n",
    "    mse_s = ((pred - target)**2).view(pred.size(0), -1).sum(dim=1)\n",
    "    pwr_s = (target**2).view(target.size(0), -1).sum(dim=1) + eps\n",
    "    return (mse_s / pwr_s).mean()\n",
    "\n",
    "def evaluate(model,\n",
    "             loader,\n",
    "             device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Validation loop **without any inverse-transform**.\n",
    "    Assumes your dataset already yields unit-power normalized targets.\n",
    "    Returns dict with:\n",
    "      \"RMSE\" : linear RMSE,\n",
    "      \"NMSE\" : linear NMSE.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_rmse, total_nmse, total_samples = 0.0, 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred   = model(xb)\n",
    "\n",
    "            bs = xb.size(0)\n",
    "            total_rmse  += rmse(pred, yb).item() * bs\n",
    "            total_nmse  += nmse(pred, yb).item() * bs\n",
    "            total_samples += bs\n",
    "\n",
    "    return {\n",
    "        \"RMSE\": total_rmse  / total_samples,\n",
    "        \"NMSE\": total_nmse  / total_samples\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a06b20a",
   "metadata": {},
   "source": [
    "# model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7b169d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training gru ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gru 01/10] Train Loss: 0.0064  Val RMSE: 0.0397  Val NMSE: 8.0955e-03  Val NMSE_dB: -20.9 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gru 02/10] Train Loss: 0.0028  Val RMSE: 0.0385  Val NMSE: 7.7995e-03  Val NMSE_dB: -21.1 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gru 03/10] Train Loss: 0.0025  Val RMSE: 0.0387  Val NMSE: 7.7534e-03  Val NMSE_dB: -21.1 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gru 04/10] Train Loss: 0.0024  Val RMSE: 0.0375  Val NMSE: 7.4857e-03  Val NMSE_dB: -21.3 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gru 05/10] Train Loss: 0.0023  Val RMSE: 0.0372  Val NMSE: 7.4040e-03  Val NMSE_dB: -21.3 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gru 06/10] Train Loss: 0.0022  Val RMSE: 0.0368  Val NMSE: 7.2813e-03  Val NMSE_dB: -21.4 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gru 07/10] Train Loss: 0.0021  Val RMSE: 0.0368  Val NMSE: 7.2418e-03  Val NMSE_dB: -21.4 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gru 08/10] Train Loss: 0.0020  Val RMSE: 0.0367  Val NMSE: 7.2382e-03  Val NMSE_dB: -21.4 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gru 09/10] Train Loss: 0.0020  Val RMSE: 0.0361  Val NMSE: 7.0640e-03  Val NMSE_dB: -21.5 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dlghd\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gru 10/10] Train Loss: 0.0019  Val RMSE: 0.0361  Val NMSE: 7.0833e-03  Val NMSE_dB: -21.5 dB\n",
      "gru training time: 723.12s\n",
      "\n",
      "=== Training transformer ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[transformer 01/10] Train Loss: 0.0058  Val RMSE: 0.0380  Val NMSE: 7.6651e-03  Val NMSE_dB: -21.2 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[transformer 02/10] Train Loss: 0.0025  Val RMSE: 0.0367  Val NMSE: 7.2496e-03  Val NMSE_dB: -21.4 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[transformer 03/10] Train Loss: 0.0022  Val RMSE: 0.0359  Val NMSE: 7.0929e-03  Val NMSE_dB: -21.5 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[transformer 04/10] Train Loss: 0.0021  Val RMSE: 0.0359  Val NMSE: 7.1014e-03  Val NMSE_dB: -21.5 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[transformer 05/10] Train Loss: 0.0020  Val RMSE: 0.0351  Val NMSE: 6.9362e-03  Val NMSE_dB: -21.6 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[transformer 06/10] train:  83%|█████████████████████████▊     | 5213/6268 [01:44<00:21, 49.76it/s, train_loss=0.00189]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 하이퍼파라미터\n",
    "# ─────────────────────────────────────────────\n",
    "num_epochs     = 10\n",
    "LR             = 1e-4\n",
    "start_time_all = time.time()\n",
    "results        = {}\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 모델별 Training & Validation Loop\n",
    "# ─────────────────────────────────────────────\n",
    "for model_name, ModelCls in MODEL_CATALOG.items():\n",
    "    print(f\"\\n=== Training {model_name} ===\")\n",
    "    # 1) 모델 초기화\n",
    "    args  = MODEL_PARAMS[model_name]\n",
    "    model = ModelCls(**args).to(device)\n",
    "\n",
    "    # 2) trainable 파라미터 체크\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    if len(trainable_params) == 0:\n",
    "        print(f\"⚠️  '{model_name}' has no trainable parameters — skipping training.\")\n",
    "        # 필요하다면 validation만 돌려보거나, 결과에 NaN/None 기록\n",
    "        results[model_name] = float('nan')\n",
    "        continue\n",
    "\n",
    "    # 3) 옵티마이저\n",
    "    optimizer = torch.optim.Adam(trainable_params, lr=LR)\n",
    "\n",
    "    start_time_model = time.time()\n",
    "\n",
    "    # 4) 에폭 루프\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"[{model_name} {epoch:02d}/{num_epochs}] train\", leave=False)\n",
    "        for i, (xb, yb) in enumerate(pbar, 1):\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 0:\n",
    "                pbar.set_postfix(train_loss=running_loss / i)\n",
    "\n",
    "        avg_train_loss = running_loss / i\n",
    "\n",
    "        # VALIDATION\n",
    "        metrics     = evaluate(model, val_loader, device)\n",
    "        val_rmse    = metrics[\"RMSE\"]\n",
    "        val_nmse    = metrics[\"NMSE\"]\n",
    "        val_nmse_db = 10 * math.log10(val_nmse)\n",
    "\n",
    "        print(\n",
    "            f\"[{model_name} {epoch:02d}/{num_epochs}] \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f}  \"\n",
    "            f\"Val RMSE: {val_rmse:.4f}  \"\n",
    "            f\"Val NMSE: {val_nmse:.4e}  \"\n",
    "            f\"Val NMSE_dB: {val_nmse_db:.1f} dB\"\n",
    "        )\n",
    "\n",
    "    elapsed = time.time() - start_time_model\n",
    "    print(f\"{model_name} training time: {elapsed:.2f}s\")\n",
    "    results[model_name] = val_nmse_db\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 모든 모델 요약\n",
    "# ─────────────────────────────────────────────\n",
    "print(\"\\n=== Summary of NMSE(dB) by model ===\")\n",
    "for name, nmse_db in results.items():\n",
    "    print(f\"{name:20s}: {nmse_db if not math.isnan(nmse_db) else 'skipped':>6}\")\n",
    "\n",
    "print(f\"\\nTotal training time for all models: {time.time() - start_time_all:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6932b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c721e61b-0c2f-43fa-b12d-16e02a703ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb9b24c-bd94-4495-8334-e777fd697995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
