{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0939df15-abc7-40e7-83bf-f12820dd42f2",
   "metadata": {},
   "source": [
    "# All Model saves here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cbf7fb-304f-432d-a3f9-580052d5a375",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e94396-1d80-4df3-8648-19368f82c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import (\n",
    "    TensorDataset,\n",
    "    DataLoader,\n",
    "    random_split,\n",
    "    IterableDataset\n",
    ")\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import DeepMIMOv3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set default figure size for matplotlib\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb2d8b4-7d3d-43ae-b997-2a4e6eab9a1a",
   "metadata": {},
   "source": [
    "## GPU Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a6479f7-d2b2-4632-b3a8-ffe3c49a6608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# GPU \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea87b9c9-a4b1-4442-b26f-606828c4dc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.6\n",
      "90501\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)                   \n",
    "print(torch.backends.cudnn.version())       \n",
    "print(\"CUDA available:\", torch.cuda.is_available())  # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dc1a47-3bc0-4236-813e-c7be4c21bd41",
   "metadata": {},
   "source": [
    "## DeepMIMOv3 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "821eb69b-806e-4694-adf9-fd0efcbf5e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = DeepMIMOv3.default_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1d1f75d-54c0-4198-990f-15eaeec28e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change parameters for the setup\n",
    "# Scenario O1_60 extracted at the dataset_folder\n",
    "#LWM dynamic senario\n",
    "# parameters['dataset_folder'] = r'/content/drive/MyDrive/Colab Notebooks/LWM'\n",
    "scene = 15 # scene 15\n",
    "# change my linux route\n",
    "parameters['dataset_folder'] = '/home/dlghdbs200/LWM'\n",
    "\n",
    "# scnario = 02_dyn_3p5 <- download file\n",
    "parameters['scenario'] = 'O2_dyn_3p5'\n",
    "parameters['dynamic_scenario_scenes'] = np.arange(scene) #scene 0~9\n",
    "\n",
    "# Up to 10 multipath paths per user-to-base station channel\n",
    "parameters['num_paths'] = 10\n",
    "\n",
    "# User rows 1-100\n",
    "parameters['user_rows'] = np.arange(100)\n",
    "# User subsampling\n",
    "parameters['user_subsampling'] = 0.01\n",
    "\n",
    "# Activate only the first basestation\n",
    "parameters['active_BS'] = np.array([1])\n",
    "\n",
    "parameters['activate_OFDM'] = 1\n",
    "\n",
    "parameters['OFDM']['bandwidth'] = 0.05 # 50 MHz\n",
    "parameters['OFDM']['subcarriers'] = 512 # OFDM with 512 subcarriers\n",
    "parameters['OFDM']['selected_subcarriers'] = np.arange(0, 64, 1)\n",
    "#parameters['OFDM']['subcarriers_limit'] = 64 # Keep only first 64 subcarriers\n",
    "\n",
    "parameters['ue_antenna']['shape'] = np.array([1, 1]) # Single antenna\n",
    "parameters['bs_antenna']['shape'] = np.array([1, 32]) # ULA of 32 elements\n",
    "#parameters['bs_antenna']['rotation'] = np.array([0, 30, 90]) # ULA of 32 elements\n",
    "#parameters['ue_antenna']['rotation'] = np.array([[0, 30], [30, 60], [60, 90]]) # ULA of 32 elements\n",
    "#parameters['ue_antenna']['radiation_pattern'] = 'isotropic'\n",
    "#parameters['bs_antenna']['radiation_pattern'] = 'halfwave-dipole'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9e7bb3-7cb7-4742-a949-9dd84ca31a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset setting (chunked on‑the‑fly generation)\n",
    "import time, gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 0~999 scene index , process 50 at that time\n",
    "scene_indices = np.arange(scene)\n",
    "chunk_size   = 5\n",
    "all_data     = []\n",
    "\n",
    "# Call generate_data for each scene chunk\n",
    "for i in tqdm(range(0, len(scene_indices), chunk_size)):\n",
    "    chunk = scene_indices[i : i+chunk_size].tolist()\n",
    "    parameters['dynamic_scenario_scenes'] = chunk\n",
    "\n",
    "    start = time.time()\n",
    "    data_chunk = DeepMIMOv3.generate_data(parameters)\n",
    "    print(f\"Scenes {chunk[0]}–{chunk[-1]} generation time: {time.time() - start:.2f}s\")\n",
    "\n",
    "    # combine all_data or save in the Disk\n",
    "    all_data.extend(data_chunk)\n",
    "\n",
    "    # free memory \n",
    "    del data_chunk\n",
    "    gc.collect()\n",
    "\n",
    "# comvine Dataset\n",
    "dataset = all_data\n",
    "\n",
    "\n",
    "print(parameters['user_rows'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc75e14-9605-49b0-8034-2a1dc367f97e",
   "metadata": {},
   "source": [
    "## About Information\n",
    "User : 737\n",
    "UE antenna : 1\n",
    "BS antenna : 32  Shape(a+bj)\n",
    "subcarrier : 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48148074-4682-4ab8-acbe-71ef042a5020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unmasked Data Model(gru\n",
    "# separate maksed data and unmasked data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe56d11-0ebb-4c13-b4c2-61fadd9c714b",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ba0a2d3-ee35-43dc-b457-bd400e5b75c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnMaskedChannelSeqDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    IterableDataset (un-masked version)\n",
    "\n",
    "    * Task : predict the next-step channel vector from the past `seq_len` steps.\n",
    "    * Pipeline\n",
    "        1. **Power-normalise** each complex channel vector → real + imag concat.\n",
    "        2. **Min–Max scale** inputs and targets with *one* shared scaler\n",
    "           (fitted on the same power-normalised data).\n",
    "        3. Yield `(sequence, target)` pairs as `torch.FloatTensor`.\n",
    "    * Optionally reuse externally provided scalers (train/val split consistency).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        scenes,\n",
    "        seq_len: int = 5,\n",
    "        eps: float   = 1e-9,\n",
    "        scalers: tuple[MinMaxScaler, MinMaxScaler] | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scenes  = scenes\n",
    "        self.seq_len = seq_len\n",
    "        self.eps     = eps\n",
    "\n",
    "        # --- channel tensor dimensions --------------------------------------\n",
    "        ch0          = scenes[0][0]['user']['channel']        # (U, 1, A, S)\n",
    "        self.U       = ch0.shape[0]                           # users\n",
    "        self.A       = ch0.shape[2]                           # BS antennas\n",
    "        self.S       = ch0.shape[3]                           # sub-carriers\n",
    "        self.vec_len = 2 * self.A                            # real + imag\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # 1) Fit / reuse MinMax scalers on power-normalised data\n",
    "        # --------------------------------------------------------------------\n",
    "        if scalers is None:\n",
    "            X_list, y_list = [], []\n",
    "            T = len(scenes)\n",
    "            for t in range(self.seq_len, T):\n",
    "                past  = scenes[t - self.seq_len : t]\n",
    "                s_tgt = scenes[t]\n",
    "\n",
    "                for u in range(self.U):\n",
    "                    for s in range(self.S):\n",
    "                        # power-normalised sequence (seq_len, vec_len)\n",
    "                        seq_np = np.stack([\n",
    "                            self._power_norm(p[0]['user']['channel'][u, 0, :, s])\n",
    "                            for p in past\n",
    "                        ], axis=0).astype(np.float32)\n",
    "\n",
    "                        # power-normalised target   (vec_len,)\n",
    "                        tgt_np = self._power_norm(\n",
    "                            s_tgt[0]['user']['channel'][u, 0, :, s]\n",
    "                        ).astype(np.float32)\n",
    "\n",
    "                        # skip if empty (all zeros)\n",
    "                        if not np.any(seq_np) or not np.any(tgt_np):\n",
    "                            continue\n",
    "\n",
    "                        X_list.append(seq_np.reshape(-1, self.vec_len))\n",
    "                        y_list.append(tgt_np)\n",
    "\n",
    "            X_all = np.vstack(X_list)          # (N*seq_len, vec_len)\n",
    "            y_all = np.stack(y_list)           # (N, vec_len)\n",
    "            self.scaler_x = MinMaxScaler().fit(X_all)\n",
    "            self.scaler_y = MinMaxScaler().fit(y_all)\n",
    "        else:\n",
    "            # use pre-computed scalers (train/val share the same)\n",
    "            self.scaler_x, self.scaler_y = scalers\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # iterator\n",
    "    # ------------------------------------------------------------------------\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Yields:\n",
    "            seq_tensor   : FloatTensor (seq_len, vec_len)\n",
    "            target_tensor: FloatTensor (vec_len,)\n",
    "        Both tensors are power-normalised **and** Min–Max scaled.\n",
    "        \"\"\"\n",
    "        T = len(self.scenes)\n",
    "        for t in range(self.seq_len, T):\n",
    "            past  = self.scenes[t - self.seq_len : t]\n",
    "            s_tgt = self.scenes[t]\n",
    "\n",
    "            for u in range(self.U):\n",
    "                for s in range(self.S):\n",
    "                    seq_np = np.stack([\n",
    "                        self._power_norm(p[0]['user']['channel'][u, 0, :, s])\n",
    "                        for p in past\n",
    "                    ], axis=0)\n",
    "                    tgt_np = self._power_norm(\n",
    "                        s_tgt[0]['user']['channel'][u, 0, :, s]\n",
    "                    )\n",
    "\n",
    "                    if not np.any(seq_np) or not np.any(tgt_np):\n",
    "                        continue\n",
    "\n",
    "                    # identical scalers for train / val\n",
    "                    N, D = seq_np.shape\n",
    "                    seq_np = self.scaler_x.transform(seq_np.reshape(-1, D)).reshape(N, D)\n",
    "                    tgt_np = self.scaler_y.transform(tgt_np.reshape(1, -1)).reshape(-1,)\n",
    "\n",
    "                    yield torch.from_numpy(seq_np), torch.from_numpy(tgt_np)\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # helpers\n",
    "    # ------------------------------------------------------------------------\n",
    "    def _power_norm(self, h: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert complex vector → real|imag concatenation\n",
    "        and force average power to 1.\n",
    "        \"\"\"\n",
    "        v      = np.concatenate([h.real, h.imag]).astype(np.float32)\n",
    "        power  = np.mean(v * v) + self.eps\n",
    "        return v / np.sqrt(power)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Rough size estimate (not used by IterableDataset).\"\"\"\n",
    "        return (len(self.scenes) - self.seq_len) * self.U * self.S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d365ac-69a6-494c-bfc7-198bdeb29582",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedChannelSeqDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    IterableDataset for masked channel sequence data.\n",
    "\n",
    "    - Predicts the next-step channel vector from a sequence of past vectors.\n",
    "    - Applies power normalization and MinMax scaling to both inputs and targets.\n",
    "    - Masks 15% of the patches according to:\n",
    "        * 80% chance: replace selected patch with zeros\n",
    "        * 10% chance: replace selected patch with Gaussian noise\n",
    "        * 10% chance: leave the selected patch unchanged\n",
    "      The other 85% of samples are returned unmasked.\n",
    "    \"\"\"\n",
    "    def __init__(self, scenes, seq_len=5, eps=1e-9, noise_std=1.0):\n",
    "        super().__init__()\n",
    "        self.scenes    = scenes\n",
    "        self.seq_len   = seq_len\n",
    "        self.eps       = eps\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        # Determine dimensions: U=users, A=antennas, S=subcarriers\n",
    "        ch0 = scenes[0][0]['user']['channel']   # shape: (U, 1, A, S)\n",
    "        self.U       = ch0.shape[0]\n",
    "        self.A       = ch0.shape[2]\n",
    "        self.S       = ch0.shape[3]\n",
    "        self.vec_len = 2 * self.A               # real+imag concatenated\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # 1) PRECOMPUTE power-norm → fit MinMax scalers on exactly the same data\n",
    "        # ----------------------------------------------------------------------\n",
    "        X_list, y_list = [], []\n",
    "        T = len(scenes)\n",
    "        for t in range(self.seq_len, T):\n",
    "            past         = scenes[t - self.seq_len : t]\n",
    "            target_scene = scenes[t]\n",
    "            for u in range(self.U):\n",
    "                for s in range(self.S):\n",
    "                    # power-normalize each time-slice in the sequence\n",
    "                    seq_np = np.stack([\n",
    "                        self._power_norm(ps[0]['user']['channel'][u,0,:,s])\n",
    "                        for ps in past\n",
    "                    ], axis=0).astype(np.float32)  # (seq_len, vec_len)\n",
    "\n",
    "                    # power-normalize target\n",
    "                    tgt_np = self._power_norm(\n",
    "                        target_scene[0]['user']['channel'][u,0,:,s]\n",
    "                    ).astype(np.float32)            # (vec_len,)\n",
    "\n",
    "                    # skip if invalid\n",
    "                    if not np.any(seq_np) or not np.any(tgt_np):\n",
    "                        continue\n",
    "\n",
    "                    # flatten sequence for scaler\n",
    "                    X_list.append(seq_np.reshape(-1, self.vec_len))\n",
    "                    y_list.append(tgt_np)\n",
    "\n",
    "        # fit scalers on the exact same power-normalized data\n",
    "        X_all = np.vstack(X_list)  # (num_samples*seq_len, vec_len)\n",
    "        y_all = np.stack(y_list)   # (num_samples, vec_len)\n",
    "        self.scaler_x = MinMaxScaler().fit(X_all)\n",
    "        self.scaler_y = MinMaxScaler().fit(y_all)\n",
    "\n",
    "        # prepare a zero-mask vector\n",
    "        self.mask_value = torch.zeros(self.vec_len, dtype=torch.float32)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Yields:\n",
    "            seq_tensor      (seq_len, vec_len) : normalized & scaled sequence\n",
    "            masked_pos      (1,) tensor long   : index of masked time step\n",
    "            target_tensor   (vec_len,)          : normalized & scaled target\n",
    "        \"\"\"\n",
    "        T = len(self.scenes)\n",
    "        # it is not necessary mask because already pretrained about LWM \n",
    "        # so Change mask_prob = 0 \n",
    "        mask_prob  = 0\n",
    "        zero_prob  = mask_prob * 0.8\n",
    "        noise_prob = mask_prob * 0.1\n",
    "\n",
    "        for t in range(self.seq_len, T):\n",
    "            past       = self.scenes[t - self.seq_len : t]\n",
    "            scene_dict = self.scenes[t]\n",
    "\n",
    "            for u in range(self.U):\n",
    "                for s in range(self.S):\n",
    "                    # power-normalize each slice\n",
    "                    seq_np = np.stack([\n",
    "                        self._power_norm(ps[0]['user']['channel'][u,0,:,s])\n",
    "                        for ps in past\n",
    "                    ], axis=0)\n",
    "                    tgt_np = self._power_norm(\n",
    "                        scene_dict[0]['user']['channel'][u,0,:,s]\n",
    "                    )\n",
    "\n",
    "                    if not np.any(seq_np) or not np.any(tgt_np):\n",
    "                        continue\n",
    "\n",
    "                    # MinMax transform (using the same scalers)\n",
    "                    N, D = seq_np.shape\n",
    "                    seq_np = self.scaler_x.transform(seq_np.reshape(-1, D)).reshape(N, D)\n",
    "                    tgt_np = self.scaler_y.transform(tgt_np.reshape(1, -1)).reshape(-1,)\n",
    "\n",
    "                    seq_tensor   = torch.from_numpy(seq_np)\n",
    "                    target_tensor= torch.from_numpy(tgt_np)\n",
    "\n",
    "                    # select mask position\n",
    "                    mpos = random.randrange(self.seq_len)\n",
    "\n",
    "                    r = random.random()\n",
    "                    if r < zero_prob:\n",
    "                        masked_seq = seq_tensor.clone()\n",
    "                        masked_seq[mpos] = self.mask_value\n",
    "                        yield masked_seq, torch.tensor([mpos]), target_tensor\n",
    "\n",
    "                    elif r < zero_prob + noise_prob:\n",
    "                        masked_seq = seq_tensor.clone()\n",
    "                        masked_seq[mpos] = torch.randn(self.vec_len) * self.noise_std\n",
    "                        yield masked_seq, torch.tensor([mpos]), target_tensor\n",
    "\n",
    "                    elif r < mask_prob:\n",
    "                        # masked-but-unchanged\n",
    "                        yield seq_tensor, torch.tensor([mpos]), target_tensor\n",
    "\n",
    "                    else:\n",
    "                        # unmasked\n",
    "                        yield seq_tensor, torch.tensor([mpos]), target_tensor\n",
    "\n",
    "    def _power_norm(self, h: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert complex vector to real+imag concat and normalize power to 1.\n",
    "        \"\"\"\n",
    "        v = np.concatenate([h.real, h.imag]).astype(np.float32)\n",
    "        power = np.mean(v * v) + self.eps\n",
    "        return v / np.sqrt(power)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.scenes) - self.seq_len) * self.U * self.S\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e1373a-f803-49f3-b891-f9d6f59e5dc9",
   "metadata": {},
   "source": [
    "## Split Train/Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29548f28-3188-4562-bb96-634ed3bac496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❷ Train/Validation DataLoader split train : val = 6 : 4\n",
    "seq_len      = 5\n",
    "split_ratio  = 0.6\n",
    "split_idx    = int(len(dataset) * split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fd809a5-3720-4ffd-90ce-b2d5ccf541f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasked_train_ds = UnMaskedChannelSeqDataset(dataset[:split_idx], seq_len=seq_len)\n",
    "unmasked_val_ds   = UnMaskedChannelSeqDataset(dataset[split_idx:], seq_len=seq_len)\n",
    "\n",
    "# iterate over train_ds to compute min and max of features/targets\n",
    "\n",
    "batch_size   = 32\n",
    "unmasked_train_loader = DataLoader(unmasked_train_ds, batch_size=batch_size, shuffle=False)\n",
    "unmasked_val_loader   = DataLoader(unmasked_val_ds,   batch_size=batch_size, shuffle=False)\n",
    "# ─────────────────────────────────────────────\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49c214a4-1b26-49a7-b308-e16a000f0ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❷ Train/Validation DataLoader split train : val = 6 : 4\n",
    "\n",
    "masked_train_ds = MaskedChannelSeqDataset(dataset[:split_idx], seq_len=seq_len)\n",
    "masked_val_ds   = MaskedChannelSeqDataset(dataset[split_idx:], seq_len=seq_len)\n",
    "\n",
    "# iterate over train_ds to compute min and max of features/targets\n",
    "\n",
    "batch_size   = 32\n",
    "masked_train_loader = DataLoader(masked_train_ds, batch_size=batch_size, shuffle=False)\n",
    "masked_val_loader   = DataLoader(masked_val_ds,   batch_size=batch_size, shuffle=False)\n",
    "# ─────────────────────────────────────────────\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b29d65-387d-4326-b989-afe274279c38",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0383c46a-7e6c-4907-aaa4-e6b1902a770d",
   "metadata": {},
   "source": [
    "LWMWithHead: A wrapper class that uses a pre-trained LWM (Transformer encoder) as the backbone,\n",
    "             and attaches a new fully-connected (FC) head for downstream tasks\n",
    "             (regression, classification, etc.).\n",
    "\n",
    "Changes:\n",
    "- input_dim: Dimension of the actual input data (e.g., 64)\n",
    "- patch_length: Patch length expected by the backbone (e.g., 16)\n",
    "- Replaces the original element_length parameter with these two distinct parameters\n",
    "- Applies a projection layer (self.input_proj) in forward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80c33fca-38fa-4842-b886-69972e5d28a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LWMWithHead(nn.Module):\n",
    "    \"\"\"\n",
    "    LWMWithHead: A wrapper class that uses a pre-trained LWM (Transformer encoder) as the backbone,\n",
    "                 and attaches a new fully-connected (FC) head for downstream tasks\n",
    "                 (regression, classification, etc.).\n",
    "\n",
    "    Changes:\n",
    "    - input_dim: Dimension of the actual input data (e.g., 64)\n",
    "    - patch_length: Patch length expected by the backbone (e.g., 16)\n",
    "    - Replaces the original element_length parameter with these two distinct parameters\n",
    "    - Applies a projection layer (self.input_proj) in forward()\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,                 # Dimension of the actual input data (e.g., 64)\n",
    "        patch_length: int,              # Patch length expected by the backbone (e.g., 16)\n",
    "        d_model: int = 64,              # LWM hidden size\n",
    "        max_len: int = 129,             # Positional encoding max length\n",
    "        n_layers: int = 12,             # Number of Transformer encoder layers\n",
    "        hidden_dim: int = 256,          # FC head hidden dimension\n",
    "        out_dim: int = 64,              # FC head output dimension\n",
    "        freeze_backbone: bool = True,   # Whether to freeze the backbone\n",
    "        checkpoint_path: str | None = \"./model_weights.pth\",\n",
    "        device: str = \"cuda\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # apply a projection layer to match backbone's expected patch_length\n",
    "        self.input_proj = nn.Linear(input_dim, patch_length)\n",
    "\n",
    "        # initialize backbone\n",
    "        if checkpoint_path is None:\n",
    "            # randomly initialized backbone\n",
    "            self.backbone = lwm(\n",
    "                element_length=patch_length,\n",
    "                d_model=d_model,\n",
    "                max_len=max_len,\n",
    "                n_layers=n_layers\n",
    "            ).to(device)\n",
    "        else:\n",
    "            # load pre-trained weights\n",
    "            self.backbone = lwm.from_pretrained(\n",
    "                ckpt_name=checkpoint_path,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "        # freeze backbone parameters if required\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # attach a new fully-connected head for downstream tasks\n",
    "        self.head = nn.Sequential(\n",
    "            # change 2 layer -> 1 layer\n",
    "            nn.Linear(d_model, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, masked_pos: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: Tensor of shape (B, L, input_dim)\n",
    "            masked_pos: Tensor of shape (B, num_mask)\n",
    "        Returns:\n",
    "            out: Tensor of shape (B, out_dim)\n",
    "        \"\"\"\n",
    "        # project inputs to patch_length dimension\n",
    "        x = self.input_proj(input_ids)\n",
    "\n",
    "        # backbone forward: returns (logits_lm, enc_output)\n",
    "        _, enc_output = self.backbone(x, masked_pos)\n",
    "\n",
    "        # extract CLS token feature (first token)\n",
    "        feat = enc_output[:, 0, :]\n",
    "\n",
    "        # pass through FC head to get final output\n",
    "        out = self.head(feat)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2e8f0ee-6782-4eb2-b6c3-fe363ed8d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GRUWithHead(nn.Module):\n",
    "    \"\"\"\n",
    "    GRUWithHead (projected):\n",
    "      • Projects the raw feature dimension (input_dim) to a smaller patch_length\n",
    "        so every backbone receives the same patch-sized input (like LWM).\n",
    "      • Stacks N GRU layers, then an FC head for downstream tasks.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int    = 64,   # raw feature dimension coming from the DataLoader\n",
    "        patch_length: int = 16,   # target dimension fed to the GRU backbone\n",
    "        d_model: int      = 64,   # GRU hidden size\n",
    "        n_layers: int     = 12,   # number of stacked GRU layers\n",
    "        bidirectional: bool = True,\n",
    "        dropout: float      = 0.1,\n",
    "        hidden_dim: int     = 256, # FC-head hidden size\n",
    "        out_dim: int        = 64,  # FC-head output size\n",
    "        freeze_backbone: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 0) Project raw_dim → patch_length (64 → 16)\n",
    "        self.input_proj = nn.Linear(input_dim, patch_length)\n",
    "\n",
    "        # 1) GRU backbone that expects 'patch_length' features per time step\n",
    "        self.backbone = nn.GRU(\n",
    "            input_size     = patch_length,\n",
    "            hidden_size    = d_model,\n",
    "            num_layers     = n_layers,\n",
    "            batch_first    = True,\n",
    "            bidirectional  = bidirectional,\n",
    "            dropout        = dropout if n_layers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # 2) Fully-connected head\n",
    "        gru_out_dim = d_model * (2 if bidirectional else 1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(gru_out_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x : Tensor of shape (batch, seq_len, input_dim) – raw features\n",
    "        Returns:\n",
    "            Tensor of shape (batch, out_dim)\n",
    "        \"\"\"\n",
    "        # project raw features to patch_length\n",
    "        x_proj = self.input_proj(x)                 # (B, seq_len, patch_length)\n",
    "\n",
    "        # sequence modelling with GRU\n",
    "        out, _ = self.backbone(x_proj)              # (B, seq_len, num_dirs*d_model)\n",
    "\n",
    "        # use the last time-step representation\n",
    "        feat = out[:, -1, :]                        # (B, gru_out_dim)\n",
    "\n",
    "        # downstream head\n",
    "        return self.head(feat)                      # (B, out_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f9d84d1-33a1-4f34-9e97-51242fc2d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # Create positional encoding matrix of shape (1, max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            Tensor: x plus positional encodings\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, feat_dim: int, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # Optional linear projection from feat_dim to d_model\n",
    "        self.proj = nn.Linear(feat_dim, d_model) if feat_dim != d_model else None\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch, seq_len, feat_dim)\n",
    "        Returns:\n",
    "            Tensor of shape (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        if self.proj is not None:\n",
    "            x = self.proj(x)\n",
    "        return self.pos_enc(x)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dim_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        # Multi-Head Self-Attention\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        # Position-wise Feed-Forward Network\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_ff),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_ff, d_model)\n",
    "        )\n",
    "        # Layer Normalization and Dropout for residual connections\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        src_mask: torch.Tensor = None,\n",
    "        src_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (seq_len, batch, d_model)\n",
    "            src_mask: Optional Tensor of shape (seq_len, seq_len)\n",
    "            src_key_padding_mask: Optional Tensor of shape (batch, seq_len)\n",
    "        Returns:\n",
    "            Tensor of shape (seq_len, batch, d_model)\n",
    "        \"\"\"\n",
    "        # Self-attention sublayer\n",
    "        attn_out, _ = self.self_attn(x, x, x, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)\n",
    "        x = x + self.dropout1(attn_out)\n",
    "        x = self.norm1(x)\n",
    "        # Feed-forward sublayer\n",
    "        ff_out = self.ff(x)\n",
    "        x = x + self.dropout2(ff_out)\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderCustom(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feat_dim: int,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        dim_ff: int,\n",
    "        n_layers: int,\n",
    "        dropout: float = 0.1,\n",
    "        max_len: int = 5000\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Input embedding: feature projection + positional encoding\n",
    "        self.input_embedding = InputEmbedding(feat_dim, d_model, max_len)\n",
    "        # Stack of N encoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, n_heads, dim_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        src_mask: torch.Tensor = None,\n",
    "        src_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch, seq_len, feat_dim)\n",
    "        Returns:\n",
    "            Tensor of shape (seq_len, batch, d_model)\n",
    "        \"\"\"\n",
    "        x = self.input_embedding(x)       # (batch, seq_len, d_model)\n",
    "        x = x.transpose(0, 1)             # (seq_len, batch, d_model)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dim_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        # Masked Self-Attention\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        # Encoder-Decoder Attention\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        # Position-wise Feed-Forward Network\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_ff),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_ff, d_model)\n",
    "        )\n",
    "        # Layer Normalizations and Dropouts\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt: torch.Tensor,\n",
    "        memory: torch.Tensor,\n",
    "        tgt_mask: torch.Tensor = None,\n",
    "        memory_mask: torch.Tensor = None,\n",
    "        tgt_key_padding_mask: torch.Tensor = None,\n",
    "        memory_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tgt: Tensor of shape (tgt_len, batch, d_model)\n",
    "            memory: Tensor of shape (src_len, batch, d_model)\n",
    "        Returns:\n",
    "            Tensor of shape (tgt_len, batch, d_model)\n",
    "        \"\"\"\n",
    "        # Masked self-attention sublayer\n",
    "        attn1, _ = self.self_attn(\n",
    "            tgt, tgt, tgt,\n",
    "            attn_mask=tgt_mask,\n",
    "            key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "        tgt = tgt + self.dropout1(attn1)\n",
    "        tgt = self.norm1(tgt)\n",
    "        # Encoder-decoder attention sublayer\n",
    "        attn2, _ = self.multihead_attn(\n",
    "            tgt, memory, memory,\n",
    "            attn_mask=memory_mask,\n",
    "            key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "        tgt = tgt + self.dropout2(attn2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        # Feed-forward sublayer\n",
    "        ff_out = self.ff(tgt)\n",
    "        tgt = tgt + self.dropout3(ff_out)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "class TransformerDecoderCustom(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feat_dim: int,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        dim_ff: int,\n",
    "        n_layers: int,\n",
    "        dropout: float = 0.1,\n",
    "        max_len: int = 5000\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Input embedding for target sequence\n",
    "        self.input_embedding = InputEmbedding(feat_dim, d_model, max_len)\n",
    "        # Stack of N decoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, n_heads, dim_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        # Final projection back to feature dimension\n",
    "        self.output_linear = nn.Linear(d_model, feat_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt: torch.Tensor,\n",
    "        memory: torch.Tensor,\n",
    "        tgt_mask: torch.Tensor = None,\n",
    "        memory_mask: torch.Tensor = None,\n",
    "        tgt_key_padding_mask: torch.Tensor = None,\n",
    "        memory_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tgt: Tensor of shape (batch, tgt_len, feat_dim)\n",
    "            memory: Tensor of shape (src_len, batch, d_model)\n",
    "        Returns:\n",
    "            Tensor of shape (batch, tgt_len, feat_dim)\n",
    "        \"\"\"\n",
    "        x = self.input_embedding(tgt)       # (batch, tgt_len, d_model)\n",
    "        x = x.transpose(0, 1)               # (tgt_len, batch, d_model)\n",
    "        for layer in self.layers:\n",
    "            x = layer(\n",
    "                x,\n",
    "                memory,\n",
    "                tgt_mask=tgt_mask,\n",
    "                memory_mask=memory_mask,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                memory_key_padding_mask=memory_key_padding_mask\n",
    "            )\n",
    "        x = x.transpose(0, 1)               # (batch, tgt_len, d_model)\n",
    "        return self.output_linear(x)        # project back to feat_dim\n",
    "\n",
    "        \n",
    "\n",
    "class TransformerWithDecoderHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int    = 64,   # raw feature dimension\n",
    "        patch_length: int = 16,   # sequence length consumed by encoder/decoder\n",
    "        d_model: int      = 64,   # hidden size inside the transformer\n",
    "        n_heads: int      = 4,\n",
    "        dim_ff: int       = 256,\n",
    "        n_layers: int     = 12,\n",
    "        dropout: float    = 0.1,\n",
    "        hidden_dim: int   = 256,\n",
    "        out_dim: int      = 64,\n",
    "        max_len: int      = 5000,\n",
    "        freeze_backbone: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 0) Project raw input dimension to patch length\n",
    "        self.input_proj = nn.Linear(input_dim, patch_length)\n",
    "\n",
    "        # 1) Encoder: processes the source sequence\n",
    "        self.encoder = TransformerEncoderCustom(\n",
    "            feat_dim = patch_length,\n",
    "            d_model  = d_model,\n",
    "            n_heads  = n_heads,\n",
    "            dim_ff   = dim_ff,\n",
    "            n_layers = n_layers,\n",
    "            dropout  = dropout,\n",
    "            max_len  = max_len,\n",
    "        )\n",
    "        if freeze_backbone:\n",
    "            for p in self.encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # 2) Decoder: generates target sequence using encoder memory\n",
    "        self.decoder = TransformerDecoderCustom(\n",
    "            feat_dim = patch_length,\n",
    "            d_model  = d_model,\n",
    "            n_heads  = n_heads,\n",
    "            dim_ff   = dim_ff,\n",
    "            n_layers = n_layers,\n",
    "            dropout  = dropout,\n",
    "            max_len  = max_len,\n",
    "        )\n",
    "\n",
    "        # 3) Task head: maps final decoder output to desired output dimension\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,                # (batch, src_len, input_dim)\n",
    "        tgt: torch.Tensor,                # (batch, tgt_len, input_dim)\n",
    "        src_mask: torch.Tensor = None,\n",
    "        src_key_padding_mask: torch.Tensor = None,\n",
    "        tgt_mask: torch.Tensor = None,\n",
    "        tgt_key_padding_mask: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        # 1) Encode source sequence to produce memory\n",
    "        src_patch = self.input_proj(src)  # (batch, src_len, patch_length)\n",
    "        memory = self.encoder(\n",
    "            src_patch,\n",
    "            src_mask=src_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask\n",
    "        )  # (src_len, batch, d_model)\n",
    "\n",
    "        # 2) Decode target sequence using encoder memory\n",
    "        tgt_patch = self.input_proj(tgt)  # (batch, tgt_len, patch_length)\n",
    "        dec_out = self.decoder(\n",
    "            tgt_patch,\n",
    "            memory,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=None,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=src_key_padding_mask\n",
    "        )  # (batch, tgt_len, d_model)\n",
    "\n",
    "        # 3) Use last time-step output from decoder for prediction\n",
    "        last_step = dec_out[:, -1, :]      # (batch, d_model)\n",
    "        return self.head(last_step)        # (batch, out_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2503278-03dd-4b6d-8497-e9f9d83e10fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNWithHead(nn.Module):\n",
    "    \"\"\"\n",
    "    RNNWithHead (projected):\n",
    "      • Projects raw feature vectors from `input_dim` to `patch_length`\n",
    "      • Feeds the projected sequence to an RNN backbone\n",
    "      • Maps the last hidden state through an FC head\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int    = 64,   # raw feature dimension coming from DataLoader\n",
    "        patch_length: int = 16,   # dimension consumed by the RNN backbone\n",
    "        hidden_size: int  = 64,   # RNN hidden size\n",
    "        num_layers: int   = 12,   # number of stacked RNN layers\n",
    "        bidirectional: bool = True,\n",
    "        dropout: float      = 0.1,\n",
    "        hidden_dim: int     = 256, # FC-head hidden size\n",
    "        out_dim: int        = 64,  # FC-head output size\n",
    "        freeze_backbone: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 0) project raw 64-dim → 16-dim\n",
    "        self.input_proj = nn.Linear(input_dim, patch_length)\n",
    "\n",
    "        # 1) RNN backbone\n",
    "        self.backbone = nn.RNN(\n",
    "            input_size     = patch_length,\n",
    "            hidden_size    = hidden_size,\n",
    "            num_layers     = num_layers,\n",
    "            batch_first    = True,\n",
    "            bidirectional  = bidirectional,\n",
    "            dropout        = dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # 2) FC head\n",
    "        rnn_out_dim = hidden_size * (2 if bidirectional else 1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(rnn_out_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, input_dim=64)\n",
    "        returns: (batch, out_dim)\n",
    "        \"\"\"\n",
    "        x_proj = self.input_proj(x)           # (batch, seq_len, 16)\n",
    "        out, _ = self.backbone(x_proj)        # (batch, seq_len, rnn_out_dim)\n",
    "        feat   = out[:, -1, :]                # take last time step\n",
    "        return self.head(feat)                # (batch, out_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b210558c-7f51-4e6b-ad3a-8efa5237a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMWithHead(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTMWithHead (projected):\n",
    "      • Projects raw feature vectors from `input_dim` to a compact `patch_length`\n",
    "      • Feeds the projected sequence to an LSTM backbone\n",
    "      • Uses the last hidden state to drive an FC head for the downstream task\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int    = 64,   # raw feature dimension (e.g., 64)\n",
    "        patch_length: int = 16,   # dimension consumed by the LSTM backbone\n",
    "        hidden_size: int  = 64,   # LSTM hidden size\n",
    "        num_layers: int   = 12,   # number of stacked LSTM layers\n",
    "        bidirectional: bool = True,\n",
    "        dropout: float      = 0.1,\n",
    "        hidden_dim: int     = 256, # FC-head hidden size\n",
    "        out_dim: int        = 64,  # FC-head output size\n",
    "        freeze_backbone: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 0) Raw 64-dim → 16-dim patch projection\n",
    "        self.input_proj = nn.Linear(input_dim, patch_length)\n",
    "\n",
    "        # 1) LSTM backbone that expects `patch_length` features\n",
    "        self.backbone = nn.LSTM(\n",
    "            input_size     = patch_length,\n",
    "            hidden_size    = hidden_size,\n",
    "            num_layers     = num_layers,\n",
    "            batch_first    = True,\n",
    "            bidirectional  = bidirectional,\n",
    "            dropout        = dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # 2) FC head\n",
    "        lstm_out_dim = hidden_size * (2 if bidirectional else 1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(lstm_out_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, input_dim=64)\n",
    "        returns: (batch, out_dim)\n",
    "        \"\"\"\n",
    "        # project raw features to patch_length\n",
    "        x_proj = self.input_proj(x)             # (B, seq_len, 16)\n",
    "\n",
    "        # sequence modeling with LSTM\n",
    "        out, _ = self.backbone(x_proj)          # (B, seq_len, lstm_out_dim)\n",
    "\n",
    "        # take the last time-step representation\n",
    "        feat = out[:, -1, :]                    # (B, lstm_out_dim)\n",
    "\n",
    "        # downstream head\n",
    "        return self.head(feat)                  # (B, out_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4766f28-d102-4ed0-8272-7d5098ad9f15",
   "metadata": {},
   "source": [
    "## fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e104da9-d8e8-42ea-a810-3567b9ae412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────\n",
    "# Shared hyper-parameters\n",
    "# ──────────────────────────\n",
    "INPUT_DIM     = 64     # raw feature dimension\n",
    "PATCH_LENGTH  = 16     # dimension fed to every backbone\n",
    "D_MODEL       = 64     # internal hidden size (GRU/LSTM/Transformer)\n",
    "N_LAYERS      = 12     # stacked layers\n",
    "HIDDEN_DIM    = 256    # head hidden dimension\n",
    "OUT_DIM       = 64     # head output dimension\n",
    "DROPOUT       = 0.1    # dropout for recurrent / transformer blocks\n",
    "BIDIRECTIONAL = True   # use bidirectional RNNs\n",
    "DEVICE        = \"cuda\"\n",
    "\n",
    "# ──────────────────────────\n",
    "# Model class catalog\n",
    "# ──────────────────────────\n",
    "MODEL_CATALOG = {\n",
    "    \"LWM_freeze_backbone\"     : LWMWithHead,\n",
    "    \"LWM_pretrained_Fine_tune\": LWMWithHead,\n",
    "    \"LWM_Fine_tune\"           : LWMWithHead,\n",
    "    \"gru\"                     : GRUWithHead,\n",
    "    \"RNN\"                     : RNNWithHead,\n",
    "    \"LSTM\"                    : LSTMWithHead,\n",
    "    \"Transformer\"             : TransformerWithHead,\n",
    "}\n",
    "\n",
    "# ──────────────────────────\n",
    "# Per-model constructor kwargs\n",
    "# ──────────────────────────\n",
    "MODEL_PARAMS = {\n",
    "    # ── LWM variants ─────────────────────────────\n",
    "    \"LWM_freeze_backbone\": {\n",
    "        \"input_dim\"       : INPUT_DIM,\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"d_model\"         : D_MODEL,\n",
    "        \"max_len\"         : PATCH_LENGTH + 1,\n",
    "        \"n_layers\"        : N_LAYERS,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"freeze_backbone\" : True,\n",
    "        \"checkpoint_path\" : \"./model_weights.pth\",\n",
    "        \"device\"          : DEVICE,\n",
    "    },\n",
    "    \"LWM_pretrained_Fine_tune\": {\n",
    "        \"input_dim\"       : INPUT_DIM,\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"d_model\"         : D_MODEL,\n",
    "        \"max_len\"         : PATCH_LENGTH + 1,\n",
    "        \"n_layers\"        : N_LAYERS,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"freeze_backbone\" : False,\n",
    "        \"checkpoint_path\" : \"./model_weights.pth\",\n",
    "        \"device\"          : DEVICE,\n",
    "    },\n",
    "    \"LWM_Fine_tune\": {\n",
    "        \"input_dim\"       : INPUT_DIM,\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"d_model\"         : D_MODEL,\n",
    "        \"max_len\"         : PATCH_LENGTH + 1,\n",
    "        \"n_layers\"        : N_LAYERS,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"freeze_backbone\" : False,\n",
    "        \"checkpoint_path\" : None,\n",
    "        \"device\"          : DEVICE,\n",
    "    },\n",
    "\n",
    "    # ── GRU (projected) ──────────────────────────\n",
    "    \"gru\": {\n",
    "        \"input_dim\"       : INPUT_DIM,     # 64 → project → 16\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"d_model\"         : D_MODEL,\n",
    "        \"n_layers\"        : N_LAYERS,\n",
    "        \"bidirectional\"   : BIDIRECTIONAL,\n",
    "        \"dropout\"         : DROPOUT,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"freeze_backbone\" : False,\n",
    "    },\n",
    "\n",
    "    # ── Vanilla RNN (projected) ──────────────────\n",
    "    \"RNN\": {\n",
    "        \"input_dim\"       : INPUT_DIM,\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"hidden_size\"     : D_MODEL,\n",
    "        \"num_layers\"      : N_LAYERS,\n",
    "        \"bidirectional\"   : BIDIRECTIONAL,\n",
    "        \"dropout\"         : 0.0,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"freeze_backbone\" : False,\n",
    "    },\n",
    "\n",
    "    # ── LSTM (projected) ─────────────────────────\n",
    "    \"LSTM\": {\n",
    "        \"input_dim\"       : INPUT_DIM,\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"hidden_size\"     : D_MODEL,\n",
    "        \"num_layers\"      : N_LAYERS,\n",
    "        \"bidirectional\"   : BIDIRECTIONAL,\n",
    "        \"dropout\"         : DROPOUT,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"freeze_backbone\" : False,\n",
    "    },\n",
    "\n",
    "    # ── Transformer (projected) ──────────────────\n",
    "    \"Transformer\": {\n",
    "        \"input_dim\"       : INPUT_DIM,\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"d_model\"         : D_MODEL,\n",
    "        \"n_heads\"         : 4,\n",
    "        \"dim_ff\"          : 256,\n",
    "        \"n_layers\"        : N_LAYERS,\n",
    "        \"dropout\"         : DROPOUT,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"max_len\"         : PATCH_LENGTH + 1,\n",
    "        \"freeze_backbone\" : False,\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c37de2-4bc1-42aa-9a9e-cfa43bf0c3fd",
   "metadata": {},
   "source": [
    "## model evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e9e706d-4875-4ee2-93d9-f792571fe2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def rmse(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Root-Mean-Squared Error\n",
    "    \"\"\"\n",
    "    return torch.sqrt(F.mse_loss(pred, target, reduction=\"mean\"))   # √MSE\n",
    "\n",
    "def nmse(pred: torch.Tensor, target: torch.Tensor, eps : float = 1e-12) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Normalized MSE  =  E[‖ŷ − y‖²] / E[‖y‖²]\n",
    "    \"\"\"\n",
    "    # (B, …) → (B,)  \n",
    "    mse_per_sample   = ((pred - target)**2).view(pred.size(0), -1).sum(dim=1)\n",
    "    power_per_sample = (target**2).view(target.size(0), -1).sum(dim=1) + eps\n",
    "    return (mse_per_sample / power_per_sample).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3930b75e-0895-4fb1-8039-744c86730b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_evaluate(model, loader, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Validation loop for IterableDataset.\n",
    "    Returns average RMSE and NMSE over all samples.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_rmse, total_nmse, total_samples = 0.0, 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, masked_pos, target in loader:\n",
    "            # Move to device\n",
    "            input_ids, masked_pos, target = (\n",
    "                input_ids.to(device),\n",
    "                masked_pos.to(device),\n",
    "                target.to(device),\n",
    "            )\n",
    "            # Batch size\n",
    "            bs = input_ids.size(0)\n",
    "\n",
    "            # Forward\n",
    "            pred = model(input_ids, masked_pos)\n",
    "\n",
    "            # Accumulate batch metrics\n",
    "            total_rmse    += rmse(pred, target).item() * bs\n",
    "            total_nmse    += nmse(pred, target).item() * bs\n",
    "            total_samples += bs\n",
    "\n",
    "    # Compute averages\n",
    "    return {\n",
    "        \"RMSE\": total_rmse / total_samples,\n",
    "        \"NMSE\": total_nmse / total_samples\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92d4ceda-b090-4b01-91a4-698c319420f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unmasked_evaluate(model, loader, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Validation loop for IterableDataset.\n",
    "    Returns average RMSE and NMSE over all samples.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_rmse, total_nmse, total_samples = 0.0, 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, target in loader:\n",
    "            # Move to device\n",
    "            input_ids,target = (\n",
    "                input_ids.to(device),\n",
    "                target.to(device),\n",
    "            )\n",
    "            # Batch size\n",
    "            bs = input_ids.size(0)\n",
    "\n",
    "            # Forward\n",
    "            pred = model(input_ids)\n",
    "\n",
    "            # Accumulate batch metrics\n",
    "            total_rmse    += rmse(pred, target).item() * bs\n",
    "            total_nmse    += nmse(pred, target).item() * bs\n",
    "            total_samples += bs\n",
    "\n",
    "    # Compute averages\n",
    "    return {\n",
    "        \"RMSE\": total_rmse / total_samples,\n",
    "        \"NMSE\": total_nmse / total_samples\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c0dcd-d4af-4d7b-87cd-154d81d210fb",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "317fde92-dd4d-4092-b93c-7b1b83674c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training LWM_freeze_backbone ===\n",
      "Model loaded successfully from ./model_weights.pth to cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/10] TrainLoss: 0.0161  Val RMSE: 0.0937  Val NMSE: 4.6070e-02  Val NMSE_dB: -13.4 dB  TrainTime: 178.35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/10] TrainLoss: 0.0098  Val RMSE: 0.0965  Val NMSE: 4.3769e-02  Val NMSE_dB: -13.6 dB  TrainTime: 199.94s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/10] TrainLoss: 0.0085  Val RMSE: 0.0954  Val NMSE: 4.1781e-02  Val NMSE_dB: -13.8 dB  TrainTime: 197.94s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04/10] TrainLoss: 0.0079  Val RMSE: 0.0946  Val NMSE: 4.0371e-02  Val NMSE_dB: -13.9 dB  TrainTime: 191.84s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/10] TrainLoss: 0.0074  Val RMSE: 0.0947  Val NMSE: 3.9880e-02  Val NMSE_dB: -14.0 dB  TrainTime: 209.05s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/10] TrainLoss: 0.0072  Val RMSE: 0.0935  Val NMSE: 3.8744e-02  Val NMSE_dB: -14.1 dB  TrainTime: 207.86s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/10] TrainLoss: 0.0069  Val RMSE: 0.0927  Val NMSE: 3.7826e-02  Val NMSE_dB: -14.2 dB  TrainTime: 226.54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/10] TrainLoss: 0.0066  Val RMSE: 0.0917  Val NMSE: 3.6920e-02  Val NMSE_dB: -14.3 dB  TrainTime: 211.93s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/10] TrainLoss: 0.0064  Val RMSE: 0.0910  Val NMSE: 3.6269e-02  Val NMSE_dB: -14.4 dB  TrainTime: 199.85s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/10] TrainLoss: 0.0062  Val RMSE: 0.0899  Val NMSE: 3.5388e-02  Val NMSE_dB: -14.5 dB  TrainTime: 209.33s\n",
      "🕒 LWM_freeze_backbone – avg train time / epoch: 203.26s\n",
      "\n",
      "=== Training LWM_pretrained_Fine_tune ===\n",
      "Model loaded successfully from ./model_weights.pth to cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/10] TrainLoss: 0.0121  Val RMSE: 0.0935  Val NMSE: 3.9756e-02  Val NMSE_dB: -14.0 dB  TrainTime: 283.47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/10] TrainLoss: 0.0061  Val RMSE: 0.0860  Val NMSE: 3.2402e-02  Val NMSE_dB: -14.9 dB  TrainTime: 248.61s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/10] TrainLoss: 0.0048  Val RMSE: 0.0828  Val NMSE: 2.9650e-02  Val NMSE_dB: -15.3 dB  TrainTime: 243.50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04/10] TrainLoss: 0.0042  Val RMSE: 0.0826  Val NMSE: 2.9147e-02  Val NMSE_dB: -15.4 dB  TrainTime: 243.18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/10] TrainLoss: 0.0038  Val RMSE: 0.0806  Val NMSE: 2.7571e-02  Val NMSE_dB: -15.6 dB  TrainTime: 248.54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/10] TrainLoss: 0.0034  Val RMSE: 0.0768  Val NMSE: 2.5119e-02  Val NMSE_dB: -16.0 dB  TrainTime: 249.99s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/10] TrainLoss: 0.0031  Val RMSE: 0.0746  Val NMSE: 2.3762e-02  Val NMSE_dB: -16.2 dB  TrainTime: 263.29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/10] TrainLoss: 0.0029  Val RMSE: 0.0743  Val NMSE: 2.3604e-02  Val NMSE_dB: -16.3 dB  TrainTime: 251.32s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/10] TrainLoss: 0.0027  Val RMSE: 0.0734  Val NMSE: 2.3078e-02  Val NMSE_dB: -16.4 dB  TrainTime: 259.99s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/10] TrainLoss: 0.0026  Val RMSE: 0.0723  Val NMSE: 2.2374e-02  Val NMSE_dB: -16.5 dB  TrainTime: 260.29s\n",
      "🕒 LWM_pretrained_Fine_tune – avg train time / epoch: 255.22s\n",
      "\n",
      "=== Training LWM_Fine_tune ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/10] TrainLoss: 0.0100  Val RMSE: 0.0804  Val NMSE: 2.9590e-02  Val NMSE_dB: -15.3 dB  TrainTime: 257.87s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/10] TrainLoss: 0.0046  Val RMSE: 0.0755  Val NMSE: 2.5004e-02  Val NMSE_dB: -16.0 dB  TrainTime: 246.35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/10] TrainLoss: 0.0036  Val RMSE: 0.0771  Val NMSE: 2.5469e-02  Val NMSE_dB: -15.9 dB  TrainTime: 270.84s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04/10] TrainLoss: 0.0033  Val RMSE: 0.0780  Val NMSE: 2.6250e-02  Val NMSE_dB: -15.8 dB  TrainTime: 263.10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/10] TrainLoss: 0.0031  Val RMSE: 0.0775  Val NMSE: 2.6013e-02  Val NMSE_dB: -15.8 dB  TrainTime: 252.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/10] TrainLoss: 0.0029  Val RMSE: 0.0759  Val NMSE: 2.4909e-02  Val NMSE_dB: -16.0 dB  TrainTime: 255.69s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/10] TrainLoss: 0.0028  Val RMSE: 0.0749  Val NMSE: 2.4076e-02  Val NMSE_dB: -16.2 dB  TrainTime: 266.35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/10] TrainLoss: 0.0026  Val RMSE: 0.0737  Val NMSE: 2.3383e-02  Val NMSE_dB: -16.3 dB  TrainTime: 257.50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/10] TrainLoss: 0.0026  Val RMSE: 0.0740  Val NMSE: 2.3594e-02  Val NMSE_dB: -16.3 dB  TrainTime: 289.07s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/10] TrainLoss: 0.0025  Val RMSE: 0.0725  Val NMSE: 2.2655e-02  Val NMSE_dB: -16.4 dB  TrainTime: 264.57s\n",
      "🕒 LWM_Fine_tune – avg train time / epoch: 262.35s\n",
      "\n",
      "=== Training gru ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/10] TrainLoss: 0.0133  Val RMSE: 0.0965  Val NMSE: 4.7734e-02  Val NMSE_dB: -13.2 dB  TrainTime: 102.90s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/10] TrainLoss: 0.0094  Val RMSE: 0.0941  Val NMSE: 4.0900e-02  Val NMSE_dB: -13.9 dB  TrainTime: 102.45s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/10] TrainLoss: 0.0077  Val RMSE: 0.0933  Val NMSE: 3.8140e-02  Val NMSE_dB: -14.2 dB  TrainTime: 100.42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04/10] TrainLoss: 0.0056  Val RMSE: 0.0807  Val NMSE: 2.9176e-02  Val NMSE_dB: -15.3 dB  TrainTime: 97.12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/10] TrainLoss: 0.0048  Val RMSE: 0.0799  Val NMSE: 2.8064e-02  Val NMSE_dB: -15.5 dB  TrainTime: 102.83s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/10] TrainLoss: 0.0044  Val RMSE: 0.0799  Val NMSE: 2.7736e-02  Val NMSE_dB: -15.6 dB  TrainTime: 105.15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/10] TrainLoss: 0.0043  Val RMSE: 0.0792  Val NMSE: 2.7370e-02  Val NMSE_dB: -15.6 dB  TrainTime: 103.06s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/10] TrainLoss: 0.0041  Val RMSE: 0.0791  Val NMSE: 2.7319e-02  Val NMSE_dB: -15.6 dB  TrainTime: 104.34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/10] TrainLoss: 0.0040  Val RMSE: 0.0789  Val NMSE: 2.7186e-02  Val NMSE_dB: -15.7 dB  TrainTime: 100.67s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/10] TrainLoss: 0.0040  Val RMSE: 0.0776  Val NMSE: 2.6378e-02  Val NMSE_dB: -15.8 dB  TrainTime: 96.74s\n",
      "🕒 gru – avg train time / epoch: 101.57s\n",
      "\n",
      "=== Training RNN ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/10] TrainLoss: 0.0105  Val RMSE: 0.0922  Val NMSE: 3.7228e-02  Val NMSE_dB: -14.3 dB  TrainTime: 82.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/10] TrainLoss: 0.0051  Val RMSE: 0.0795  Val NMSE: 2.7211e-02  Val NMSE_dB: -15.7 dB  TrainTime: 80.35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/10] TrainLoss: 0.0036  Val RMSE: 0.0773  Val NMSE: 2.5327e-02  Val NMSE_dB: -16.0 dB  TrainTime: 84.99s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04/10] TrainLoss: 0.0031  Val RMSE: 0.0761  Val NMSE: 2.4850e-02  Val NMSE_dB: -16.0 dB  TrainTime: 81.75s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/10] TrainLoss: 0.0029  Val RMSE: 0.0757  Val NMSE: 2.4588e-02  Val NMSE_dB: -16.1 dB  TrainTime: 81.28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/10] TrainLoss: 0.0027  Val RMSE: 0.0771  Val NMSE: 2.5349e-02  Val NMSE_dB: -16.0 dB  TrainTime: 86.28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/10] TrainLoss: 0.0026  Val RMSE: 0.0763  Val NMSE: 2.4770e-02  Val NMSE_dB: -16.1 dB  TrainTime: 83.45s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/10] TrainLoss: 0.0025  Val RMSE: 0.0746  Val NMSE: 2.3767e-02  Val NMSE_dB: -16.2 dB  TrainTime: 81.03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/10] TrainLoss: 0.0025  Val RMSE: 0.0745  Val NMSE: 2.3680e-02  Val NMSE_dB: -16.3 dB  TrainTime: 82.28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/10] TrainLoss: 0.0024  Val RMSE: 0.0738  Val NMSE: 2.3224e-02  Val NMSE_dB: -16.3 dB  TrainTime: 83.54s\n",
      "🕒 RNN – avg train time / epoch: 82.70s\n",
      "\n",
      "=== Training LSTM ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/10] TrainLoss: 0.0144  Val RMSE: 0.0965  Val NMSE: 4.5628e-02  Val NMSE_dB: -13.4 dB  TrainTime: 106.42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/10] TrainLoss: 0.0098  Val RMSE: 0.0981  Val NMSE: 4.6012e-02  Val NMSE_dB: -13.4 dB  TrainTime: 110.11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/10] TrainLoss: 0.0096  Val RMSE: 0.0899  Val NMSE: 4.1869e-02  Val NMSE_dB: -13.8 dB  TrainTime: 103.74s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04/10] TrainLoss: 0.0090  Val RMSE: 0.0911  Val NMSE: 4.0407e-02  Val NMSE_dB: -13.9 dB  TrainTime: 111.23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/10] TrainLoss: 0.0086  Val RMSE: 0.0867  Val NMSE: 3.8076e-02  Val NMSE_dB: -14.2 dB  TrainTime: 108.98s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/10] TrainLoss: 0.0084  Val RMSE: 0.0897  Val NMSE: 3.9757e-02  Val NMSE_dB: -14.0 dB  TrainTime: 116.99s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/10] TrainLoss: 0.0080  Val RMSE: 0.0882  Val NMSE: 3.7880e-02  Val NMSE_dB: -14.2 dB  TrainTime: 113.07s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/10] TrainLoss: 0.0078  Val RMSE: 0.0951  Val NMSE: 4.3845e-02  Val NMSE_dB: -13.6 dB  TrainTime: 110.78s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/10] TrainLoss: 0.0078  Val RMSE: 0.0856  Val NMSE: 3.6254e-02  Val NMSE_dB: -14.4 dB  TrainTime: 115.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/10] TrainLoss: 0.0076  Val RMSE: 0.0845  Val NMSE: 3.5379e-02  Val NMSE_dB: -14.5 dB  TrainTime: 101.64s\n",
      "🕒 LSTM – avg train time / epoch: 109.80s\n",
      "\n",
      "=== Training Transformer ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/10] TrainLoss: 0.0095  Val RMSE: 0.0815  Val NMSE: 2.9474e-02  Val NMSE_dB: -15.3 dB  TrainTime: 177.27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/10] TrainLoss: 0.0041  Val RMSE: 0.0794  Val NMSE: 2.7534e-02  Val NMSE_dB: -15.6 dB  TrainTime: 180.03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/10] TrainLoss: 0.0034  Val RMSE: 0.0799  Val NMSE: 2.7257e-02  Val NMSE_dB: -15.6 dB  TrainTime: 190.83s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04/10] TrainLoss: 0.0031  Val RMSE: 0.0806  Val NMSE: 2.7254e-02  Val NMSE_dB: -15.6 dB  TrainTime: 176.64s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/10] TrainLoss: 0.0029  Val RMSE: 0.0777  Val NMSE: 2.5605e-02  Val NMSE_dB: -15.9 dB  TrainTime: 177.21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/10] TrainLoss: 0.0027  Val RMSE: 0.0774  Val NMSE: 2.5852e-02  Val NMSE_dB: -15.9 dB  TrainTime: 197.48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/10] TrainLoss: 0.0026  Val RMSE: 0.0755  Val NMSE: 2.4732e-02  Val NMSE_dB: -16.1 dB  TrainTime: 185.53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/10] TrainLoss: 0.0025  Val RMSE: 0.0739  Val NMSE: 2.3508e-02  Val NMSE_dB: -16.3 dB  TrainTime: 176.48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/10] TrainLoss: 0.0024  Val RMSE: 0.0745  Val NMSE: 2.3674e-02  Val NMSE_dB: -16.3 dB  TrainTime: 202.06s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/10] TrainLoss: 0.0023  Val RMSE: 0.0737  Val NMSE: 2.3275e-02  Val NMSE_dB: -16.3 dB  TrainTime: 253.84s\n",
      "🕒 Transformer – avg train time / epoch: 191.74s\n",
      "\n",
      "=== Summary of best NMSE(dB) by model ===\n",
      "LWM_freeze_backbone      : -14.51145928693418\n",
      "LWM_pretrained_Fine_tune : -16.502598380117625\n",
      "LWM_Fine_tune            : -16.448264367718\n",
      "gru                      : -15.787501648372423\n",
      "RNN                      : -16.340585468085763\n",
      "LSTM                     : -14.512591902959414\n",
      "Transformer              : -16.33109735752703\n",
      "\n",
      "Total training time for all models: 13561.51s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Unified training / validation script\n",
    "------------------------------------\n",
    "* Trains every architecture listed in MODEL_CATALOG\n",
    "* Chooses masked / un-masked DataLoader automatically\n",
    "* Reports per-epoch speed & validation scores\n",
    "* Saves **best** and **last** checkpoints under ./checkpoints/\n",
    "\"\"\"\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 0) Globals and hyper-parameters\n",
    "# ─────────────────────────────────────────────\n",
    "device      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion   = nn.MSELoss().to(device)\n",
    "\n",
    "NUM_EPOCHS  = 10\n",
    "LR          = 1e-4                         # learning-rate\n",
    "CKPT_DIR    = Path(\"checkpoints\")          # where *.pth files will be stored\n",
    "CKPT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "total_start = time.time()                  # wall-clock timer for *all* models\n",
    "results     = {}                           # best-epoch NMSE(dB) for every model\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 1) Train / validate each model\n",
    "# ─────────────────────────────────────────────\n",
    "for model_name, ModelCls in MODEL_CATALOG.items():\n",
    "\n",
    "    print(f\"\\n=== Training {model_name} ===\")\n",
    "    model_args = MODEL_PARAMS[model_name]\n",
    "    model      = ModelCls(**model_args).to(device)\n",
    "\n",
    "    # collect only trainable parameters\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    if len(trainable_params) == 0:\n",
    "        print(f\"⚠️  '{model_name}' has no trainable parameters — skipping.\")\n",
    "        results[model_name] = float(\"nan\")\n",
    "        continue\n",
    "\n",
    "    optimizer   = torch.optim.Adam(trainable_params, lr=LR)\n",
    "    epoch_times = []                       # per-epoch training duration\n",
    "    best_nmse   = float(\"inf\")             # track the best val-NMSE\n",
    "\n",
    "    # pick loaders / evaluation fn based on model family\n",
    "    uses_mask  = model_name.startswith(\"LWM_\")\n",
    "    tr_loader  = masked_train_loader if uses_mask else unmasked_train_loader\n",
    "    val_loader = masked_val_loader  if uses_mask else unmasked_val_loader\n",
    "    eval_fn    = masked_evaluate    if uses_mask else unmasked_evaluate\n",
    "\n",
    "    # ── EPOCH LOOP ──────────────────────────\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "\n",
    "        # ---------- TRAIN ----------\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        run_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(tr_loader,\n",
    "                    desc=f\"[{model_name} {epoch:02d}/{NUM_EPOCHS}] train\",\n",
    "                    leave=False)\n",
    "\n",
    "        for b, batch in enumerate(pbar, 1):\n",
    "            if uses_mask:\n",
    "                xb, mpos, yb = [x.to(device) for x in batch]\n",
    "                pred = model(xb, mpos).squeeze(-1)\n",
    "            else:\n",
    "                xb, yb = [x.to(device) for x in batch]\n",
    "                pred   = model(xb)\n",
    "\n",
    "            loss = criterion(pred, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            run_loss += loss.item()\n",
    "            if b % 100 == 0:\n",
    "                pbar.set_postfix(train_loss=run_loss / b)\n",
    "\n",
    "        epoch_times.append(time.time() - t0)\n",
    "        avg_train_loss = run_loss / b\n",
    "\n",
    "        # ---------- VALID ----------\n",
    "        metrics   = eval_fn(model, val_loader, device)\n",
    "        val_rmse  = metrics[\"RMSE\"]\n",
    "        val_nmse  = metrics[\"NMSE\"]\n",
    "        val_nmse_db = 10 * torch.log10(torch.tensor(val_nmse)).item()\n",
    "\n",
    "        # save best checkpoint\n",
    "        if val_nmse < best_nmse:\n",
    "            best_nmse = val_nmse\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                CKPT_DIR / f\"{model_name}_best.pth\"\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            f\"[{epoch:02d}/{NUM_EPOCHS}] \"\n",
    "            f\"TrainLoss: {avg_train_loss:.4f}  \"\n",
    "            f\"Val RMSE: {val_rmse:.4f}  \"\n",
    "            f\"Val NMSE: {val_nmse:.4e}  \"\n",
    "            f\"Val NMSE_dB: {val_nmse_db:.1f} dB  \"\n",
    "            f\"TrainTime: {epoch_times[-1]:.2f}s\"\n",
    "        )\n",
    "\n",
    "    # after all epochs – save *last* weights\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        CKPT_DIR / f\"{model_name}_last.pth\"\n",
    "    )\n",
    "\n",
    "    avg_ep_time = sum(epoch_times) / len(epoch_times)\n",
    "    print(f\"🕒 {model_name} – avg train time / epoch: {avg_ep_time:.2f}s\")\n",
    "\n",
    "    # store best NMSE_dB for the summary\n",
    "    results[model_name] = 10 * math.log10(best_nmse)\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 2) Summary\n",
    "# ─────────────────────────────────────────────\n",
    "print(\"\\n=== Summary of best NMSE(dB) by model ===\")\n",
    "for name, nmse_db in results.items():\n",
    "    print(f\"{name:25s}: {nmse_db if not math.isnan(nmse_db) else 'skipped':>6}\")\n",
    "\n",
    "print(f\"\\nTotal training time for all models: {time.time() - total_start:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e41b5c9-77dd-4300-ac44-925dd560ccfd",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac929b23-8f2c-4b46-b746-d450e45bbe3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from ./model_weights.pth to cuda\n",
      "Model loaded successfully from ./model_weights.pth to cuda\n",
      "⏱ LWM_freeze_backbone       | total  25.94s  | /batch  23.93 ms  | /sample   0.75 ms\n",
      "⏱ LWM_pretrained_Fine_tune  | total  26.06s  | /batch  24.04 ms  | /sample   0.75 ms\n",
      "⏱ LWM_Fine_tune             | total  25.77s  | /batch  23.77 ms  | /sample   0.74 ms\n",
      "⏱ gru                       | total  12.57s  | /batch  11.60 ms  | /sample   0.36 ms\n",
      "⏱ RNN                       | total  12.26s  | /batch  11.31 ms  | /sample   0.35 ms\n",
      "⏱ LSTM                      | total  12.61s  | /batch  11.64 ms  | /sample   0.36 ms\n",
      "⏱ Transformer               | total  21.13s  | /batch  19.49 ms  | /sample   0.61 ms\n",
      "\n",
      "=== Inference-time summary ===\n",
      "model                     | total [s] |  /batch [ms] |  /sample [ms]\n",
      "--------------------------------------------------------------------\n",
      "LWM_freeze_backbone       |     25.94 |        23.93 |          0.75\n",
      "LWM_pretrained_Fine_tune  |     26.06 |        24.04 |          0.75\n",
      "LWM_Fine_tune             |     25.77 |        23.77 |          0.74\n",
      "gru                       |     12.57 |        11.60 |          0.36\n",
      "RNN                       |     12.26 |        11.31 |          0.35\n",
      "LSTM                      |     12.61 |        11.64 |          0.36\n",
      "Transformer               |     21.13 |        19.49 |          0.61\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 0)  Load the *best* checkpoints into `trained_models`\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "CKPT_DIR = Path(\"checkpoints\")                 # folder with *.pth files\n",
    "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "trained_models = {}\n",
    "for name, ModelCls in MODEL_CATALOG.items():\n",
    "    ckpt_path = CKPT_DIR / f\"{name}_best.pth\"\n",
    "    if ckpt_path.exists():\n",
    "        model = ModelCls(**MODEL_PARAMS[name])\n",
    "        model.load_state_dict(torch.load(ckpt_path, map_location=\"cpu\"))\n",
    "        trained_models[name] = model          # keep on CPU for now\n",
    "    else:\n",
    "        print(f\"⚠️  {ckpt_path} not found — skipping this model.\")\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 1)  Pure-inference timing loop (no loss / labels)\n",
    "# ─────────────────────────────────────────────\n",
    "torch.backends.cudnn.benchmark = True          # let cuDNN pick fastest kernels\n",
    "INFER_TIME = {}                                # {model: (total, per_batch, per_sample)}\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    uses_mask = name.startswith(\"LWM_\")\n",
    "    v_loader  = masked_val_loader if uses_mask else unmasked_val_loader\n",
    "\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    # ― Warm-up one batch to ramp GPU clocks and cache kernels\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(v_loader))\n",
    "        if uses_mask:\n",
    "            seq, mpos, _ = [x.to(device) for x in batch]\n",
    "            _ = model(seq, mpos)\n",
    "        else:\n",
    "            seq, _ = [x.to(device) for x in batch]\n",
    "            _ = model(seq)\n",
    "\n",
    "    # ― Timed inference pass over the entire loader\n",
    "    torch.cuda.synchronize()\n",
    "    t0        = time.time()\n",
    "    n_batches = 0\n",
    "    n_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in v_loader:\n",
    "            if uses_mask:\n",
    "                seq, mpos, _ = [x.to(device) for x in batch]\n",
    "                _  = model(seq, mpos)\n",
    "                bs = seq.size(0)\n",
    "            else:\n",
    "                seq, _ = [x.to(device) for x in batch]\n",
    "                _  = model(seq)\n",
    "                bs = seq.size(0)\n",
    "\n",
    "            n_batches += 1\n",
    "            n_samples += bs\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    INFER_TIME[name] = (\n",
    "        elapsed,                # total seconds\n",
    "        elapsed / n_batches,    # seconds per batch\n",
    "        elapsed / n_samples     # seconds per sample\n",
    "    )\n",
    "\n",
    "    print(f\"⏱ {name:25s} | total {elapsed:6.2f}s  \"\n",
    "          f\"| /batch {elapsed/n_batches*1e3:6.2f} ms  \"\n",
    "          f\"| /sample {elapsed/n_samples*1e3:6.2f} ms\")\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 2)  Pretty summary table\n",
    "# ─────────────────────────────────────────────\n",
    "print(\"\\n=== Inference-time summary ===\")\n",
    "header = f\"{'model':25s} | {'total [s]':>9} | {'/batch [ms]':>12} | {'/sample [ms]':>13}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "for n, (tot, pb, ps) in INFER_TIME.items():\n",
    "    print(f\"{n:25s} | {tot:9.2f} | {pb*1e3:12.2f} | {ps*1e3:13.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5ad5758-e0d4-4a80-a591-4b82af28765d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq: min 0.0 max 0.6701632738113403    tgt: min 0.329348623752594 max 0.666236400604248\n"
     ]
    }
   ],
   "source": [
    "# after you (re)create masked_train_loader\n",
    "batch = next(iter(masked_train_loader))\n",
    "seq, mpos, tgt = batch   # seq: (B, L, D), tgt: (B, D)\n",
    "print(\n",
    "    \"seq: min\", seq.min().item(), \"max\", seq.max().item(),\n",
    "    \"   tgt: min\", tgt.min().item(), \"max\", tgt.max().item()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a90966-4bb9-40c9-a8a6-6d2435ceec06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff42f6d-3ed7-43b4-b35e-16e0d7345bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f748d3-0edc-4e77-bbf6-be96d7d8d99e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a224fa-03a3-4c41-989a-47e2327c5c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bea2d4-1027-495f-a2e3-efbc703aff5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dd53c6-5322-4744-95d6-90cbe8da1d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9f299d-07db-4e49-aac5-ebddf12c7538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
