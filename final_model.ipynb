{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0939df15-abc7-40e7-83bf-f12820dd42f2",
   "metadata": {},
   "source": [
    "# All Model saves here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cbf7fb-304f-432d-a3f9-580052d5a375",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e94396-1d80-4df3-8648-19368f82c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import DeepMIMOv3\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "plt . rcParams [ 'figure.figsize' ]  =  [ 12 ,  8 ]  # 기본 플롯 크기 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb2d8b4-7d3d-43ae-b997-2a4e6eab9a1a",
   "metadata": {},
   "source": [
    "## GPU Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a6479f7-d2b2-4632-b3a8-ffe3c49a6608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# GPU 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea87b9c9-a4b1-4442-b26f-606828c4dc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.6\n",
      "90501\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)                   \n",
    "print(torch.backends.cudnn.version())       \n",
    "print(\"CUDA available:\", torch.cuda.is_available())  # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dc1a47-3bc0-4236-813e-c7be4c21bd41",
   "metadata": {},
   "source": [
    "## DeepMIMOv3 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "821eb69b-806e-4694-adf9-fd0efcbf5e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = DeepMIMOv3.default_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1d1f75d-54c0-4198-990f-15eaeec28e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change parameters for the setup\n",
    "# Scenario O1_60 extracted at the dataset_folder\n",
    "#LWM dynamic senario\n",
    "# parameters['dataset_folder'] = r'/content/drive/MyDrive/Colab Notebooks/LWM'\n",
    "scene = 15 # scene 15\n",
    "# change my linux route\n",
    "parameters['dataset_folder'] = '/home/dlghdbs200/LWM'\n",
    "\n",
    "# scnario = 02_dyn_3p5 <- download file\n",
    "parameters['scenario'] = 'O2_dyn_3p5'\n",
    "parameters['dynamic_scenario_scenes'] = np.arange(scene) #scene 0~9\n",
    "\n",
    "# Up to 10 multipath paths per user-to-base station channel\n",
    "parameters['num_paths'] = 10\n",
    "\n",
    "# User rows 1-100\n",
    "parameters['user_rows'] = np.arange(100)\n",
    "# User subsampling\n",
    "parameters['user_subsampling'] = 0.01\n",
    "\n",
    "# Activate only the first basestation\n",
    "parameters['active_BS'] = np.array([1])\n",
    "\n",
    "parameters['activate_OFDM'] = 1\n",
    "\n",
    "parameters['OFDM']['bandwidth'] = 0.05 # 50 MHz\n",
    "parameters['OFDM']['subcarriers'] = 512 # OFDM with 512 subcarriers\n",
    "parameters['OFDM']['selected_subcarriers'] = np.arange(0, 64, 1)\n",
    "#parameters['OFDM']['subcarriers_limit'] = 64 # Keep only first 64 subcarriers\n",
    "\n",
    "parameters['ue_antenna']['shape'] = np.array([1, 1]) # Single antenna\n",
    "parameters['bs_antenna']['shape'] = np.array([1, 32]) # ULA of 32 elements\n",
    "#parameters['bs_antenna']['rotation'] = np.array([0, 30, 90]) # ULA of 32 elements\n",
    "#parameters['ue_antenna']['rotation'] = np.array([[0, 30], [30, 60], [60, 90]]) # ULA of 32 elements\n",
    "#parameters['ue_antenna']['radiation_pattern'] = 'isotropic'\n",
    "#parameters['bs_antenna']['radiation_pattern'] = 'halfwave-dipole'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a9e7bb3-7cb7-4742-a949-9dd84ca31a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                             | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following parameters seem unnecessary:\n",
      "{'activate_OFDM'}\n",
      "\n",
      "Scene 1/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  39%|████████████████████▍                               | 27179/69006 [00:00<00:00, 271750.74it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 272716.01it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 6164.60it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5203.85it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 592.75it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 2/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  40%|████████████████████▊                               | 27596/69006 [00:00<00:00, 275943.82it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 277390.08it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 6172.08it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4650.00it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 213.37it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 3/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  40%|████████████████████▊                               | 27620/69006 [00:00<00:00, 276181.17it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 276871.32it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 6335.37it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 2400.86it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 557.16it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 4/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  38%|███████████████████▋                                | 26091/69006 [00:00<00:00, 260879.77it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 269747.83it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 6210.00it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1912.59it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 538.98it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 5/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  39%|████████████████████▌                               | 27229/69006 [00:00<00:00, 272267.54it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 273997.84it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 6019.40it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4686.37it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 264.78it/s]\u001b[A\n",
      " 33%|████████████████████████████▎                                                        | 1/3 [00:07<00:15,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenes 0–4 generation time: 7.56s\n",
      "The following parameters seem unnecessary:\n",
      "{'activate_OFDM'}\n",
      "\n",
      "Scene 1/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  39%|████████████████████                                | 26610/69006 [00:00<00:00, 266069.80it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 260655.75it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 5934.86it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5562.74it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 288.41it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 2/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  38%|███████████████████▉                                | 26417/69006 [00:00<00:00, 264145.69it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 263237.15it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 5835.36it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 2139.95it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 516.16it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 3/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  40%|████████████████████▊                               | 27586/69006 [00:00<00:00, 275841.85it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 270010.80it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 5793.83it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4457.28it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 358.89it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 4/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:   8%|████▍                                                  | 5495/69006 [00:02<00:33, 1902.54it/s]\u001b[A\n",
      "Reading ray-tracing:  45%|████████████████████████                             | 31265/69006 [00:02<00:02, 13958.32it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|█████████████████████████████████████████████████████| 69006/69006 [00:03<00:00, 22048.38it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 5914.88it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4733.98it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 646.27it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 5/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  39%|████████████████████▍                               | 27176/69006 [00:00<00:00, 271736.29it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 268112.88it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 5487.77it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 2176.60it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 613.38it/s]\u001b[A\n",
      " 67%|████████████████████████████████████████████████████████▋                            | 2/3 [00:18<00:09,  9.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenes 5–9 generation time: 10.12s\n",
      "The following parameters seem unnecessary:\n",
      "{'activate_OFDM'}\n",
      "\n",
      "Scene 1/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  39%|████████████████████                                | 26654/69006 [00:00<00:00, 266523.10it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 261302.18it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 5809.61it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 3039.35it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 686.92it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 2/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  39%|████████████████████▏                               | 26844/69006 [00:00<00:00, 268421.06it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 268846.56it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 5714.71it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4185.93it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 779.90it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 3/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  38%|███████████████████▉                                | 26428/69006 [00:00<00:00, 264262.61it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 263775.01it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 5713.83it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 3137.10it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 688.72it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 4/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  39%|████████████████████▏                               | 26828/69006 [00:00<00:00, 268250.20it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 271252.53it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 5658.98it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5809.29it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 786.63it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 5/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  40%|████████████████████▌                               | 27347/69006 [00:00<00:00, 273447.44it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 271154.69it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 5763.44it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 6105.25it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 686.69it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:25<00:00,  8.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenes 10–14 generation time: 7.24s\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## dataset setting (chunked on‑the‑fly generation)\n",
    "import time, gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 0~999 scene index , process 50 at that time\n",
    "scene_indices = np.arange(scene)\n",
    "chunk_size   = 5\n",
    "all_data     = []\n",
    "\n",
    "# Call generate_data for each scene chunk\n",
    "for i in tqdm(range(0, len(scene_indices), chunk_size)):\n",
    "    chunk = scene_indices[i : i+chunk_size].tolist()\n",
    "    parameters['dynamic_scenario_scenes'] = chunk\n",
    "\n",
    "    start = time.time()\n",
    "    data_chunk = DeepMIMOv3.generate_data(parameters)\n",
    "    print(f\"Scenes {chunk[0]}–{chunk[-1]} generation time: {time.time() - start:.2f}s\")\n",
    "\n",
    "    # combine all_data or save in the Disk\n",
    "    all_data.extend(data_chunk)\n",
    "\n",
    "    # free memory \n",
    "    del data_chunk\n",
    "    gc.collect()\n",
    "\n",
    "# comvine Dataset\n",
    "dataset = all_data\n",
    "\n",
    "\n",
    "print(parameters['user_rows'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc75e14-9605-49b0-8034-2a1dc367f97e",
   "metadata": {},
   "source": [
    "## About Information\n",
    "User : 737\n",
    "UE antenna : 1\n",
    "BS antenna : 32  Shape(a+bj)\n",
    "subcarrier : 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48148074-4682-4ab8-acbe-71ef042a5020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unmasked Data Model(gru\n",
    "# separate maksed data and unmasked data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe56d11-0ebb-4c13-b4c2-61fadd9c714b",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ba0a2d3-ee35-43dc-b457-bd400e5b75c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class UnMaskedChannelSeqDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    IterableDataset for masked channel sequence data.\n",
    "    - Predicts the next-step channel vector from a sequence of past vectors.\n",
    "    - Applies power normalization and MinMax scaling to both inputs and targets.\n",
    "    \"\"\"\n",
    "    def __init__(self, scenes, seq_len=5, eps=1e-9):\n",
    "        super().__init__()\n",
    "        self.scenes = scenes\n",
    "        self.seq_len = seq_len\n",
    "        self.eps = eps\n",
    "\n",
    "        # Determine dimensions: users (U), antennas (A), subcarriers (S), and vector length\n",
    "        ch0 = scenes[0][0]['user']['channel']  # Example shape: (U, 1, A, S), complex values\n",
    "        self.U = ch0.shape[0]                  # Number of users\n",
    "        self.A = ch0.shape[2]                  # Number of antennas\n",
    "        self.S = ch0.shape[3]                  # Number of subcarriers\n",
    "        self.vec_len = 2 * self.A              # Real+imag length after concatenation\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # Precompute MinMax scaler on entire dataset\n",
    "        # ----------------------------------------------------------------------\n",
    "        X_list, y_list = [], []\n",
    "        T = len(scenes)\n",
    "        # Slide over time index to collect sequences and targets\n",
    "        for t in range(self.seq_len, T):\n",
    "            past = scenes[t - self.seq_len : t]\n",
    "            target = scenes[t]\n",
    "            for u in range(self.U):\n",
    "                for s in range(self.S):\n",
    "                    # Build numpy sequence of shape (seq_len, vec_len)\n",
    "                    seq_np = np.stack([\n",
    "                        np.concatenate([\n",
    "                            ps[0]['user']['channel'][u, 0, :, s].real,\n",
    "                            ps[0]['user']['channel'][u, 0, :, s].imag\n",
    "                        ])\n",
    "                        for ps in past\n",
    "                    ], axis=0).astype(np.float32)\n",
    "\n",
    "                    # Build numpy target of shape (vec_len,)\n",
    "                    target_np = np.concatenate([\n",
    "                        target[0]['user']['channel'][u, 0, :, s].real,\n",
    "                        target[0]['user']['channel'][u, 0, :, s].imag\n",
    "                    ]).astype(np.float32)\n",
    "\n",
    "                    # Skip if all zeros (invalid data)\n",
    "                    if not np.any(seq_np) or not np.any(target_np):\n",
    "                        continue\n",
    "\n",
    "                    # Flatten sequence for fitting scaler\n",
    "                    X_list.append(seq_np.reshape(-1, self.vec_len))\n",
    "                    y_list.append(target_np)\n",
    "\n",
    "        # Stack all data for fitting the MinMax scaler\n",
    "        X_all = np.vstack(X_list)  # Shape: (num_samples*seq_len, vec_len)\n",
    "        y_all = np.stack(y_list)   # Shape: (num_samples, vec_len)\n",
    "\n",
    "        # Fit MinMax scalers for inputs and targets\n",
    "        self.scaler_x = MinMaxScaler().fit(X_all)\n",
    "        self.scaler_y = MinMaxScaler().fit(y_all)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Yield power-normalized and MinMax-scaled sequences, mask positions, and targets.\n",
    "        Each item: (seq_tensor, masked_pos_tensor, target_tensor)\n",
    "        Shapes: seq_tensor (seq_len, vec_len), masked_pos_tensor (1,), target_tensor (vec_len,)\n",
    "        \"\"\"\n",
    "        T = len(self.scenes)\n",
    "        for t in range(self.seq_len, T):\n",
    "            past = self.scenes[t - self.seq_len : t]\n",
    "            target = self.scenes[t]\n",
    "            for u in range(self.U):\n",
    "                for s in range(self.S):\n",
    "                    # Compute power-normalized numpy arrays\n",
    "                    seq_np = np.stack([\n",
    "                        self._power_norm(ps[0]['user']['channel'][u, 0, :, s])\n",
    "                        for ps in past\n",
    "                    ], axis=0)\n",
    "                    target_np = self._power_norm(target[0]['user']['channel'][u, 0, :, s])\n",
    "\n",
    "                    # Skip sequences or targets that are all zero\n",
    "                    if not np.any(seq_np) or not np.any(target_np):\n",
    "                        continue\n",
    "\n",
    "                    # Apply MinMax scaling: reshape, transform, and reshape back\n",
    "                    N, D = seq_np.shape\n",
    "                    seq_np = self.scaler_x.transform(seq_np.reshape(-1, D)).reshape(N, D)\n",
    "                    target_np = self.scaler_y.transform(target_np.reshape(1, -1)).reshape(-1,)\n",
    "\n",
    "                    # Convert to torch tensors and yield with masked position\n",
    "                    seq = torch.from_numpy(seq_np)\n",
    "                    target = torch.from_numpy(target_np)\n",
    "                    yield seq, target\n",
    "\n",
    "    def _power_norm(self, h: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert complex-valued vector to concatenated real-imag vector and normalize power to 1.\n",
    "        \"\"\"\n",
    "        v = np.concatenate([h.real, h.imag]).astype(np.float32)\n",
    "        power = np.mean(v * v) + self.eps\n",
    "        return v / np.sqrt(power)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of valid (sequence, target) pairs in the dataset.\n",
    "        \"\"\"\n",
    "        return (len(self.scenes) - self.seq_len) * self.U * self.S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97d365ac-69a6-494c-bfc7-198bdeb29582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "class MaskedChannelSeqDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    IterableDataset for masked channel sequence data.\n",
    "    - Predicts the next-step channel vector from a sequence of past vectors.\n",
    "    - Applies power normalization and MinMax scaling to both inputs and targets.\n",
    "    - MCM is 15% about the all data\n",
    "    - MCM \n",
    "      :80% probability: replace the selected patch entirely with a fixed mask vector m (e.g., a vector of zeros)\n",
    "      :10% probability: replace it with a random noise vector sampled from a normal distribution (e.g., N(0, σ²))\n",
    "      :10% probability: leave the original patch unchanged\n",
    "    \"\"\"\n",
    "    def __init__(self, scenes, seq_len=5, eps=1e-9, noise_std = 1.0):\n",
    "        super().__init__()\n",
    "        self.scenes = scenes\n",
    "        self.seq_len = seq_len\n",
    "        self.eps = eps\n",
    "\n",
    "        # Determine dimensions: users (U), antennas (A), subcarriers (S), and vector length\n",
    "        ch0 = scenes[0][0]['user']['channel']  # Example shape: (U, 1, A, S), complex values\n",
    "        self.U = ch0.shape[0]                  # Number of users\n",
    "        self.A = ch0.shape[2]                  # Number of antennas\n",
    "        self.S = ch0.shape[3]                  # Number of subcarriers\n",
    "        self.vec_len = 2 * self.A              # Real+imag length after concatenation\n",
    "\n",
    "        # masked parameter\n",
    "        self.mask_value = torch.zeros(self.vec_len, dtype=torch.float32)  \n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # Precompute MinMax scaler on entire dataset\n",
    "        # ----------------------------------------------------------------------\n",
    "        X_list, y_list = [], []\n",
    "        T = len(scenes)\n",
    "        # Slide over time index to collect sequences and targets\n",
    "        for t in range(self.seq_len, T):\n",
    "            past = scenes[t - self.seq_len : t]\n",
    "            target = scenes[t]\n",
    "            mpos = random.randrange(self.seq_len)\n",
    "            for u in range(self.U):\n",
    "                for s in range(self.S):\n",
    "                    # Build numpy sequence of shape (seq_len, vec_len)\n",
    "                    seq_np = np.stack([\n",
    "                        np.concatenate([\n",
    "                            ps[0]['user']['channel'][u, 0, :, s].real,\n",
    "                            ps[0]['user']['channel'][u, 0, :, s].imag\n",
    "                        ])\n",
    "                        for ps in past\n",
    "                    ], axis=0).astype(np.float32)\n",
    "\n",
    "                    # Build numpy target of shape (vec_len,)\n",
    "                    target_np = np.concatenate([\n",
    "                        target[0]['user']['channel'][u, 0, :, s].real,\n",
    "                        target[0]['user']['channel'][u, 0, :, s].imag\n",
    "                    ]).astype(np.float32)\n",
    "\n",
    "                    # Skip if all zeros (invalid data)\n",
    "                    if not np.any(seq_np) or not np.any(target_np):\n",
    "                        continue\n",
    "\n",
    "                    # Flatten sequence for fitting scaler\n",
    "                    X_list.append(seq_np.reshape(-1, self.vec_len))\n",
    "                    y_list.append(target_np)\n",
    "\n",
    "        # Stack all data for fitting the MinMax scaler\n",
    "        X_all = np.vstack(X_list)  # Shape: (num_samples*seq_len, vec_len)\n",
    "        y_all = np.stack(y_list)   # Shape: (num_samples, vec_len)\n",
    "\n",
    "        # Fit MinMax scalers for inputs and targets\n",
    "        self.scaler_x = MinMaxScaler().fit(X_all)\n",
    "        self.scaler_y = MinMaxScaler().fit(y_all)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Yield power-normalized and MinMax-scaled sequences, mask positions, and targets.\n",
    "        Each item: (seq_tensor, masked_pos_tensor, target_tensor)\n",
    "        Shapes: seq_tensor (seq_len, vec_len), masked_pos_tensor (1,), target_tensor (vec_len,)\n",
    "        \"\"\"\n",
    "        T = len(self.scenes)\n",
    "        for t in range(self.seq_len, T):\n",
    "            past = self.scenes[t - self.seq_len : t]\n",
    "            target = self.scenes[t]\n",
    "            for u in range(self.U):\n",
    "                for s in range(self.S):\n",
    "                    # Compute power-normalized numpy arrays\n",
    "                    seq_np = np.stack([\n",
    "                        self._power_norm(ps[0]['user']['channel'][u, 0, :, s])\n",
    "                        for ps in past\n",
    "                    ], axis=0)\n",
    "                    target_np = self._power_norm(target[0]['user']['channel'][u, 0, :, s])\n",
    "\n",
    "                    # Skip sequences or targets that are all zero\n",
    "                    if not np.any(seq_np) or not np.any(target_np):\n",
    "                        continue\n",
    "\n",
    "                    # Apply MinMax scaling: reshape, transform, and reshape back\n",
    "                    N, D = seq_np.shape\n",
    "                    seq_np = self.scaler_x.transform(seq_np.reshape(-1, D)).reshape(N, D)\n",
    "                    target_np = self.scaler_y.transform(target_np.reshape(1, -1)).reshape(-1,)\n",
    "\n",
    "                    # Convert to torch tensors and yield with masked position\n",
    "                    seq = torch.from_numpy(seq_np)\n",
    "                    target = torch.from_numpy(target_np)\n",
    "\n",
    "                    # select mask position\n",
    "                    mpos = random.randrange(self.seq_len)\n",
    "\n",
    "                    # 80/10/10 rules\n",
    "                    if random.random() < 0.15:\n",
    "                        # select mpos position\n",
    "                        mpos = random.randrange(self.seq_len)\n",
    "\n",
    "                        # 80/10/10\n",
    "                        r = random.random()\n",
    "                        seq_masked = seq.clone()\n",
    "                        \n",
    "                        if r < 0.8:\n",
    "                            # 80% full masked\n",
    "                            seq_masked[mpos] = self.mask_value\n",
    "                        elif r < 0.9:\n",
    "                            # 10% random noise -> std\n",
    "                            seq_masked[mpos] = torch.randn(self.vec_len) * self.noise_std\n",
    "                        \n",
    "                        yield seq_masked, torch.tensor([mpos], dtype=torch.long), target\n",
    "\n",
    "    def _power_norm(self, h: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert complex-valued vector to concatenated real-imag vector and normalize power to 1.\n",
    "        \"\"\"\n",
    "        v = np.concatenate([h.real, h.imag]).astype(np.float32)\n",
    "        power = np.mean(v * v) + self.eps\n",
    "        return v / np.sqrt(power)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of valid (sequence, target) pairs in the dataset.\n",
    "        \"\"\"\n",
    "        return (len(self.scenes) - self.seq_len) * self.U * self.S\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e1373a-f803-49f3-b891-f9d6f59e5dc9",
   "metadata": {},
   "source": [
    "## Split Train/Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29548f28-3188-4562-bb96-634ed3bac496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❷ Train/Validation DataLoader split train : val = 6 : 4\n",
    "seq_len      = 5\n",
    "split_ratio  = 0.6\n",
    "split_idx    = int(len(dataset) * split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fd809a5-3720-4ffd-90ce-b2d5ccf541f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasked_train_ds = UnMaskedChannelSeqDataset(dataset[:split_idx], seq_len=seq_len)\n",
    "unmasked_val_ds   = UnMaskedChannelSeqDataset(dataset[split_idx:], seq_len=seq_len)\n",
    "\n",
    "# iterate over train_ds to compute min and max of features/targets\n",
    "\n",
    "batch_size   = 32\n",
    "unmasked_train_loader = DataLoader(unmasked_train_ds, batch_size=batch_size, shuffle=False)\n",
    "unmasked_val_loader   = DataLoader(unmasked_val_ds,   batch_size=batch_size, shuffle=False)\n",
    "# ─────────────────────────────────────────────\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49c214a4-1b26-49a7-b308-e16a000f0ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❷ Train/Validation DataLoader split train : val = 6 : 4\n",
    "\n",
    "masked_train_ds = MaskedChannelSeqDataset(dataset[:split_idx], seq_len=seq_len)\n",
    "masked_val_ds   = MaskedChannelSeqDataset(dataset[split_idx:], seq_len=seq_len)\n",
    "\n",
    "# iterate over train_ds to compute min and max of features/targets\n",
    "\n",
    "batch_size   = 32\n",
    "masked_train_loader = DataLoader(masked_train_ds, batch_size=batch_size, shuffle=False)\n",
    "masked_val_loader   = DataLoader(masked_val_ds,   batch_size=batch_size, shuffle=False)\n",
    "# ─────────────────────────────────────────────\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b29d65-387d-4326-b989-afe274279c38",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0383c46a-7e6c-4907-aaa4-e6b1902a770d",
   "metadata": {},
   "source": [
    "LWMWithHead: A wrapper class that uses a pre-trained LWM (Transformer encoder) as the backbone,\n",
    "             and attaches a new fully-connected (FC) head for downstream tasks\n",
    "             (regression, classification, etc.).\n",
    "\n",
    "Changes:\n",
    "- input_dim: Dimension of the actual input data (e.g., 64)\n",
    "- patch_length: Patch length expected by the backbone (e.g., 16)\n",
    "- Replaces the original element_length parameter with these two distinct parameters\n",
    "- Applies a projection layer (self.input_proj) in forward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80c33fca-38fa-4842-b886-69972e5d28a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from lwm_model import lwm\n",
    "\n",
    "class LWMWithHead(nn.Module):\n",
    "    \"\"\"\n",
    "    LWMWithHead: A wrapper class that uses a pre-trained LWM (Transformer encoder) as the backbone,\n",
    "                 and attaches a new fully-connected (FC) head for downstream tasks\n",
    "                 (regression, classification, etc.).\n",
    "\n",
    "    Changes:\n",
    "    - input_dim: Dimension of the actual input data (e.g., 64)\n",
    "    - patch_length: Patch length expected by the backbone (e.g., 16)\n",
    "    - Replaces the original element_length parameter with these two distinct parameters\n",
    "    - Applies a projection layer (self.input_proj) in forward()\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,                 # Dimension of the actual input data (e.g., 64)\n",
    "        patch_length: int,              # Patch length expected by the backbone (e.g., 16)\n",
    "        d_model: int = 64,              # LWM hidden size\n",
    "        max_len: int = 129,             # Positional encoding max length\n",
    "        n_layers: int = 12,             # Number of Transformer encoder layers\n",
    "        hidden_dim: int = 256,          # FC head hidden dimension\n",
    "        out_dim: int = 64,              # FC head output dimension\n",
    "        freeze_backbone: bool = True,   # Whether to freeze the backbone\n",
    "        checkpoint_path: str | None = \"./model_weights.pth\",\n",
    "        device: str = \"cuda\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # apply a projection layer to match backbone's expected patch_length\n",
    "        self.input_proj = nn.Linear(input_dim, patch_length)\n",
    "\n",
    "        # initialize backbone\n",
    "        if checkpoint_path is None:\n",
    "            # randomly initialized backbone\n",
    "            self.backbone = lwm(\n",
    "                element_length=patch_length,\n",
    "                d_model=d_model,\n",
    "                max_len=max_len,\n",
    "                n_layers=n_layers\n",
    "            ).to(device)\n",
    "        else:\n",
    "            # load pre-trained weights\n",
    "            self.backbone = lwm.from_pretrained(\n",
    "                ckpt_name=checkpoint_path,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "        # freeze backbone parameters if required\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # attach a new fully-connected head for downstream tasks\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, masked_pos: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: Tensor of shape (B, L, input_dim)\n",
    "            masked_pos: Tensor of shape (B, num_mask)\n",
    "        Returns:\n",
    "            out: Tensor of shape (B, out_dim)\n",
    "        \"\"\"\n",
    "        # project inputs to patch_length dimension\n",
    "        x = self.input_proj(input_ids)\n",
    "\n",
    "        # backbone forward: returns (logits_lm, enc_output)\n",
    "        _, enc_output = self.backbone(x, masked_pos)\n",
    "\n",
    "        # extract CLS token feature (first token)\n",
    "        feat = enc_output[:, 0, :]\n",
    "\n",
    "        # pass through FC head to get final output\n",
    "        out = self.head(feat)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2e8f0ee-6782-4eb2-b6c3-fe363ed8d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GRUWithHead(nn.Module):\n",
    "    \"\"\"\n",
    "    GRUWithHead: A wrapper class that uses a GRU backbone and attaches a fully-connected (FC) head\n",
    "                 for downstream tasks (regression, classification, etc.).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        feat_dim: int = 16,           # Dimension of input features (patch_length / element_length)\n",
    "        d_model: int = 64,            # GRU hidden size\n",
    "        n_layers: int = 12,           # Number of GRU layers to stack\n",
    "        bidirectional: bool = True,   # Whether to use a bidirectional GRU\n",
    "        dropout: float = 0.1,         # Dropout probability between GRU layers\n",
    "        hidden_dim: int = 256,        # FC head hidden dimension\n",
    "        out_dim: int = 64,            # FC head output dimension\n",
    "        freeze_backbone: bool = False # Whether to freeze GRU backbone weights\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) GRU backbone\n",
    "        self.backbone = nn.GRU(\n",
    "            input_size   = feat_dim,\n",
    "            hidden_size  = d_model,\n",
    "            num_layers   = n_layers,\n",
    "            batch_first  = True,\n",
    "            bidirectional= bidirectional,\n",
    "            dropout      = dropout if n_layers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "        # 2) Optionally freeze backbone parameters\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # Build FC head \n",
    "        gru_out_dim = d_model * (2 if bidirectional else 1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(gru_out_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, feat_dim)\n",
    "        Returns:\n",
    "            out: Tensor of shape (batch_size, out_dim)\n",
    "        \"\"\"\n",
    "        # 1) Pass through GRU backbone\n",
    "        out, _ = self.backbone(x)  # out shape: (B, seq_len, num_directions * d_model)\n",
    "\n",
    "        # 2) Take the last time-step output as sequence representation\n",
    "        feat = out[:, -1, :]       # shape: (B, gru_out_dim)\n",
    "\n",
    "        # 3) Pass through FC head to get final output\n",
    "        return self.head(feat)     # shape: (B, out_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f9d84d1-33a1-4f34-9e97-51242fc2d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # Create positional encoding matrix of shape (1, max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            Tensor: x plus positional encodings\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, feat_dim: int, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # Optional linear projection from feat_dim to d_model\n",
    "        self.proj = nn.Linear(feat_dim, d_model) if feat_dim != d_model else None\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch, seq_len, feat_dim)\n",
    "        Returns:\n",
    "            Tensor of shape (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        if self.proj is not None:\n",
    "            x = self.proj(x)\n",
    "        return self.pos_enc(x)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dim_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        # Multi-Head Self-Attention\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        # Position-wise Feed-Forward Network\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_ff),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_ff, d_model)\n",
    "        )\n",
    "        # Layer Normalization and Dropout for residual connections\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        src_mask: torch.Tensor = None,\n",
    "        src_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (seq_len, batch, d_model)\n",
    "            src_mask: Optional Tensor of shape (seq_len, seq_len)\n",
    "            src_key_padding_mask: Optional Tensor of shape (batch, seq_len)\n",
    "        Returns:\n",
    "            Tensor of shape (seq_len, batch, d_model)\n",
    "        \"\"\"\n",
    "        # Self-attention sublayer\n",
    "        attn_out, _ = self.self_attn(x, x, x, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)\n",
    "        x = x + self.dropout1(attn_out)\n",
    "        x = self.norm1(x)\n",
    "        # Feed-forward sublayer\n",
    "        ff_out = self.ff(x)\n",
    "        x = x + self.dropout2(ff_out)\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderCustom(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feat_dim: int,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        dim_ff: int,\n",
    "        n_layers: int,\n",
    "        dropout: float = 0.1,\n",
    "        max_len: int = 5000\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Input embedding: feature projection + positional encoding\n",
    "        self.input_embedding = InputEmbedding(feat_dim, d_model, max_len)\n",
    "        # Stack of N encoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, n_heads, dim_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        src_mask: torch.Tensor = None,\n",
    "        src_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch, seq_len, feat_dim)\n",
    "        Returns:\n",
    "            Tensor of shape (seq_len, batch, d_model)\n",
    "        \"\"\"\n",
    "        x = self.input_embedding(x)       # (batch, seq_len, d_model)\n",
    "        x = x.transpose(0, 1)             # (seq_len, batch, d_model)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dim_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        # Masked Self-Attention\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        # Encoder-Decoder Attention\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        # Position-wise Feed-Forward Network\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_ff),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_ff, d_model)\n",
    "        )\n",
    "        # Layer Normalizations and Dropouts\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt: torch.Tensor,\n",
    "        memory: torch.Tensor,\n",
    "        tgt_mask: torch.Tensor = None,\n",
    "        memory_mask: torch.Tensor = None,\n",
    "        tgt_key_padding_mask: torch.Tensor = None,\n",
    "        memory_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tgt: Tensor of shape (tgt_len, batch, d_model)\n",
    "            memory: Tensor of shape (src_len, batch, d_model)\n",
    "        Returns:\n",
    "            Tensor of shape (tgt_len, batch, d_model)\n",
    "        \"\"\"\n",
    "        # Masked self-attention sublayer\n",
    "        attn1, _ = self.self_attn(\n",
    "            tgt, tgt, tgt,\n",
    "            attn_mask=tgt_mask,\n",
    "            key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "        tgt = tgt + self.dropout1(attn1)\n",
    "        tgt = self.norm1(tgt)\n",
    "        # Encoder-decoder attention sublayer\n",
    "        attn2, _ = self.multihead_attn(\n",
    "            tgt, memory, memory,\n",
    "            attn_mask=memory_mask,\n",
    "            key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "        tgt = tgt + self.dropout2(attn2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        # Feed-forward sublayer\n",
    "        ff_out = self.ff(tgt)\n",
    "        tgt = tgt + self.dropout3(ff_out)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "class TransformerDecoderCustom(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feat_dim: int,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        dim_ff: int,\n",
    "        n_layers: int,\n",
    "        dropout: float = 0.1,\n",
    "        max_len: int = 5000\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Input embedding for target sequence\n",
    "        self.input_embedding = InputEmbedding(feat_dim, d_model, max_len)\n",
    "        # Stack of N decoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, n_heads, dim_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        # Final projection back to feature dimension\n",
    "        self.output_linear = nn.Linear(d_model, feat_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt: torch.Tensor,\n",
    "        memory: torch.Tensor,\n",
    "        tgt_mask: torch.Tensor = None,\n",
    "        memory_mask: torch.Tensor = None,\n",
    "        tgt_key_padding_mask: torch.Tensor = None,\n",
    "        memory_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tgt: Tensor of shape (batch, tgt_len, feat_dim)\n",
    "            memory: Tensor of shape (src_len, batch, d_model)\n",
    "        Returns:\n",
    "            Tensor of shape (batch, tgt_len, feat_dim)\n",
    "        \"\"\"\n",
    "        x = self.input_embedding(tgt)       # (batch, tgt_len, d_model)\n",
    "        x = x.transpose(0, 1)               # (tgt_len, batch, d_model)\n",
    "        for layer in self.layers:\n",
    "            x = layer(\n",
    "                x,\n",
    "                memory,\n",
    "                tgt_mask=tgt_mask,\n",
    "                memory_mask=memory_mask,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                memory_key_padding_mask=memory_key_padding_mask\n",
    "            )\n",
    "        x = x.transpose(0, 1)               # (batch, tgt_len, d_model)\n",
    "        return self.output_linear(x)        # project back to feat_dim\n",
    "\n",
    "class TransformerWithHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feat_dim: int,\n",
    "        d_model: int = 256,\n",
    "        n_heads: int = 8,\n",
    "        dim_ff: int = 512,\n",
    "        n_layers: int = 12,\n",
    "        dropout: float = 0.1,\n",
    "        hidden_dim: int = 256,\n",
    "        out_dim: int = 64,\n",
    "        max_len: int = 5000\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Encoder-only backbone\n",
    "        self.encoder = TransformerEncoderCustom(\n",
    "            feat_dim, d_model, n_heads, dim_ff, n_layers, dropout, max_len\n",
    "        )\n",
    "        # Classification head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        src_mask: torch.Tensor = None,\n",
    "        src_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch, seq_len, feat_dim)\n",
    "        Returns:\n",
    "            Tensor of shape (batch, out_dim)\n",
    "        \"\"\"\n",
    "        # Encode input sequence\n",
    "        enc_output = self.encoder(x, src_mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        # Take the last token's representation\n",
    "        last_token = enc_output[-1]         # (batch, d_model)\n",
    "        return self.head(last_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2503278-03dd-4b6d-8497-e9f9d83e10fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNWithHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feat_dim: int,\n",
    "        hidden_size: int      = 128,\n",
    "        num_layers: int       = 12,\n",
    "        bidirectional: bool   = True,\n",
    "        hidden_dim: int       = 256,\n",
    "        out_dim: int          = 64,\n",
    "        freeze_backbone: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # RNN backbone (no dropout)\n",
    "        self.backbone = nn.RNN(\n",
    "            input_size   = feat_dim,\n",
    "            hidden_size  = hidden_size,\n",
    "            num_layers   = num_layers,\n",
    "            batch_first  = True,\n",
    "            bidirectional= bidirectional,\n",
    "            dropout      = 0.1\n",
    "        )\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # Compute RNN output dimension based on bi-directionality\n",
    "        rnn_out_dim = hidden_size * (2 if bidirectional else 1)\n",
    "\n",
    "        # Fully-connected head (dropout layer removed)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(rnn_out_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, feat_dim)\n",
    "        out, _ = self.backbone(x)       # out: (batch_size, seq_len, rnn_out_dim)\n",
    "        feat   = out[:, -1, :]          # last time step (batch_size, rnn_out_dim)\n",
    "        return self.head(feat)          # (batch_size, out_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b210558c-7f51-4e6b-ad3a-8efa5237a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMWithHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feat_dim: int,\n",
    "        hidden_size: int = 128,\n",
    "        num_layers: int = 1,\n",
    "        bidirectional: bool = False,\n",
    "        dropout: float = 0.0,\n",
    "        hidden_dim: int = 256,\n",
    "        out_dim: int = None,\n",
    "        freeze_backbone: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Determine output dimension (default to input feature size if not specified)\n",
    "        out_dim = feat_dim if out_dim is None else out_dim\n",
    "\n",
    "        # LSTM backbone for sequence modeling\n",
    "        # - input_size: number of features per time step\n",
    "        # - hidden_size: dimensionality of LSTM hidden state\n",
    "        # - num_layers: number of stacked LSTM layers\n",
    "        # - bidirectional: whether to use a bi-directional LSTM\n",
    "        # - dropout: dropout probability between LSTM layers (if num_layers > 1)\n",
    "        self.backbone = nn.LSTM(\n",
    "            input_size=feat_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # Compute the LSTM output dimension accounting for bidirectionality\n",
    "        lstm_out_dim = hidden_size * (2 if bidirectional else 1)\n",
    "\n",
    "        # Fully-connected head to map the final LSTM state to desired output\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(lstm_out_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, feat_dim)\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, out_dim)\n",
    "        \"\"\"\n",
    "        # Pass the input through the LSTM backbone\n",
    "        # out shape: (batch_size, seq_len, lstm_out_dim)\n",
    "        out, _ = self.backbone(x)\n",
    "\n",
    "        # Extract features from the last time step\n",
    "        # feat shape: (batch_size, lstm_out_dim)\n",
    "        feat = out[:, -1, :]\n",
    "\n",
    "        # Compute final output via the head\n",
    "        return self.head(feat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4766f28-d102-4ed0-8272-7d5098ad9f15",
   "metadata": {},
   "source": [
    "## fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e104da9-d8e8-42ea-a810-3567b9ae412c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GRU' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# model catalog\u001b[39;00m\n\u001b[1;32m      5\u001b[0m MODEL_CATALOG \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLWM_freeze_backbone\u001b[39m\u001b[38;5;124m\"\u001b[39m : LWMWithHead, \u001b[38;5;66;03m# freeze backbone\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLWM_pretrained_Fine_tune\u001b[39m\u001b[38;5;124m\"\u001b[39m : LWMWithHead, \u001b[38;5;66;03m# not freeze backbone\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLWM_Fine_tune\u001b[39m\u001b[38;5;124m\"\u001b[39m : LWMWithHead, \u001b[38;5;66;03m# not pretrained not backbone\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgru\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[43mGRU\u001b[49m\n\u001b[1;32m     10\u001b[0m     \n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GRU' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "INPUT_DIM = 64 # real data feature dimension\n",
    "PATCH_LENGTH = 16\n",
    "D_MODEL = 64\n",
    "N_LAYERS = 12 \n",
    "HIDDEN_DIM = 256 # Head Hidden dim\n",
    "OUT_DIM = 64\n",
    "DROPOUT = 0.1 # the same LWM dropout\n",
    "BIDIRECTIONAL = True #  \n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "# model catalog\n",
    "MODEL_CATALOG = {\n",
    "    \"LWM_freeze_backbone\" : LWMWithHead, # freeze backbone\n",
    "    \"LWM_pretrained_Fine_tune\" : LWMWithHead, # not freeze backbone\n",
    "    \"LWM_Fine_tune\" : LWMWithHead, # not pretrained not backbone\n",
    "    \"gru\" : GRUWithHead, # gru Model\n",
    "    \"RNN\" : RNNWithHead, # RNN Model\n",
    "    \"LSTM\" : LSTMWithHead, # LSTM Model\n",
    "    \"Transformer\" : TransformerWithHead # Transformer Model\n",
    "}\n",
    "\n",
    "MODEL_PARAMS = {\n",
    "    \"LWM_freeze_backbone\" : {\n",
    "        input_dim = INPUT_DIM,\n",
    "        patch_length = PATCH_LENGTH,\n",
    "        d_model = D_MODEL,\n",
    "        hidden_dim = HIDDEN_DIM,\n",
    "        out_dim = OUT_DIM,\n",
    "        freeze_backbone = True,\n",
    "        checkpoint_path = \"./model_weights.pth\",\n",
    "        device = DEVICE,\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f23597-8581-4cdf-926e-f797a7b2ac15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9e706d-4875-4ee2-93d9-f792571fe2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3930b75e-0895-4fb1-8039-744c86730b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d4ceda-b090-4b01-91a4-698c319420f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dc2a3f-b572-464d-ba7c-9992b281fd5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317fde92-dd4d-4092-b93c-7b1b83674c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac929b23-8f2c-4b46-b746-d450e45bbe3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d402db77-53c1-442d-b0ec-e5adc90189c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb901b30-47df-4851-bb62-03a9b0dba914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a079e9-2360-4236-8da6-37c8e75f9af9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
