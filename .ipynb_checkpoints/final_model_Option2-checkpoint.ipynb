{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0939df15-abc7-40e7-83bf-f12820dd42f2",
   "metadata": {},
   "source": [
    "# All Model saves here\n",
    "- option2 : user separate 2:1 = train : val do not overlap dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cbf7fb-304f-432d-a3f9-580052d5a375",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0e94396-1d80-4df3-8648-19368f82c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import DeepMIMOv3\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import IterableDataset\n",
    "import numpy as np\n",
    "import time, gc\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from lwm_model import lwm\n",
    "from torch.optim import Adam\n",
    "from pathlib import Path\n",
    "import torch, time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb2d8b4-7d3d-43ae-b997-2a4e6eab9a1a",
   "metadata": {},
   "source": [
    "## GPU Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a6479f7-d2b2-4632-b3a8-ffe3c49a6608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# GPU \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea87b9c9-a4b1-4442-b26f-606828c4dc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.6\n",
      "90501\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)                   \n",
    "print(torch.backends.cudnn.version())       \n",
    "print(\"CUDA available:\", torch.cuda.is_available())  # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dc1a47-3bc0-4236-813e-c7be4c21bd41",
   "metadata": {},
   "source": [
    "## DeepMIMOv3 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "821eb69b-806e-4694-adf9-fd0efcbf5e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = DeepMIMOv3.default_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1d1f75d-54c0-4198-990f-15eaeec28e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change parameters for the setup\n",
    "# Scenario O1_60 extracted at the dataset_folder\n",
    "#LWM dynamic senario\n",
    "# parameters['dataset_folder'] = r'/content/drive/MyDrive/Colab Notebooks/LWM'\n",
    "scene = 15 # scene 15\n",
    "# change my linux route\n",
    "parameters['dataset_folder'] = '/home/dlghdbs200/LWM'\n",
    "\n",
    "# scnario = 02_dyn_3p5 <- download file\n",
    "parameters['scenario'] = 'O2_dyn_3p5'\n",
    "parameters['dynamic_scenario_scenes'] = np.arange(scene) #scene 0~9\n",
    "\n",
    "# Up to 10 multipath paths per user-to-base station channel\n",
    "parameters['num_paths'] = 10\n",
    "\n",
    "# User rows 1-100\n",
    "parameters['user_rows'] = np.arange(100)\n",
    "# User subsampling\n",
    "parameters['user_subsampling'] = 0.01\n",
    "\n",
    "# Activate only the first basestation\n",
    "parameters['active_BS'] = np.array([1])\n",
    "\n",
    "parameters['activate_OFDM'] = 1\n",
    "\n",
    "parameters['OFDM']['bandwidth'] = 0.05 # 50 MHz\n",
    "parameters['OFDM']['subcarriers'] = 512 # OFDM with 512 subcarriers\n",
    "parameters['OFDM']['selected_subcarriers'] = np.arange(0, 64, 1)\n",
    "#parameters['OFDM']['subcarriers_limit'] = 64 # Keep only first 64 subcarriers\n",
    "\n",
    "parameters['ue_antenna']['shape'] = np.array([1, 1]) # Single antenna\n",
    "parameters['bs_antenna']['shape'] = np.array([1, 32]) # ULA of 32 elements\n",
    "#parameters['bs_antenna']['rotation'] = np.array([0, 30, 90]) # ULA of 32 elements\n",
    "#parameters['ue_antenna']['rotation'] = np.array([[0, 30], [30, 60], [60, 90]]) # ULA of 32 elements\n",
    "#parameters['ue_antenna']['radiation_pattern'] = 'isotropic'\n",
    "#parameters['bs_antenna']['radiation_pattern'] = 'halfwave-dipole'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a9e7bb3-7cb7-4742-a949-9dd84ca31a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                             | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following parameters seem unnecessary:\n",
      "{'activate_OFDM'}\n",
      "\n",
      "Scene 1/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  37%|███████████████████▏                                | 25483/69006 [00:00<00:00, 254812.02it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 253802.35it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 5716.91it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4341.93it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 257.41it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 2/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  42%|█████████████████████▋                              | 28709/69006 [00:00<00:00, 287074.53it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 295244.02it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 6981.51it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 3738.24it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 839.53it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 3/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  43%|██████████████████████▌                             | 29934/69006 [00:00<00:00, 299326.01it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 308203.92it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 8688.76it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4957.81it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 752.88it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 4/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  29%|███████████████▏                                    | 20235/69006 [00:00<00:00, 202334.76it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 257273.47it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 7000.75it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4981.36it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 854.59it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 5/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  40%|████████████████████▉                               | 27725/69006 [00:00<00:00, 277178.89it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 288647.60it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 7833.78it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 3437.95it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 912.20it/s]\u001b[A\n",
      " 33%|████████████████████████████▎                                                        | 1/3 [00:07<00:14,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenes 0–4 generation time: 6.85s\n",
      "The following parameters seem unnecessary:\n",
      "{'activate_OFDM'}\n",
      "\n",
      "Scene 1/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  42%|██████████████████████                              | 29319/69006 [00:00<00:00, 293158.83it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 289630.25it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 6408.60it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 6442.86it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 667.46it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 2/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  50%|█████████████████████████▊                          | 34247/69006 [00:00<00:00, 342446.65it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 325862.60it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 7762.07it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 3912.60it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 523.50it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 3/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  48%|████████████████████████▊                           | 32866/69006 [00:00<00:00, 328648.56it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 321801.87it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 7946.01it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5511.57it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 610.97it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 4/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  48%|█████████████████████████                           | 33188/69006 [00:00<00:00, 331841.55it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 320336.26it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 8020.82it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4888.47it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 817.76it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 5/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  44%|██████████████████████▋                             | 30045/69006 [00:00<00:00, 300432.38it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 307785.06it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 8188.17it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 2300.77it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 972.93it/s]\u001b[A\n",
      " 67%|████████████████████████████████████████████████████████▋                            | 2/3 [00:13<00:06,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenes 5–9 generation time: 6.66s\n",
      "The following parameters seem unnecessary:\n",
      "{'activate_OFDM'}\n",
      "\n",
      "Scene 1/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  49%|█████████████████████████▍                          | 33797/69006 [00:00<00:00, 337952.60it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 328450.04it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 7864.12it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 6721.64it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 686.13it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 2/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  42%|█████████████████████▌                              | 28640/69006 [00:00<00:00, 286390.03it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 284788.94it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 6819.75it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5511.57it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 785.16it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 3/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  40%|████████████████████▉                               | 27707/69006 [00:00<00:00, 277050.45it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 292877.26it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 6825.56it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4132.32it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 745.92it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 4/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  26%|█████████████▌                                      | 18071/69006 [00:00<00:00, 180698.97it/s]\u001b[A\n",
      "Reading ray-tracing:  54%|███████████████████████████▉                        | 37128/69006 [00:00<00:00, 186491.56it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 219720.20it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 6482.70it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5426.01it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 793.77it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 5/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  43%|██████████████████████▎                             | 29621/69006 [00:00<00:00, 296187.69it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 283010.35it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 7317.34it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 3748.26it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 909.83it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:20<00:00,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenes 10–14 generation time: 6.85s\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## dataset setting (chunked on‑the‑fly generation)\n",
    "import time, gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 0~999 scene index , process 50 at that time\n",
    "scene_indices = np.arange(scene)\n",
    "chunk_size   = 5\n",
    "all_data     = []\n",
    "\n",
    "# Call generate_data for each scene chunk\n",
    "for i in tqdm(range(0, len(scene_indices), chunk_size)):\n",
    "    chunk = scene_indices[i : i+chunk_size].tolist()\n",
    "    parameters['dynamic_scenario_scenes'] = chunk\n",
    "\n",
    "    start = time.time()\n",
    "    data_chunk = DeepMIMOv3.generate_data(parameters)\n",
    "    print(f\"Scenes {chunk[0]}–{chunk[-1]} generation time: {time.time() - start:.2f}s\")\n",
    "\n",
    "    # combine all_data or save in the Disk\n",
    "    all_data.extend(data_chunk)\n",
    "\n",
    "    # free memory \n",
    "    del data_chunk\n",
    "    gc.collect()\n",
    "\n",
    "# comvine Dataset\n",
    "dataset = all_data\n",
    "\n",
    "\n",
    "print(parameters['user_rows'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864fd513-23b4-418e-a936-2cbf42cf2cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3836f84f-7318-4d46-8959-200f2a292c69",
   "metadata": {},
   "source": [
    "## do not overlap dataset and separate train : val = 2 : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "209e7717-425d-4692-84f0-2928ffa755cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# all User\n",
    "U = dataset[0][0]['user']['channel'].shape[0]   # ex) 737\n",
    "\n",
    "# separate 2:1 = train : val\n",
    "user_ids = np.arange(U)\n",
    "random.shuffle(user_ids)          \n",
    "cut = int(len(user_ids) * 2 / 3)\n",
    "\n",
    "train_users = set(user_ids[:cut])   # 2/3 → Train\n",
    "val_users   = set(user_ids[cut:])   # 1/3 → Val\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc75e14-9605-49b0-8034-2a1dc367f97e",
   "metadata": {},
   "source": [
    "## About Information\n",
    "User : 737\n",
    "UE antenna : 1\n",
    "BS antenna : 32  Shape(a+bj)\n",
    "subcarrier : 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48148074-4682-4ab8-acbe-71ef042a5020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unmasked Data Model(gru\n",
    "# separate maksed data and unmasked data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe56d11-0ebb-4c13-b4c2-61fadd9c714b",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ba0a2d3-ee35-43dc-b457-bd400e5b75c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UnMaskedChannelSeqDataset\n",
    "#   • Predict next-step channel vector from past `seq_len` steps (no masking)\n",
    "#   • Supports user-level Train / Val split via `user_filter`\n",
    "#   • Power-normalises complex channel → real + imag concat, then Min–Max scales\n",
    "# =============================================================================\n",
    "from typing import Optional, Set, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "class UnMaskedChannelSeqDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    IterableDataset (un-masked version).\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    scenes : list\n",
    "        DeepMIMO scene dictionaries.\n",
    "    seq_len : int\n",
    "        Number of past time-steps used as input.\n",
    "    eps : float\n",
    "        Small constant to avoid division by zero in power normalisation.\n",
    "    scalers : tuple(MinMaxScaler, MinMaxScaler) | None\n",
    "        Pre-fitted (x, y) scalers.  If None, fit scalers on *this* dataset.\n",
    "    user_filter : set[int] | None\n",
    "        If given, only yield samples for those user indices.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        scenes,\n",
    "        seq_len: int = 5,\n",
    "        eps: float = 1e-9,\n",
    "        scalers: Optional[Tuple[MinMaxScaler, MinMaxScaler]] = None,\n",
    "        user_filter: Optional[Set[int]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scenes      = scenes\n",
    "        self.seq_len     = seq_len\n",
    "        self.eps         = eps\n",
    "        self.user_filter = user_filter\n",
    "\n",
    "        # Channel tensor dimensions -------------------------------------------------\n",
    "        ch0          = scenes[0][0]['user']['channel']   # (U, 1, A, S)\n",
    "        self.U       = ch0.shape[0]                      # users\n",
    "        self.A       = ch0.shape[2]                      # BS antennas\n",
    "        self.S       = ch0.shape[3]                      # sub-carriers\n",
    "        self.vec_len = 2 * self.A                       # real + imag concatenation\n",
    "\n",
    "        # Fit / reuse MinMax scalers ------------------------------------------------\n",
    "        if scalers is None:\n",
    "            X_list, y_list = [], []\n",
    "            T = len(scenes)\n",
    "            for t in range(self.seq_len, T):\n",
    "                past  = scenes[t - self.seq_len : t]\n",
    "                s_tgt = scenes[t]\n",
    "\n",
    "                for u in range(self.U):\n",
    "                    if self.user_filter is not None and u not in self.user_filter:\n",
    "                        continue\n",
    "                    for s in range(self.S):\n",
    "                        seq_np = np.stack(\n",
    "                            [self._power_norm(p[0]['user']['channel'][u, 0, :, s])\n",
    "                             for p in past],\n",
    "                            axis=0, dtype=np.float32\n",
    "                        )\n",
    "                        tgt_np = self._power_norm(\n",
    "                            s_tgt[0]['user']['channel'][u, 0, :, s]\n",
    "                        ).astype(np.float32)\n",
    "\n",
    "                        if not np.any(seq_np) or not np.any(tgt_np):\n",
    "                            continue\n",
    "\n",
    "                        X_list.append(seq_np.reshape(-1, self.vec_len))\n",
    "                        y_list.append(tgt_np)\n",
    "\n",
    "            X_all = np.vstack(X_list)\n",
    "            y_all = np.stack(y_list)\n",
    "            self.scaler_x = MinMaxScaler().fit(X_all)\n",
    "            self.scaler_y = MinMaxScaler().fit(y_all)\n",
    "        else:\n",
    "            self.scaler_x, self.scaler_y = scalers\n",
    "\n",
    "    # -----------------------------------------------------------------------------  \n",
    "    # Iterator\n",
    "    # -----------------------------------------------------------------------------\n",
    "    def __iter__(self):\n",
    "        T = len(self.scenes)\n",
    "        for t in range(self.seq_len, T):\n",
    "            past  = self.scenes[t - self.seq_len : t]\n",
    "            s_tgt = self.scenes[t]\n",
    "\n",
    "            for u in range(self.U):\n",
    "                if self.user_filter is not None and u not in self.user_filter:\n",
    "                    continue\n",
    "                for s in range(self.S):\n",
    "                    seq_np = np.stack(\n",
    "                        [self._power_norm(p[0]['user']['channel'][u, 0, :, s])\n",
    "                         for p in past],\n",
    "                        axis=0\n",
    "                    )\n",
    "                    tgt_np = self._power_norm(\n",
    "                        s_tgt[0]['user']['channel'][u, 0, :, s]\n",
    "                    )\n",
    "\n",
    "                    if not np.any(seq_np) or not np.any(tgt_np):\n",
    "                        continue\n",
    "\n",
    "                    N, D = seq_np.shape\n",
    "                    seq_np = self.scaler_x.transform(seq_np.reshape(-1, D)).reshape(N, D)\n",
    "                    tgt_np = self.scaler_y.transform(tgt_np.reshape(1, -1)).reshape(-1,)\n",
    "\n",
    "                    yield (\n",
    "                        torch.from_numpy(seq_np).float(),  # (seq_len, vec_len)\n",
    "                        torch.from_numpy(tgt_np).float()   # (vec_len,)\n",
    "                    )\n",
    "\n",
    "    # -----------------------------------------------------------------------------  \n",
    "    # Helpers\n",
    "    # -----------------------------------------------------------------------------\n",
    "    def _power_norm(self, h: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Convert complex vector → real|imag concat, then normalise power to 1.\"\"\"\n",
    "        v     = np.concatenate([h.real, h.imag]).astype(np.float32)\n",
    "        power = np.mean(v * v) + self.eps\n",
    "        return v / np.sqrt(power)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Rough size estimate (IterableDataset doesn't rely on this).\"\"\"\n",
    "        return (len(self.scenes) - self.seq_len) * self.U * self.S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "97d365ac-69a6-494c-bfc7-198bdeb29582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MaskedChannelSeqDataset\n",
    "#   • Same preprocessing as UnMasked, plus 15 % BERT-style masking on sequences\n",
    "#   • Masking policy:\n",
    "#       80 % → replace with zeros\n",
    "#       10 % → replace with Gaussian noise\n",
    "#       10 % → keep original patch (label remains the same)\n",
    "# =============================================================================\n",
    "\n",
    "class MaskedChannelSeqDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    IterableDataset (masked version).\n",
    "\n",
    "    Additional Args\n",
    "    ---------------\n",
    "    noise_std : float\n",
    "        Standard deviation for Gaussian noise patches.\n",
    "    user_filter : set[int] | None\n",
    "        If given, only yield samples for those user indices.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        scenes,\n",
    "        seq_len: int = 5,\n",
    "        eps: float = 1e-9,\n",
    "        noise_std: float = 1.0,\n",
    "        user_filter: Optional[Set[int]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scenes      = scenes\n",
    "        self.seq_len     = seq_len\n",
    "        self.eps         = eps\n",
    "        self.noise_std   = noise_std\n",
    "        self.user_filter = user_filter\n",
    "\n",
    "        # Channel tensor dimensions -------------------------------------------------\n",
    "        ch0          = scenes[0][0]['user']['channel']\n",
    "        self.U       = ch0.shape[0]\n",
    "        self.A       = ch0.shape[2]\n",
    "        self.S       = ch0.shape[3]\n",
    "        self.vec_len = 2 * self.A\n",
    "\n",
    "        # Fit scalers on power-normalised data --------------------------------------\n",
    "        X_list, y_list = [], []\n",
    "        T = len(scenes)\n",
    "        for t in range(self.seq_len, T):\n",
    "            past  = scenes[t - self.seq_len : t]\n",
    "            s_tgt = scenes[t]\n",
    "            for u in range(self.U):\n",
    "                if self.user_filter is not None and u not in self.user_filter:\n",
    "                    continue\n",
    "                for s in range(self.S):\n",
    "                    seq_np = np.stack(\n",
    "                        [self._power_norm(p[0]['user']['channel'][u, 0, :, s])\n",
    "                         for p in past],\n",
    "                        axis=0, dtype=np.float32\n",
    "                    )\n",
    "                    tgt_np = self._power_norm(\n",
    "                        s_tgt[0]['user']['channel'][u, 0, :, s]\n",
    "                    ).astype(np.float32)\n",
    "\n",
    "                    if not np.any(seq_np) or not np.any(tgt_np):\n",
    "                        continue\n",
    "\n",
    "                    X_list.append(seq_np.reshape(-1, self.vec_len))\n",
    "                    y_list.append(tgt_np)\n",
    "\n",
    "        X_all = np.vstack(X_list)\n",
    "        y_all = np.stack(y_list)\n",
    "        self.scaler_x = MinMaxScaler().fit(X_all)\n",
    "        self.scaler_y = MinMaxScaler().fit(y_all)\n",
    "\n",
    "        # Pre-allocated zero mask\n",
    "        self.mask_value = torch.zeros(self.vec_len, dtype=torch.float32)\n",
    "\n",
    "    # -----------------------------------------------------------------------------  \n",
    "    # Iterator\n",
    "    # -----------------------------------------------------------------------------\n",
    "    def __iter__(self):\n",
    "        mask_prob  = 0\n",
    "        zero_prob  = mask_prob * 0.8\n",
    "        noise_prob = mask_prob * 0.1\n",
    "\n",
    "        T = len(self.scenes)\n",
    "        for t in range(self.seq_len, T):\n",
    "            past  = self.scenes[t - self.seq_len : t]\n",
    "            s_tgt = self.scenes[t]\n",
    "\n",
    "            for u in range(self.U):\n",
    "                if self.user_filter is not None and u not in self.user_filter:\n",
    "                    continue\n",
    "                for s in range(self.S):\n",
    "                    seq_np = np.stack(\n",
    "                        [self._power_norm(p[0]['user']['channel'][u, 0, :, s])\n",
    "                         for p in past],\n",
    "                        axis=0\n",
    "                    )\n",
    "                    tgt_np = self._power_norm(\n",
    "                        s_tgt[0]['user']['channel'][u, 0, :, s]\n",
    "                    )\n",
    "\n",
    "                    if not np.any(seq_np) or not np.any(tgt_np):\n",
    "                        continue\n",
    "\n",
    "                    N, D = seq_np.shape\n",
    "                    seq_np = self.scaler_x.transform(seq_np.reshape(-1, D)).reshape(N, D)\n",
    "                    tgt_np = self.scaler_y.transform(tgt_np.reshape(1, -1)).reshape(-1,)\n",
    "\n",
    "                    seq_tensor   = torch.from_numpy(seq_np).float()\n",
    "                    target_tensor= torch.from_numpy(tgt_np).float()\n",
    "\n",
    "                    mpos = np.random.randint(self.seq_len)\n",
    "                    r    = np.random.rand()\n",
    "\n",
    "                    if r < zero_prob:\n",
    "                        masked_seq = seq_tensor.clone()\n",
    "                        masked_seq[mpos] = self.mask_value\n",
    "                        yield masked_seq, torch.tensor([mpos]), target_tensor\n",
    "\n",
    "                    elif r < zero_prob + noise_prob:\n",
    "                        masked_seq = seq_tensor.clone()\n",
    "                        masked_seq[mpos] = torch.randn(self.vec_len) * self.noise_std\n",
    "                        yield masked_seq, torch.tensor([mpos]), target_tensor\n",
    "\n",
    "                    elif r < mask_prob:\n",
    "                        yield seq_tensor, torch.tensor([mpos]), target_tensor  # kept patch\n",
    "\n",
    "                    else:  # no masking\n",
    "                        yield seq_tensor, torch.tensor([mpos]), target_tensor\n",
    "\n",
    "    # -----------------------------------------------------------------------------  \n",
    "    # Helpers\n",
    "    # -----------------------------------------------------------------------------\n",
    "    def _power_norm(self, h: np.ndarray) -> np.ndarray:\n",
    "        v     = np.concatenate([h.real, h.imag]).astype(np.float32)\n",
    "        power = np.mean(v * v) + self.eps\n",
    "        return v / np.sqrt(power)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.scenes) - self.seq_len) * self.U * self.S\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b0cc00-cfa9-49f3-b981-87e55e3d9e20",
   "metadata": {},
   "source": [
    "## define trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2fc802ee-c248-4351-9d38-bc8386b76424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_params(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e1373a-f803-49f3-b891-f9d6f59e5dc9",
   "metadata": {},
   "source": [
    "## Split Train/Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "29548f28-3188-4562-bb96-634ed3bac496",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len   = 5\n",
    "ratio     = 0.6          # 6 : 4\n",
    "batch_size= 32\n",
    "\n",
    "# 1) Derive user-ID sets --------------------------------------------------------\n",
    "U = dataset[0][0]['user']['channel'].shape[0]   # e.g. 737\n",
    "user_ids = np.arange(U)\n",
    "random.shuffle(user_ids)                        # reproducible → random.seed(42)\n",
    "cut = int(U * ratio)\n",
    "\n",
    "train_users = set(user_ids[:cut])   # 60 %\n",
    "val_users   = set(user_ids[cut:])   # 40 %\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3fd809a5-3720-4ffd-90ce-b2d5ccf541f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Un-masked datasets  (share scaler to avoid leakage) -----------------------\n",
    "unmasked_train_ds = UnMaskedChannelSeqDataset(\n",
    "    scenes      = dataset,\n",
    "    seq_len     = seq_len,\n",
    "    user_filter = train_users\n",
    ")\n",
    "\n",
    "unmasked_val_ds = UnMaskedChannelSeqDataset(\n",
    "    scenes      = dataset,\n",
    "    seq_len     = seq_len,\n",
    "    scalers     = (unmasked_train_ds.scaler_x,   # reuse train scalers\n",
    "                   unmasked_train_ds.scaler_y),\n",
    "    user_filter = val_users\n",
    ")\n",
    "\n",
    "unmasked_train_loader = DataLoader(unmasked_train_ds, batch_size=batch_size, shuffle=False)\n",
    "unmasked_val_loader   = DataLoader(unmasked_val_ds,   batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "49c214a4-1b26-49a7-b308-e16a000f0ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Masked datasets -----------------------------------------------------------\n",
    "masked_train_ds = MaskedChannelSeqDataset(\n",
    "    scenes      = dataset,\n",
    "    seq_len     = seq_len,\n",
    "    user_filter = train_users\n",
    ")\n",
    "\n",
    "masked_val_ds = MaskedChannelSeqDataset(\n",
    "    scenes      = dataset,\n",
    "    seq_len     = seq_len,\n",
    "    user_filter = val_users\n",
    ")\n",
    "\n",
    "masked_train_loader = DataLoader(masked_train_ds, batch_size=batch_size, shuffle=False)\n",
    "masked_val_loader   = DataLoader(masked_val_ds,   batch_size=batch_size, shuffle=False)\n",
    "# ─────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b29d65-387d-4326-b989-afe274279c38",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0383c46a-7e6c-4907-aaa4-e6b1902a770d",
   "metadata": {},
   "source": [
    "LWMWithHead: A wrapper class that uses a pre-trained LWM (Transformer encoder) as the backbone,\n",
    "             and attaches a new fully-connected (FC) head for downstream tasks\n",
    "             (regression, classification, etc.).\n",
    "\n",
    "Changes:\n",
    "- input_dim: Dimension of the actual input data (e.g., 64)\n",
    "- patch_length: Patch length expected by the backbone (e.g., 16)\n",
    "- Replaces the original element_length parameter with these two distinct parameters\n",
    "- Applies a projection layer (self.input_proj) in forward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "80c33fca-38fa-4842-b886-69972e5d28a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LWMWithHead(nn.Module):\n",
    "    \"\"\"\n",
    "    LWMWithHead: A wrapper class that uses a pre-trained LWM (Transformer encoder) as the backbone,\n",
    "                 and attaches a new fully-connected (FC) head for downstream tasks\n",
    "                 (regression, classification, etc.).\n",
    "\n",
    "    Changes:\n",
    "    - input_dim: Dimension of the actual input data (e.g., 64)\n",
    "    - patch_length: Patch length expected by the backbone (e.g., 16)\n",
    "    - Replaces the original element_length parameter with these two distinct parameters\n",
    "    - Applies a projection layer (self.input_proj) in forward()\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,                 # Dimension of the actual input data (e.g., 64)\n",
    "        patch_length: int,              # Patch length expected by the backbone (e.g., 16)\n",
    "        d_model: int = 64,              # LWM hidden size\n",
    "        max_len: int = 129,             # Positional encoding max length\n",
    "        n_layers: int = 12,             # Number of Transformer encoder layers\n",
    "        hidden_dim: int = 256,          # FC head hidden dimension\n",
    "        out_dim: int = 64,              # FC head output dimension\n",
    "        freeze_backbone: bool = True,   # Whether to freeze the backbone\n",
    "        checkpoint_path: str | None = \"./model_weights.pth\",\n",
    "        device: str = \"cuda\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # apply a projection layer to match backbone's expected patch_length\n",
    "        self.input_proj = nn.Linear(input_dim, patch_length)\n",
    "\n",
    "        # initialize backbone\n",
    "        if checkpoint_path is None:\n",
    "            # randomly initialized backbone\n",
    "            self.backbone = lwm(\n",
    "                element_length=patch_length,\n",
    "                d_model=d_model,\n",
    "                max_len=max_len,\n",
    "                n_layers=n_layers\n",
    "            ).to(device)\n",
    "        else:\n",
    "            # load pre-trained weights\n",
    "            self.backbone = lwm.from_pretrained(\n",
    "                ckpt_name=checkpoint_path,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "        # freeze backbone parameters if required\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # attach a new fully-connected head for downstream tasks\n",
    "        self.head = nn.Sequential(\n",
    "            # change 2 layer -> 1 layer\n",
    "            nn.Linear(d_model, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, masked_pos: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: Tensor of shape (B, L, input_dim)\n",
    "            masked_pos: Tensor of shape (B, num_mask)\n",
    "        Returns:\n",
    "            out: Tensor of shape (B, out_dim)\n",
    "        \"\"\"\n",
    "        # project inputs to patch_length dimension\n",
    "        x = self.input_proj(input_ids)\n",
    "\n",
    "        # backbone forward: returns (logits_lm, enc_output)\n",
    "        _, enc_output = self.backbone(x, masked_pos)\n",
    "\n",
    "        # extract CLS token feature (first token)\n",
    "        feat = enc_output[:, 0, :]\n",
    "\n",
    "        # pass through FC head to get final output\n",
    "        out = self.head(feat)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2e8f0ee-6782-4eb2-b6c3-fe363ed8d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GRUWithHead(nn.Module):\n",
    "    \"\"\"\n",
    "    GRUWithHead (projected):\n",
    "      • Projects the raw feature dimension (input_dim) to a smaller patch_length\n",
    "        so every backbone receives the same patch-sized input (like LWM).\n",
    "      • Stacks N GRU layers, then an FC head for downstream tasks.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int    = 64,   # raw feature dimension coming from the DataLoader\n",
    "        patch_length: int = 16,   # target dimension fed to the GRU backbone\n",
    "        d_model: int      = 64,   # GRU hidden size\n",
    "        n_layers: int     = 12,   # number of stacked GRU layers\n",
    "        bidirectional: bool = True,\n",
    "        dropout: float      = 0.1,\n",
    "        hidden_dim: int     = 256, # FC-head hidden size\n",
    "        out_dim: int        = 64,  # FC-head output size\n",
    "        freeze_backbone: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 0) Project raw_dim → patch_length (64 → 16)\n",
    "        self.input_proj = nn.Linear(input_dim, patch_length)\n",
    "\n",
    "        # 1) GRU backbone that expects 'patch_length' features per time step\n",
    "        self.backbone = nn.GRU(\n",
    "            input_size     = patch_length,\n",
    "            hidden_size    = d_model,\n",
    "            num_layers     = n_layers,\n",
    "            batch_first    = True,\n",
    "            bidirectional  = bidirectional,\n",
    "            dropout        = dropout if n_layers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # 2) Fully-connected head\n",
    "        gru_out_dim = d_model * (2 if bidirectional else 1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(gru_out_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x : Tensor of shape (batch, seq_len, input_dim) – raw features\n",
    "        Returns:\n",
    "            Tensor of shape (batch, out_dim)\n",
    "        \"\"\"\n",
    "        # project raw features to patch_length\n",
    "        x_proj = self.input_proj(x)                 # (B, seq_len, patch_length)\n",
    "\n",
    "        # sequence modelling with GRU\n",
    "        out, _ = self.backbone(x_proj)              # (B, seq_len, num_dirs*d_model)\n",
    "\n",
    "        # use the last time-step representation\n",
    "        feat = out[:, -1, :]                        # (B, gru_out_dim)\n",
    "\n",
    "        # downstream head\n",
    "        return self.head(feat)                      # (B, out_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6f9d84d1-33a1-4f34-9e97-51242fc2d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # Create positional encoding matrix of shape (1, max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            Tensor: x plus positional encodings\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, feat_dim: int, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # Optional linear projection from feat_dim to d_model\n",
    "        self.proj = nn.Linear(feat_dim, d_model) if feat_dim != d_model else None\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch, seq_len, feat_dim)\n",
    "        Returns:\n",
    "            Tensor of shape (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        if self.proj is not None:\n",
    "            x = self.proj(x)\n",
    "        return self.pos_enc(x)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dim_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        # Multi-Head Self-Attention\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        # Position-wise Feed-Forward Network\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_ff),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_ff, d_model)\n",
    "        )\n",
    "        # Layer Normalization and Dropout for residual connections\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        src_mask: torch.Tensor = None,\n",
    "        src_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (seq_len, batch, d_model)\n",
    "            src_mask: Optional Tensor of shape (seq_len, seq_len)\n",
    "            src_key_padding_mask: Optional Tensor of shape (batch, seq_len)\n",
    "        Returns:\n",
    "            Tensor of shape (seq_len, batch, d_model)\n",
    "        \"\"\"\n",
    "        # Self-attention sublayer\n",
    "        attn_out, _ = self.self_attn(x, x, x, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)\n",
    "        x = x + self.dropout1(attn_out)\n",
    "        x = self.norm1(x)\n",
    "        # Feed-forward sublayer\n",
    "        ff_out = self.ff(x)\n",
    "        x = x + self.dropout2(ff_out)\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderCustom(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feat_dim: int,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        dim_ff: int,\n",
    "        n_layers: int,\n",
    "        dropout: float = 0.1,\n",
    "        max_len: int = 5000\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Input embedding: feature projection + positional encoding\n",
    "        self.input_embedding = InputEmbedding(feat_dim, d_model, max_len)\n",
    "        # Stack of N encoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, n_heads, dim_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        src_mask: torch.Tensor = None,\n",
    "        src_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch, seq_len, feat_dim)\n",
    "        Returns:\n",
    "            Tensor of shape (seq_len, batch, d_model)\n",
    "        \"\"\"\n",
    "        x = self.input_embedding(x)       # (batch, seq_len, d_model)\n",
    "        x = x.transpose(0, 1)             # (seq_len, batch, d_model)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dim_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        # Masked Self-Attention\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        # Encoder-Decoder Attention\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        # Position-wise Feed-Forward Network\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_ff),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_ff, d_model)\n",
    "        )\n",
    "        # Layer Normalizations and Dropouts\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt: torch.Tensor,\n",
    "        memory: torch.Tensor,\n",
    "        tgt_mask: torch.Tensor = None,\n",
    "        memory_mask: torch.Tensor = None,\n",
    "        tgt_key_padding_mask: torch.Tensor = None,\n",
    "        memory_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tgt: Tensor of shape (tgt_len, batch, d_model)\n",
    "            memory: Tensor of shape (src_len, batch, d_model)\n",
    "        Returns:\n",
    "            Tensor of shape (tgt_len, batch, d_model)\n",
    "        \"\"\"\n",
    "        # Masked self-attention sublayer\n",
    "        attn1, _ = self.self_attn(\n",
    "            tgt, tgt, tgt,\n",
    "            attn_mask=tgt_mask,\n",
    "            key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "        tgt = tgt + self.dropout1(attn1)\n",
    "        tgt = self.norm1(tgt)\n",
    "        # Encoder-decoder attention sublayer\n",
    "        attn2, _ = self.multihead_attn(\n",
    "            tgt, memory, memory,\n",
    "            attn_mask=memory_mask,\n",
    "            key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "        tgt = tgt + self.dropout2(attn2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        # Feed-forward sublayer\n",
    "        ff_out = self.ff(tgt)\n",
    "        tgt = tgt + self.dropout3(ff_out)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "class TransformerDecoderCustom(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feat_dim: int,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        dim_ff: int,\n",
    "        n_layers: int,\n",
    "        dropout: float = 0.1,\n",
    "        max_len: int = 5000\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Input embedding for target sequence\n",
    "        self.input_embedding = InputEmbedding(feat_dim, d_model, max_len)\n",
    "        # Stack of N decoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, n_heads, dim_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        # Final projection back to feature dimension\n",
    "        self.output_linear = nn.Linear(d_model, feat_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt: torch.Tensor,\n",
    "        memory: torch.Tensor,\n",
    "        tgt_mask: torch.Tensor = None,\n",
    "        memory_mask: torch.Tensor = None,\n",
    "        tgt_key_padding_mask: torch.Tensor = None,\n",
    "        memory_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tgt: Tensor of shape (batch, tgt_len, feat_dim)\n",
    "            memory: Tensor of shape (src_len, batch, d_model)\n",
    "        Returns:\n",
    "            Tensor of shape (batch, tgt_len, feat_dim)\n",
    "        \"\"\"\n",
    "        x = self.input_embedding(tgt)       # (batch, tgt_len, d_model)\n",
    "        x = x.transpose(0, 1)               # (tgt_len, batch, d_model)\n",
    "        for layer in self.layers:\n",
    "            x = layer(\n",
    "                x,\n",
    "                memory,\n",
    "                tgt_mask=tgt_mask,\n",
    "                memory_mask=memory_mask,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                memory_key_padding_mask=memory_key_padding_mask\n",
    "            )\n",
    "        x = x.transpose(0, 1)               # (batch, tgt_len, d_model)\n",
    "        return self.output_linear(x)        # project back to feat_dim\n",
    "\n",
    "        \n",
    "\n",
    "class TransformerWithHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int    = 64,   # raw feature dimension\n",
    "        patch_length: int = 16,   # sequence length consumed by encoder/decoder\n",
    "        d_model: int      = 64,   # hidden size inside the transformer\n",
    "        n_heads: int      = 4,\n",
    "        dim_ff: int       = 256,\n",
    "        n_layers: int     = 12,\n",
    "        dropout: float    = 0.1,\n",
    "        hidden_dim: int   = 256,\n",
    "        out_dim: int      = 64,\n",
    "        max_len: int      = 5000,\n",
    "        freeze_backbone: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 0) Project raw input dimension to patch length\n",
    "        self.input_proj = nn.Linear(input_dim, patch_length)\n",
    "\n",
    "        # 1) Encoder: processes the source sequence\n",
    "        self.encoder = TransformerEncoderCustom(\n",
    "            feat_dim = patch_length,\n",
    "            d_model  = d_model,\n",
    "            n_heads  = n_heads,\n",
    "            dim_ff   = dim_ff,\n",
    "            n_layers = n_layers,\n",
    "            dropout  = dropout,\n",
    "            max_len  = max_len,\n",
    "        )\n",
    "        if freeze_backbone:\n",
    "            for p in self.encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # 2) Decoder: generates target sequence using encoder memory\n",
    "        self.decoder = TransformerDecoderCustom(\n",
    "            feat_dim = patch_length,\n",
    "            d_model  = d_model,\n",
    "            n_heads  = n_heads,\n",
    "            dim_ff   = dim_ff,\n",
    "            n_layers = n_layers,\n",
    "            dropout  = dropout,\n",
    "            max_len  = max_len,\n",
    "        )\n",
    "\n",
    "        # 3) Task head: maps final decoder output to desired output dimension\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,                # (batch, src_len, input_dim)\n",
    "        tgt: torch.Tensor,                # (batch, tgt_len, input_dim)\n",
    "        src_mask: torch.Tensor = None,\n",
    "        src_key_padding_mask: torch.Tensor = None,\n",
    "        tgt_mask: torch.Tensor = None,\n",
    "        tgt_key_padding_mask: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        # 1) Encode source sequence to produce memory\n",
    "        src_patch = self.input_proj(src)  # (batch, src_len, patch_length)\n",
    "        memory = self.encoder(\n",
    "            src_patch,\n",
    "            src_mask=src_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask\n",
    "        )  # (src_len, batch, d_model)\n",
    "\n",
    "        # 2) Decode target sequence using encoder memory\n",
    "        tgt_patch = self.input_proj(tgt)  # (batch, tgt_len, patch_length)\n",
    "        dec_out = self.decoder(\n",
    "            tgt_patch,\n",
    "            memory,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=None,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=src_key_padding_mask\n",
    "        )  # (batch, tgt_len, d_model)\n",
    "\n",
    "        # 3) Use last time-step output from decoder for prediction\n",
    "        last_step = dec_out[:, -1, :]      # (batch, d_model)\n",
    "        return self.head(last_step)        # (batch, out_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e2503278-03dd-4b6d-8497-e9f9d83e10fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNWithHead(nn.Module):\n",
    "    \"\"\"\n",
    "    RNNWithHead (projected):\n",
    "      • Projects raw feature vectors from `input_dim` to `patch_length`\n",
    "      • Feeds the projected sequence to an RNN backbone\n",
    "      • Maps the last hidden state through an FC head\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int    = 64,   # raw feature dimension coming from DataLoader\n",
    "        patch_length: int = 16,   # dimension consumed by the RNN backbone\n",
    "        hidden_size: int  = 64,   # RNN hidden size\n",
    "        num_layers: int   = 12,   # number of stacked RNN layers\n",
    "        bidirectional: bool = True,\n",
    "        dropout: float      = 0.1,\n",
    "        hidden_dim: int     = 256, # FC-head hidden size\n",
    "        out_dim: int        = 64,  # FC-head output size\n",
    "        freeze_backbone: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 0) project raw 64-dim → 16-dim\n",
    "        self.input_proj = nn.Linear(input_dim, patch_length)\n",
    "\n",
    "        # 1) RNN backbone\n",
    "        self.backbone = nn.RNN(\n",
    "            input_size     = patch_length,\n",
    "            hidden_size    = hidden_size,\n",
    "            num_layers     = num_layers,\n",
    "            batch_first    = True,\n",
    "            bidirectional  = bidirectional,\n",
    "            dropout        = dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # 2) FC head\n",
    "        rnn_out_dim = hidden_size * (2 if bidirectional else 1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(rnn_out_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, input_dim=64)\n",
    "        returns: (batch, out_dim)\n",
    "        \"\"\"\n",
    "        x_proj = self.input_proj(x)           # (batch, seq_len, 16)\n",
    "        out, _ = self.backbone(x_proj)        # (batch, seq_len, rnn_out_dim)\n",
    "        feat   = out[:, -1, :]                # take last time step\n",
    "        return self.head(feat)                # (batch, out_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b210558c-7f51-4e6b-ad3a-8efa5237a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMWithHead(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTMWithHead (projected):\n",
    "      • Projects raw feature vectors from `input_dim` to a compact `patch_length`\n",
    "      • Feeds the projected sequence to an LSTM backbone\n",
    "      • Uses the last hidden state to drive an FC head for the downstream task\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int    = 64,   # raw feature dimension (e.g., 64)\n",
    "        patch_length: int = 16,   # dimension consumed by the LSTM backbone\n",
    "        hidden_size: int  = 64,   # LSTM hidden size\n",
    "        num_layers: int   = 12,   # number of stacked LSTM layers\n",
    "        bidirectional: bool = True,\n",
    "        dropout: float      = 0.1,\n",
    "        hidden_dim: int     = 256, # FC-head hidden size\n",
    "        out_dim: int        = 64,  # FC-head output size\n",
    "        freeze_backbone: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 0) Raw 64-dim → 16-dim patch projection\n",
    "        self.input_proj = nn.Linear(input_dim, patch_length)\n",
    "\n",
    "        # 1) LSTM backbone that expects `patch_length` features\n",
    "        self.backbone = nn.LSTM(\n",
    "            input_size     = patch_length,\n",
    "            hidden_size    = hidden_size,\n",
    "            num_layers     = num_layers,\n",
    "            batch_first    = True,\n",
    "            bidirectional  = bidirectional,\n",
    "            dropout        = dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # 2) FC head\n",
    "        lstm_out_dim = hidden_size * (2 if bidirectional else 1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(lstm_out_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, input_dim=64)\n",
    "        returns: (batch, out_dim)\n",
    "        \"\"\"\n",
    "        # project raw features to patch_length\n",
    "        x_proj = self.input_proj(x)             # (B, seq_len, 16)\n",
    "\n",
    "        # sequence modeling with LSTM\n",
    "        out, _ = self.backbone(x_proj)          # (B, seq_len, lstm_out_dim)\n",
    "\n",
    "        # take the last time-step representation\n",
    "        feat = out[:, -1, :]                    # (B, lstm_out_dim)\n",
    "\n",
    "        # downstream head\n",
    "        return self.head(feat)                  # (B, out_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4766f28-d102-4ed0-8272-7d5098ad9f15",
   "metadata": {},
   "source": [
    "## fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2e104da9-d8e8-42ea-a810-3567b9ae412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────\n",
    "# Shared hyper-parameters\n",
    "# ──────────────────────────\n",
    "INPUT_DIM     = 64     # raw feature dimension\n",
    "PATCH_LENGTH  = 16     # dimension fed to every backbone\n",
    "D_MODEL       = 64     # internal hidden size (GRU/LSTM/Transformer)\n",
    "N_LAYERS      = 12     # stacked layers\n",
    "HIDDEN_DIM    = 256    # head hidden dimension\n",
    "OUT_DIM       = 64     # head output dimension\n",
    "DROPOUT       = 0.1    # dropout for recurrent / transformer blocks\n",
    "BIDIRECTIONAL = True   # use bidirectional RNNs\n",
    "DEVICE        = \"cuda\"\n",
    "\n",
    "# ──────────────────────────\n",
    "# Model class catalog\n",
    "# ──────────────────────────\n",
    "MODEL_CATALOG = {\n",
    "    \"LWM_freeze_backbone\"     : LWMWithHead,\n",
    "    \"LWM_pretrained_Fine_tune\": LWMWithHead,\n",
    "    \"LWM_Fine_tune\"           : LWMWithHead,\n",
    "    \"gru\"                     : GRUWithHead,\n",
    "    \"RNN\"                     : RNNWithHead,\n",
    "    \"LSTM\"                    : LSTMWithHead,\n",
    "    \"Transformer\"             : TransformerWithHead,\n",
    "}\n",
    "\n",
    "# ──────────────────────────\n",
    "# Per-model constructor kwargs\n",
    "# ──────────────────────────\n",
    "MODEL_PARAMS = {\n",
    "    # ── LWM variants ─────────────────────────────\n",
    "    \"LWM_freeze_backbone\": {\n",
    "        \"input_dim\"       : INPUT_DIM,\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"d_model\"         : D_MODEL,\n",
    "        \"max_len\"         : PATCH_LENGTH + 1,\n",
    "        \"n_layers\"        : N_LAYERS,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"freeze_backbone\" : True,\n",
    "        \"checkpoint_path\" : \"./model_weights.pth\",\n",
    "        \"device\"          : DEVICE,\n",
    "    },\n",
    "    \"LWM_pretrained_Fine_tune\": {\n",
    "        \"input_dim\"       : INPUT_DIM,\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"d_model\"         : D_MODEL,\n",
    "        \"max_len\"         : PATCH_LENGTH + 1,\n",
    "        \"n_layers\"        : N_LAYERS,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"freeze_backbone\" : False,\n",
    "        \"checkpoint_path\" : \"./model_weights.pth\",\n",
    "        \"device\"          : DEVICE,\n",
    "    },\n",
    "    \"LWM_Fine_tune\": {\n",
    "        \"input_dim\"       : INPUT_DIM,\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"d_model\"         : D_MODEL,\n",
    "        \"max_len\"         : PATCH_LENGTH + 1,\n",
    "        \"n_layers\"        : N_LAYERS,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"freeze_backbone\" : False,\n",
    "        \"checkpoint_path\" : None,\n",
    "        \"device\"          : DEVICE,\n",
    "    },\n",
    "\n",
    "    # ── GRU (projected) ──────────────────────────\n",
    "    \"gru\": {\n",
    "        \"input_dim\"       : INPUT_DIM,     # 64 → project → 16\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"d_model\"         : D_MODEL,\n",
    "        \"n_layers\"        : N_LAYERS,\n",
    "        \"bidirectional\"   : BIDIRECTIONAL,\n",
    "        \"dropout\"         : DROPOUT,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"freeze_backbone\" : False,\n",
    "    },\n",
    "\n",
    "    # ── Vanilla RNN (projected) ──────────────────\n",
    "    \"RNN\": {\n",
    "        \"input_dim\"       : INPUT_DIM,\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"hidden_size\"     : D_MODEL,\n",
    "        \"num_layers\"      : N_LAYERS,\n",
    "        \"bidirectional\"   : BIDIRECTIONAL,\n",
    "        \"dropout\"         : 0.0,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"freeze_backbone\" : False,\n",
    "    },\n",
    "\n",
    "    # ── LSTM (projected) ─────────────────────────\n",
    "    \"LSTM\": {\n",
    "        \"input_dim\"       : INPUT_DIM,\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"hidden_size\"     : D_MODEL,\n",
    "        \"num_layers\"      : N_LAYERS,\n",
    "        \"bidirectional\"   : BIDIRECTIONAL,\n",
    "        \"dropout\"         : DROPOUT,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"freeze_backbone\" : False,\n",
    "    },\n",
    "\n",
    "    # ── Transformer (projected) ──────────────────\n",
    "    \"Transformer\": {\n",
    "        \"input_dim\"       : INPUT_DIM,\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"d_model\"         : D_MODEL,\n",
    "        \"n_heads\"         : 4,\n",
    "        \"dim_ff\"          : 256,\n",
    "        \"n_layers\"        : N_LAYERS,\n",
    "        \"dropout\"         : DROPOUT,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"max_len\"         : PATCH_LENGTH + 1,\n",
    "        \"freeze_backbone\" : False,\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c37de2-4bc1-42aa-9a9e-cfa43bf0c3fd",
   "metadata": {},
   "source": [
    "## model evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1e9e706d-4875-4ee2-93d9-f792571fe2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def rmse(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Root-Mean-Squared Error\n",
    "    \"\"\"\n",
    "    return torch.sqrt(F.mse_loss(pred, target, reduction=\"mean\"))   # √MSE\n",
    "\n",
    "def nmse(pred: torch.Tensor, target: torch.Tensor, eps : float = 1e-12) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Normalized MSE  =  E[‖ŷ − y‖²] / E[‖y‖²]\n",
    "    \"\"\"\n",
    "    # (B, …) → (B,)  \n",
    "    mse_per_sample   = ((pred - target)**2).view(pred.size(0), -1).sum(dim=1)\n",
    "    power_per_sample = (target**2).view(target.size(0), -1).sum(dim=1) + eps\n",
    "    return (mse_per_sample / power_per_sample).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3930b75e-0895-4fb1-8039-744c86730b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_evaluate(model, loader, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Validation loop for IterableDataset.\n",
    "    Returns average RMSE and NMSE over all samples.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_rmse, total_nmse, total_samples = 0.0, 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, masked_pos, target in loader:\n",
    "            # Move to device\n",
    "            input_ids, masked_pos, target = (\n",
    "                input_ids.to(device),\n",
    "                masked_pos.to(device),\n",
    "                target.to(device),\n",
    "            )\n",
    "            # Batch size\n",
    "            bs = input_ids.size(0)\n",
    "\n",
    "            # Forward\n",
    "            pred = model(input_ids, masked_pos)\n",
    "\n",
    "            # Accumulate batch metrics\n",
    "            total_rmse    += rmse(pred, target).item() * bs\n",
    "            total_nmse    += nmse(pred, target).item() * bs\n",
    "            total_samples += bs\n",
    "\n",
    "    # Compute averages\n",
    "    return {\n",
    "        \"RMSE\": total_rmse / total_samples,\n",
    "        \"NMSE\": total_nmse / total_samples\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "92d4ceda-b090-4b01-91a4-698c319420f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unmasked_evaluate(model, loader, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Validation loop for IterableDataset.\n",
    "    Returns average RMSE and NMSE over all samples.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_rmse, total_nmse, total_samples = 0.0, 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, target in loader:\n",
    "            # Move to device\n",
    "            input_ids,target = (\n",
    "                input_ids.to(device),\n",
    "                target.to(device),\n",
    "            )\n",
    "            # Batch size\n",
    "            bs = input_ids.size(0)\n",
    "\n",
    "            # Forward\n",
    "            pred = model(input_ids)\n",
    "\n",
    "            # Accumulate batch metrics\n",
    "            total_rmse    += rmse(pred, target).item() * bs\n",
    "            total_nmse    += nmse(pred, target).item() * bs\n",
    "            total_samples += bs\n",
    "\n",
    "    # Compute averages\n",
    "    return {\n",
    "        \"RMSE\": total_rmse / total_samples,\n",
    "        \"NMSE\": total_nmse / total_samples\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c0dcd-d4af-4d7b-87cd-154d81d210fb",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "317fde92-dd4d-4092-b93c-7b1b83674c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training LWM_freeze_backbone ===\n",
      "Model loaded successfully from ./model_weights.pth to cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/10] TrainLoss: 0.0232  Val RMSE: 0.1001  Val NMSE: 4.6684e-02  Val NMSE_dB: -13.3 dB  TrainTime: 390.42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/10] TrainLoss: 0.0084  Val RMSE: 0.0956  Val NMSE: 4.2755e-02  Val NMSE_dB: -13.7 dB  TrainTime: 398.22s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/10] TrainLoss: 0.0076  Val RMSE: 0.0891  Val NMSE: 3.7240e-02  Val NMSE_dB: -14.3 dB  TrainTime: 386.59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04/10] TrainLoss: 0.0070  Val RMSE: 0.0880  Val NMSE: 3.6033e-02  Val NMSE_dB: -14.4 dB  TrainTime: 375.14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/10] TrainLoss: 0.0068  Val RMSE: 0.0876  Val NMSE: 3.5339e-02  Val NMSE_dB: -14.5 dB  TrainTime: 380.29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/10] TrainLoss: 0.0066  Val RMSE: 0.0865  Val NMSE: 3.4159e-02  Val NMSE_dB: -14.7 dB  TrainTime: 401.33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/10] TrainLoss: 0.0063  Val RMSE: 0.0847  Val NMSE: 3.2553e-02  Val NMSE_dB: -14.9 dB  TrainTime: 394.96s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/10] TrainLoss: 0.0060  Val RMSE: 0.0831  Val NMSE: 3.1275e-02  Val NMSE_dB: -15.0 dB  TrainTime: 379.29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/10] TrainLoss: 0.0058  Val RMSE: 0.0823  Val NMSE: 3.0631e-02  Val NMSE_dB: -15.1 dB  TrainTime: 395.84s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/10] TrainLoss: 0.0057  Val RMSE: 0.0817  Val NMSE: 3.0147e-02  Val NMSE_dB: -15.2 dB  TrainTime: 373.39s\n",
      "🕒 LWM_freeze_backbone – avg train time / epoch: 387.55s\n",
      "\n",
      "=== Training LWM_pretrained_Fine_tune ===\n",
      "Model loaded successfully from ./model_weights.pth to cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/10] TrainLoss: 0.0101  Val RMSE: 0.0806  Val NMSE: 2.8838e-02  Val NMSE_dB: -15.4 dB  TrainTime: 460.31s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/10] TrainLoss: 0.0042  Val RMSE: 0.0756  Val NMSE: 2.3791e-02  Val NMSE_dB: -16.2 dB  TrainTime: 477.55s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/10] TrainLoss: 0.0029  Val RMSE: 0.0708  Val NMSE: 2.0859e-02  Val NMSE_dB: -16.8 dB  TrainTime: 482.76s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04/10] TrainLoss: 0.0025  Val RMSE: 0.0718  Val NMSE: 2.1319e-02  Val NMSE_dB: -16.7 dB  TrainTime: 461.24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/10] TrainLoss: 0.0024  Val RMSE: 0.0715  Val NMSE: 2.1098e-02  Val NMSE_dB: -16.8 dB  TrainTime: 469.84s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/10] TrainLoss: 0.0022  Val RMSE: 0.0714  Val NMSE: 2.1040e-02  Val NMSE_dB: -16.8 dB  TrainTime: 486.54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/10] TrainLoss: 0.0021  Val RMSE: 0.0712  Val NMSE: 2.0877e-02  Val NMSE_dB: -16.8 dB  TrainTime: 469.16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/10] TrainLoss: 0.0021  Val RMSE: 0.0703  Val NMSE: 2.0374e-02  Val NMSE_dB: -16.9 dB  TrainTime: 441.73s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/10] TrainLoss: 0.0020  Val RMSE: 0.0693  Val NMSE: 1.9853e-02  Val NMSE_dB: -17.0 dB  TrainTime: 436.37s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/10] TrainLoss: 0.0020  Val RMSE: 0.0681  Val NMSE: 1.9187e-02  Val NMSE_dB: -17.2 dB  TrainTime: 459.01s\n",
      "🕒 LWM_pretrained_Fine_tune – avg train time / epoch: 464.45s\n",
      "\n",
      "=== Training LWM_Fine_tune ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/10] TrainLoss: 0.0095  Val RMSE: 0.0709  Val NMSE: 2.1526e-02  Val NMSE_dB: -16.7 dB  TrainTime: 507.39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/10] TrainLoss: 0.0034  Val RMSE: 0.0673  Val NMSE: 1.8899e-02  Val NMSE_dB: -17.2 dB  TrainTime: 477.56s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/10] TrainLoss: 0.0027  Val RMSE: 0.0655  Val NMSE: 1.8007e-02  Val NMSE_dB: -17.4 dB  TrainTime: 506.18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04/10] TrainLoss: 0.0025  Val RMSE: 0.0642  Val NMSE: 1.7336e-02  Val NMSE_dB: -17.6 dB  TrainTime: 453.26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/10] TrainLoss: 0.0023  Val RMSE: 0.0639  Val NMSE: 1.7187e-02  Val NMSE_dB: -17.6 dB  TrainTime: 394.30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, yb)\n\u001b[1;32m     71\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 72\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     75\u001b[0m run_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Unified training / validation script\n",
    "------------------------------------\n",
    "* Trains every architecture listed in MODEL_CATALOG\n",
    "* Chooses masked / un-masked DataLoader automatically\n",
    "* Reports per-epoch speed & validation scores\n",
    "* Saves **best** and **last** checkpoints under ./checkpoints/\n",
    "\"\"\"\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 0) Globals and hyper-parameters\n",
    "# ─────────────────────────────────────────────\n",
    "device      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion   = nn.MSELoss().to(device)\n",
    "\n",
    "NUM_EPOCHS  = 10\n",
    "LR          = 1e-4                         # learning-rate\n",
    "CKPT_DIR    = Path(\"checkpoints\")          # where *.pth files will be stored\n",
    "CKPT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "total_start = time.time()                  # wall-clock timer for *all* models\n",
    "results     = {}                           # best-epoch NMSE(dB) for every model\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 1) Train / validate each model\n",
    "# ─────────────────────────────────────────────\n",
    "for model_name, ModelCls in MODEL_CATALOG.items():\n",
    "\n",
    "    print(f\"\\n=== Training {model_name} ===\")\n",
    "    model_args = MODEL_PARAMS[model_name]\n",
    "    model      = ModelCls(**model_args).to(device)\n",
    "\n",
    "    # collect only trainable parameters\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    if len(trainable_params) == 0:\n",
    "        print(f\"⚠️  '{model_name}' has no trainable parameters — skipping.\")\n",
    "        results[model_name] = float(\"nan\")\n",
    "        continue\n",
    "\n",
    "    optimizer   = torch.optim.Adam(trainable_params, lr=LR)\n",
    "    epoch_times = []                       # per-epoch training duration\n",
    "    best_nmse   = float(\"inf\")             # track the best val-NMSE\n",
    "\n",
    "    # pick loaders / evaluation fn based on model family\n",
    "    uses_mask  = model_name.startswith(\"LWM_\")\n",
    "    tr_loader  = masked_train_loader if uses_mask else unmasked_train_loader\n",
    "    val_loader = masked_val_loader  if uses_mask else unmasked_val_loader\n",
    "    eval_fn    = masked_evaluate    if uses_mask else unmasked_evaluate\n",
    "\n",
    "    # ── EPOCH LOOP ──────────────────────────\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "\n",
    "        # ---------- TRAIN ----------\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        run_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(tr_loader,\n",
    "                    desc=f\"[{model_name} {epoch:02d}/{NUM_EPOCHS}] train\",\n",
    "                    leave=False)\n",
    "\n",
    "        for b, batch in enumerate(pbar, 1):\n",
    "            if uses_mask:\n",
    "                xb, mpos, yb = [x.to(device) for x in batch]\n",
    "                pred = model(xb, mpos).squeeze(-1)\n",
    "            else:\n",
    "                xb, yb = [x.to(device) for x in batch]\n",
    "                pred   = model(xb)\n",
    "\n",
    "            loss = criterion(pred, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            run_loss += loss.item()\n",
    "            if b % 100 == 0:\n",
    "                pbar.set_postfix(train_loss=run_loss / b)\n",
    "\n",
    "        epoch_times.append(time.time() - t0)\n",
    "        avg_train_loss = run_loss / b\n",
    "\n",
    "        # ---------- VALID ----------\n",
    "        metrics   = eval_fn(model, val_loader, device)\n",
    "        val_rmse  = metrics[\"RMSE\"]\n",
    "        val_nmse  = metrics[\"NMSE\"]\n",
    "        val_nmse_db = 10 * torch.log10(torch.tensor(val_nmse)).item()\n",
    "\n",
    "        # save best checkpoint\n",
    "        if val_nmse < best_nmse:\n",
    "            best_nmse = val_nmse\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                CKPT_DIR / f\"{model_name}_best.pth\"\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            f\"[{epoch:02d}/{NUM_EPOCHS}] \"\n",
    "            f\"TrainLoss: {avg_train_loss:.4f}  \"\n",
    "            f\"Val RMSE: {val_rmse:.4f}  \"\n",
    "            f\"Val NMSE: {val_nmse:.4e}  \"\n",
    "            f\"Val NMSE_dB: {val_nmse_db:.1f} dB  \"\n",
    "            f\"TrainTime: {epoch_times[-1]:.2f}s\"\n",
    "        )\n",
    "\n",
    "    # after all epochs – save *last* weights\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        CKPT_DIR / f\"{model_name}_last.pth\"\n",
    "    )\n",
    "\n",
    "    avg_ep_time = sum(epoch_times) / len(epoch_times)\n",
    "    print(f\"🕒 {model_name} – avg train time / epoch: {avg_ep_time:.2f}s\")\n",
    "\n",
    "    # store best NMSE_dB for the summary\n",
    "    results[model_name] = 10 * math.log10(best_nmse)\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 2) Summary\n",
    "# ─────────────────────────────────────────────\n",
    "print(\"\\n=== Summary of best NMSE(dB) by model ===\")\n",
    "for name, nmse_db in results.items():\n",
    "    print(f\"{name:25s}: {nmse_db if not math.isnan(nmse_db) else 'skipped':>6}\")\n",
    "\n",
    "print(f\"\\nTotal training time for all models: {time.time() - total_start:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e41b5c9-77dd-4300-ac44-925dd560ccfd",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac929b23-8f2c-4b46-b746-d450e45bbe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 0)  Load the *best* checkpoints into `trained_models`\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "CKPT_DIR = Path(\"checkpoints\")                 # folder with *.pth files\n",
    "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "trained_models = {}\n",
    "for name, ModelCls in MODEL_CATALOG.items():\n",
    "    ckpt_path = CKPT_DIR / f\"{name}_best.pth\"\n",
    "    if ckpt_path.exists():\n",
    "        model = ModelCls(**MODEL_PARAMS[name])\n",
    "        model.load_state_dict(torch.load(ckpt_path, map_location=\"cpu\"))\n",
    "        trained_models[name] = model          # keep on CPU for now\n",
    "    else:\n",
    "        print(f\"⚠️  {ckpt_path} not found — skipping this model.\")\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 1)  Pure-inference timing loop (no loss / labels)\n",
    "# ─────────────────────────────────────────────\n",
    "torch.backends.cudnn.benchmark = True          # let cuDNN pick fastest kernels\n",
    "INFER_TIME = {}                                # {model: (total, per_batch, per_sample)}\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    uses_mask = name.startswith(\"LWM_\")\n",
    "    v_loader  = masked_val_loader if uses_mask else unmasked_val_loader\n",
    "\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    # ― Warm-up one batch to ramp GPU clocks and cache kernels\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(v_loader))\n",
    "        if uses_mask:\n",
    "            seq, mpos, _ = [x.to(device) for x in batch]\n",
    "            _ = model(seq, mpos)\n",
    "        else:\n",
    "            seq, _ = [x.to(device) for x in batch]\n",
    "            _ = model(seq)\n",
    "\n",
    "    # ― Timed inference pass over the entire loader\n",
    "    torch.cuda.synchronize()\n",
    "    t0        = time.time()\n",
    "    n_batches = 0\n",
    "    n_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in v_loader:\n",
    "            if uses_mask:\n",
    "                seq, mpos, _ = [x.to(device) for x in batch]\n",
    "                _  = model(seq, mpos)\n",
    "                bs = seq.size(0)\n",
    "            else:\n",
    "                seq, _ = [x.to(device) for x in batch]\n",
    "                _  = model(seq)\n",
    "                bs = seq.size(0)\n",
    "\n",
    "            n_batches += 1\n",
    "            n_samples += bs\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    INFER_TIME[name] = (\n",
    "        elapsed,                # total seconds\n",
    "        elapsed / n_batches,    # seconds per batch\n",
    "        elapsed / n_samples     # seconds per sample\n",
    "    )\n",
    "\n",
    "    print(f\"⏱ {name:25s} | total {elapsed:6.2f}s  \"\n",
    "          f\"| /batch {elapsed/n_batches*1e3:6.2f} ms  \"\n",
    "          f\"| /sample {elapsed/n_samples*1e3:6.2f} ms\")\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 2)  Pretty summary table\n",
    "# ─────────────────────────────────────────────\n",
    "print(\"\\n=== Inference-time summary ===\")\n",
    "header = f\"{'model':25s} | {'total [s]':>9} | {'/batch [ms]':>12} | {'/sample [ms]':>13}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "for n, (tot, pb, ps) in INFER_TIME.items():\n",
    "    print(f\"{n:25s} | {tot:9.2f} | {pb*1e3:12.2f} | {ps*1e3:13.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b547a829-25e4-446b-a962-6da55cc44176",
   "metadata": {},
   "source": [
    "## Compare trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a90966-4bb9-40c9-a8a6-6d2435ceec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Report trainable parameters for every model\n",
    "# ─────────────────────────────────────────────\n",
    "print(\"\\n=== Trainable parameters per model ===\")\n",
    "for name, ModelCls in MODEL_CATALOG.items():\n",
    "    # instantiate model with its params (on CPU is fine for counting)\n",
    "    model = ModelCls(**MODEL_PARAMS[name])\n",
    "    count = count_trainable_params(model)\n",
    "    print(f\"{name:25s}: {count:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff42f6d-3ed7-43b4-b35e-16e0d7345bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f748d3-0edc-4e77-bbf6-be96d7d8d99e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a224fa-03a3-4c41-989a-47e2327c5c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bea2d4-1027-495f-a2e3-efbc703aff5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dd53c6-5322-4744-95d6-90cbe8da1d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9f299d-07db-4e49-aac5-ebddf12c7538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e3e680-e8e0-43ea-b4c5-d519166d2c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a842292-0f59-4b7a-9fe2-859d81fc722f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b160c67-2efe-41f2-823d-62eae883935f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
