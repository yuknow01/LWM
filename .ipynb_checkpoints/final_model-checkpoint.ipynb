{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0939df15-abc7-40e7-83bf-f12820dd42f2",
   "metadata": {},
   "source": [
    "# All Model saves here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cbf7fb-304f-432d-a3f9-580052d5a375",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e94396-1d80-4df3-8648-19368f82c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import DeepMIMOv3\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "plt . rcParams [ 'figure.figsize' ]  =  [ 12 ,  8 ]  # 기본 플롯 크기 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb2d8b4-7d3d-43ae-b997-2a4e6eab9a1a",
   "metadata": {},
   "source": [
    "## GPU Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a6479f7-d2b2-4632-b3a8-ffe3c49a6608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# GPU \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea87b9c9-a4b1-4442-b26f-606828c4dc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.6\n",
      "90501\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)                   \n",
    "print(torch.backends.cudnn.version())       \n",
    "print(\"CUDA available:\", torch.cuda.is_available())  # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dc1a47-3bc0-4236-813e-c7be4c21bd41",
   "metadata": {},
   "source": [
    "## DeepMIMOv3 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "821eb69b-806e-4694-adf9-fd0efcbf5e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = DeepMIMOv3.default_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1d1f75d-54c0-4198-990f-15eaeec28e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change parameters for the setup\n",
    "# Scenario O1_60 extracted at the dataset_folder\n",
    "#LWM dynamic senario\n",
    "# parameters['dataset_folder'] = r'/content/drive/MyDrive/Colab Notebooks/LWM'\n",
    "scene = 15 # scene 15\n",
    "# change my linux route\n",
    "parameters['dataset_folder'] = '/home/dlghdbs200/LWM'\n",
    "\n",
    "# scnario = 02_dyn_3p5 <- download file\n",
    "parameters['scenario'] = 'O2_dyn_3p5'\n",
    "parameters['dynamic_scenario_scenes'] = np.arange(scene) #scene 0~9\n",
    "\n",
    "# Up to 10 multipath paths per user-to-base station channel\n",
    "parameters['num_paths'] = 10\n",
    "\n",
    "# User rows 1-100\n",
    "parameters['user_rows'] = np.arange(100)\n",
    "# User subsampling\n",
    "parameters['user_subsampling'] = 0.01\n",
    "\n",
    "# Activate only the first basestation\n",
    "parameters['active_BS'] = np.array([1])\n",
    "\n",
    "parameters['activate_OFDM'] = 1\n",
    "\n",
    "parameters['OFDM']['bandwidth'] = 0.05 # 50 MHz\n",
    "parameters['OFDM']['subcarriers'] = 512 # OFDM with 512 subcarriers\n",
    "parameters['OFDM']['selected_subcarriers'] = np.arange(0, 64, 1)\n",
    "#parameters['OFDM']['subcarriers_limit'] = 64 # Keep only first 64 subcarriers\n",
    "\n",
    "parameters['ue_antenna']['shape'] = np.array([1, 1]) # Single antenna\n",
    "parameters['bs_antenna']['shape'] = np.array([1, 32]) # ULA of 32 elements\n",
    "#parameters['bs_antenna']['rotation'] = np.array([0, 30, 90]) # ULA of 32 elements\n",
    "#parameters['ue_antenna']['rotation'] = np.array([[0, 30], [30, 60], [60, 90]]) # ULA of 32 elements\n",
    "#parameters['ue_antenna']['radiation_pattern'] = 'isotropic'\n",
    "#parameters['bs_antenna']['radiation_pattern'] = 'halfwave-dipole'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a9e7bb3-7cb7-4742-a949-9dd84ca31a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                             | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following parameters seem unnecessary:\n",
      "{'activate_OFDM'}\n",
      "\n",
      "Scene 1/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  44%|███████████████████████▏                            | 30697/69006 [00:00<00:00, 306945.41it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 313376.96it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 8015.21it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4634.59it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 739.48it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 2/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  47%|████████████████████████▎                           | 32287/69006 [00:00<00:00, 322846.45it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 326412.38it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 7942.68it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5899.16it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 460.91it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 3/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 348279.96it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 7695.45it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 7796.10it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 375.16it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 4/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  50%|█████████████████████████▉                          | 34413/69006 [00:00<00:00, 344117.20it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 331206.18it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 7342.02it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5468.45it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1018.03it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 5/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  49%|█████████████████████████▎                          | 33563/69006 [00:00<00:00, 335601.52it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 326403.54it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 7649.90it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 3266.59it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 578.60it/s]\u001b[A\n",
      " 33%|████████████████████████████▎                                                        | 1/3 [00:07<00:14,  7.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenes 0–4 generation time: 6.93s\n",
      "The following parameters seem unnecessary:\n",
      "{'activate_OFDM'}\n",
      "\n",
      "Scene 1/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  50%|█████████████████████████▊                          | 34186/69006 [00:00<00:00, 341839.14it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 337904.94it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 8456.82it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 7294.44it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 953.68it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 2/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 349790.61it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 8430.45it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5468.45it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 735.46it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 3/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 352216.49it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 8299.43it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 6584.46it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1091.98it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 4/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 361264.09it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 7574.05it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4755.45it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1045.18it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 5/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  47%|████████████████████████▍                           | 32506/69006 [00:00<00:00, 325040.94it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 311281.62it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 7637.24it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 7182.03it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 736.36it/s]\u001b[A\n",
      " 67%|████████████████████████████████████████████████████████▋                            | 2/3 [00:13<00:06,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenes 5–9 generation time: 6.61s\n",
      "The following parameters seem unnecessary:\n",
      "{'activate_OFDM'}\n",
      "\n",
      "Scene 1/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 351877.78it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 8166.33it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 7810.62it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 979.29it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 2/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 353596.90it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 8034.77it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 8081.51it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 751.67it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 3/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 358623.53it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 7496.79it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 6043.67it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1055.97it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 4/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 353583.07it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 7069.89it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 7145.32it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 690.65it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 5/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 341961.54it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 7645.76it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 6204.59it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 977.92it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:20<00:00,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenes 10–14 generation time: 6.66s\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## dataset setting (chunked on‑the‑fly generation)\n",
    "import time, gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 0~999 scene index , process 50 at that time\n",
    "scene_indices = np.arange(scene)\n",
    "chunk_size   = 5\n",
    "all_data     = []\n",
    "\n",
    "# Call generate_data for each scene chunk\n",
    "for i in tqdm(range(0, len(scene_indices), chunk_size)):\n",
    "    chunk = scene_indices[i : i+chunk_size].tolist()\n",
    "    parameters['dynamic_scenario_scenes'] = chunk\n",
    "\n",
    "    start = time.time()\n",
    "    data_chunk = DeepMIMOv3.generate_data(parameters)\n",
    "    print(f\"Scenes {chunk[0]}–{chunk[-1]} generation time: {time.time() - start:.2f}s\")\n",
    "\n",
    "    # combine all_data or save in the Disk\n",
    "    all_data.extend(data_chunk)\n",
    "\n",
    "    # free memory \n",
    "    del data_chunk\n",
    "    gc.collect()\n",
    "\n",
    "# comvine Dataset\n",
    "dataset = all_data\n",
    "\n",
    "\n",
    "print(parameters['user_rows'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc75e14-9605-49b0-8034-2a1dc367f97e",
   "metadata": {},
   "source": [
    "## About Information\n",
    "User : 737\n",
    "UE antenna : 1\n",
    "BS antenna : 32  Shape(a+bj)\n",
    "subcarrier : 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48148074-4682-4ab8-acbe-71ef042a5020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unmasked Data Model(gru\n",
    "# separate maksed data and unmasked data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe56d11-0ebb-4c13-b4c2-61fadd9c714b",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ba0a2d3-ee35-43dc-b457-bd400e5b75c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import IterableDataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class UnMaskedChannelSeqDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    IterableDataset (un-masked version)\n",
    "\n",
    "    * Task : predict the next-step channel vector from the past `seq_len` steps.\n",
    "    * Pipeline\n",
    "        1. **Power-normalise** each complex channel vector → real + imag concat.\n",
    "        2. **Min–Max scale** inputs and targets with *one* shared scaler\n",
    "           (fitted on the same power-normalised data).\n",
    "        3. Yield `(sequence, target)` pairs as `torch.FloatTensor`.\n",
    "    * Optionally reuse externally provided scalers (train/val split consistency).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        scenes,\n",
    "        seq_len: int = 5,\n",
    "        eps: float   = 1e-9,\n",
    "        scalers: tuple[MinMaxScaler, MinMaxScaler] | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scenes  = scenes\n",
    "        self.seq_len = seq_len\n",
    "        self.eps     = eps\n",
    "\n",
    "        # --- channel tensor dimensions --------------------------------------\n",
    "        ch0          = scenes[0][0]['user']['channel']        # (U, 1, A, S)\n",
    "        self.U       = ch0.shape[0]                           # users\n",
    "        self.A       = ch0.shape[2]                           # BS antennas\n",
    "        self.S       = ch0.shape[3]                           # sub-carriers\n",
    "        self.vec_len = 2 * self.A                            # real + imag\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # 1) Fit / reuse MinMax scalers on power-normalised data\n",
    "        # --------------------------------------------------------------------\n",
    "        if scalers is None:\n",
    "            X_list, y_list = [], []\n",
    "            T = len(scenes)\n",
    "            for t in range(self.seq_len, T):\n",
    "                past  = scenes[t - self.seq_len : t]\n",
    "                s_tgt = scenes[t]\n",
    "\n",
    "                for u in range(self.U):\n",
    "                    for s in range(self.S):\n",
    "                        # power-normalised sequence (seq_len, vec_len)\n",
    "                        seq_np = np.stack([\n",
    "                            self._power_norm(p[0]['user']['channel'][u, 0, :, s])\n",
    "                            for p in past\n",
    "                        ], axis=0).astype(np.float32)\n",
    "\n",
    "                        # power-normalised target   (vec_len,)\n",
    "                        tgt_np = self._power_norm(\n",
    "                            s_tgt[0]['user']['channel'][u, 0, :, s]\n",
    "                        ).astype(np.float32)\n",
    "\n",
    "                        # skip if empty (all zeros)\n",
    "                        if not np.any(seq_np) or not np.any(tgt_np):\n",
    "                            continue\n",
    "\n",
    "                        X_list.append(seq_np.reshape(-1, self.vec_len))\n",
    "                        y_list.append(tgt_np)\n",
    "\n",
    "            X_all = np.vstack(X_list)          # (N*seq_len, vec_len)\n",
    "            y_all = np.stack(y_list)           # (N, vec_len)\n",
    "            self.scaler_x = MinMaxScaler().fit(X_all)\n",
    "            self.scaler_y = MinMaxScaler().fit(y_all)\n",
    "        else:\n",
    "            # use pre-computed scalers (train/val share the same)\n",
    "            self.scaler_x, self.scaler_y = scalers\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # iterator\n",
    "    # ------------------------------------------------------------------------\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Yields:\n",
    "            seq_tensor   : FloatTensor (seq_len, vec_len)\n",
    "            target_tensor: FloatTensor (vec_len,)\n",
    "        Both tensors are power-normalised **and** Min–Max scaled.\n",
    "        \"\"\"\n",
    "        T = len(self.scenes)\n",
    "        for t in range(self.seq_len, T):\n",
    "            past  = self.scenes[t - self.seq_len : t]\n",
    "            s_tgt = self.scenes[t]\n",
    "\n",
    "            for u in range(self.U):\n",
    "                for s in range(self.S):\n",
    "                    seq_np = np.stack([\n",
    "                        self._power_norm(p[0]['user']['channel'][u, 0, :, s])\n",
    "                        for p in past\n",
    "                    ], axis=0)\n",
    "                    tgt_np = self._power_norm(\n",
    "                        s_tgt[0]['user']['channel'][u, 0, :, s]\n",
    "                    )\n",
    "\n",
    "                    if not np.any(seq_np) or not np.any(tgt_np):\n",
    "                        continue\n",
    "\n",
    "                    # identical scalers for train / val\n",
    "                    N, D = seq_np.shape\n",
    "                    seq_np = self.scaler_x.transform(seq_np.reshape(-1, D)).reshape(N, D)\n",
    "                    tgt_np = self.scaler_y.transform(tgt_np.reshape(1, -1)).reshape(-1,)\n",
    "\n",
    "                    yield torch.from_numpy(seq_np), torch.from_numpy(tgt_np)\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # helpers\n",
    "    # ------------------------------------------------------------------------\n",
    "    def _power_norm(self, h: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert complex vector → real|imag concatenation\n",
    "        and force average power to 1.\n",
    "        \"\"\"\n",
    "        v      = np.concatenate([h.real, h.imag]).astype(np.float32)\n",
    "        power  = np.mean(v * v) + self.eps\n",
    "        return v / np.sqrt(power)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Rough size estimate (not used by IterableDataset).\"\"\"\n",
    "        return (len(self.scenes) - self.seq_len) * self.U * self.S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97d365ac-69a6-494c-bfc7-198bdeb29582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import IterableDataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "class MaskedChannelSeqDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    IterableDataset for masked channel sequence data.\n",
    "\n",
    "    - Predicts the next-step channel vector from a sequence of past vectors.\n",
    "    - Applies power normalization and MinMax scaling to both inputs and targets.\n",
    "    - Masks 15% of the patches according to:\n",
    "        * 80% chance: replace selected patch with zeros\n",
    "        * 10% chance: replace selected patch with Gaussian noise\n",
    "        * 10% chance: leave the selected patch unchanged\n",
    "      The other 85% of samples are returned unmasked.\n",
    "    \"\"\"\n",
    "    def __init__(self, scenes, seq_len=5, eps=1e-9, noise_std=1.0):\n",
    "        super().__init__()\n",
    "        self.scenes    = scenes\n",
    "        self.seq_len   = seq_len\n",
    "        self.eps       = eps\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        # Determine dimensions: U=users, A=antennas, S=subcarriers\n",
    "        ch0 = scenes[0][0]['user']['channel']   # shape: (U, 1, A, S)\n",
    "        self.U       = ch0.shape[0]\n",
    "        self.A       = ch0.shape[2]\n",
    "        self.S       = ch0.shape[3]\n",
    "        self.vec_len = 2 * self.A               # real+imag concatenated\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # 1) PRECOMPUTE power-norm → fit MinMax scalers on exactly the same data\n",
    "        # ----------------------------------------------------------------------\n",
    "        X_list, y_list = [], []\n",
    "        T = len(scenes)\n",
    "        for t in range(self.seq_len, T):\n",
    "            past         = scenes[t - self.seq_len : t]\n",
    "            target_scene = scenes[t]\n",
    "            for u in range(self.U):\n",
    "                for s in range(self.S):\n",
    "                    # power-normalize each time-slice in the sequence\n",
    "                    seq_np = np.stack([\n",
    "                        self._power_norm(ps[0]['user']['channel'][u,0,:,s])\n",
    "                        for ps in past\n",
    "                    ], axis=0).astype(np.float32)  # (seq_len, vec_len)\n",
    "\n",
    "                    # power-normalize target\n",
    "                    tgt_np = self._power_norm(\n",
    "                        target_scene[0]['user']['channel'][u,0,:,s]\n",
    "                    ).astype(np.float32)            # (vec_len,)\n",
    "\n",
    "                    # skip if invalid\n",
    "                    if not np.any(seq_np) or not np.any(tgt_np):\n",
    "                        continue\n",
    "\n",
    "                    # flatten sequence for scaler\n",
    "                    X_list.append(seq_np.reshape(-1, self.vec_len))\n",
    "                    y_list.append(tgt_np)\n",
    "\n",
    "        # fit scalers on the exact same power-normalized data\n",
    "        X_all = np.vstack(X_list)  # (num_samples*seq_len, vec_len)\n",
    "        y_all = np.stack(y_list)   # (num_samples, vec_len)\n",
    "        self.scaler_x = MinMaxScaler().fit(X_all)\n",
    "        self.scaler_y = MinMaxScaler().fit(y_all)\n",
    "\n",
    "        # prepare a zero-mask vector\n",
    "        self.mask_value = torch.zeros(self.vec_len, dtype=torch.float32)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Yields:\n",
    "            seq_tensor      (seq_len, vec_len) : normalized & scaled sequence\n",
    "            masked_pos      (1,) tensor long   : index of masked time step\n",
    "            target_tensor   (vec_len,)          : normalized & scaled target\n",
    "        \"\"\"\n",
    "        T = len(self.scenes)\n",
    "        mask_prob  = 0.15\n",
    "        zero_prob  = mask_prob * 0.8\n",
    "        noise_prob = mask_prob * 0.1\n",
    "\n",
    "        for t in range(self.seq_len, T):\n",
    "            past       = self.scenes[t - self.seq_len : t]\n",
    "            scene_dict = self.scenes[t]\n",
    "\n",
    "            for u in range(self.U):\n",
    "                for s in range(self.S):\n",
    "                    # power-normalize each slice\n",
    "                    seq_np = np.stack([\n",
    "                        self._power_norm(ps[0]['user']['channel'][u,0,:,s])\n",
    "                        for ps in past\n",
    "                    ], axis=0)\n",
    "                    tgt_np = self._power_norm(\n",
    "                        scene_dict[0]['user']['channel'][u,0,:,s]\n",
    "                    )\n",
    "\n",
    "                    if not np.any(seq_np) or not np.any(tgt_np):\n",
    "                        continue\n",
    "\n",
    "                    # MinMax transform (using the same scalers)\n",
    "                    N, D = seq_np.shape\n",
    "                    seq_np = self.scaler_x.transform(seq_np.reshape(-1, D)).reshape(N, D)\n",
    "                    tgt_np = self.scaler_y.transform(tgt_np.reshape(1, -1)).reshape(-1,)\n",
    "\n",
    "                    seq_tensor   = torch.from_numpy(seq_np)\n",
    "                    target_tensor= torch.from_numpy(tgt_np)\n",
    "\n",
    "                    # select mask position\n",
    "                    mpos = random.randrange(self.seq_len)\n",
    "\n",
    "                    r = random.random()\n",
    "                    if r < zero_prob:\n",
    "                        masked_seq = seq_tensor.clone()\n",
    "                        masked_seq[mpos] = self.mask_value\n",
    "                        yield masked_seq, torch.tensor([mpos]), target_tensor\n",
    "\n",
    "                    elif r < zero_prob + noise_prob:\n",
    "                        masked_seq = seq_tensor.clone()\n",
    "                        masked_seq[mpos] = torch.randn(self.vec_len) * self.noise_std\n",
    "                        yield masked_seq, torch.tensor([mpos]), target_tensor\n",
    "\n",
    "                    elif r < mask_prob:\n",
    "                        # masked-but-unchanged\n",
    "                        yield seq_tensor, torch.tensor([mpos]), target_tensor\n",
    "\n",
    "                    else:\n",
    "                        # unmasked\n",
    "                        yield seq_tensor, torch.tensor([mpos]), target_tensor\n",
    "\n",
    "    def _power_norm(self, h: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert complex vector to real+imag concat and normalize power to 1.\n",
    "        \"\"\"\n",
    "        v = np.concatenate([h.real, h.imag]).astype(np.float32)\n",
    "        power = np.mean(v * v) + self.eps\n",
    "        return v / np.sqrt(power)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.scenes) - self.seq_len) * self.U * self.S\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e1373a-f803-49f3-b891-f9d6f59e5dc9",
   "metadata": {},
   "source": [
    "## Split Train/Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29548f28-3188-4562-bb96-634ed3bac496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❷ Train/Validation DataLoader split train : val = 6 : 4\n",
    "seq_len      = 5\n",
    "split_ratio  = 0.6\n",
    "split_idx    = int(len(dataset) * split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fd809a5-3720-4ffd-90ce-b2d5ccf541f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasked_train_ds = UnMaskedChannelSeqDataset(dataset[:split_idx], seq_len=seq_len)\n",
    "unmasked_val_ds   = UnMaskedChannelSeqDataset(dataset[split_idx:], seq_len=seq_len)\n",
    "\n",
    "# iterate over train_ds to compute min and max of features/targets\n",
    "\n",
    "batch_size   = 32\n",
    "unmasked_train_loader = DataLoader(unmasked_train_ds, batch_size=batch_size, shuffle=False)\n",
    "unmasked_val_loader   = DataLoader(unmasked_val_ds,   batch_size=batch_size, shuffle=False)\n",
    "# ─────────────────────────────────────────────\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "400fd339-0756-482e-8dc6-fbd9a806d60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq: min 0.33174002170562744 max 0.6701632738113403    tgt: min 0.329348623752594 max 0.666236400604248\n"
     ]
    }
   ],
   "source": [
    "# after you (re)create masked_train_loader\n",
    "batch = next(iter(unmasked_train_loader))\n",
    "seq,  tgt = batch   # seq: (B, L, D), tgt: (B, D)\n",
    "print(\n",
    "    \"seq: min\", seq.min().item(), \"max\", seq.max().item(),\n",
    "    \"   tgt: min\", tgt.min().item(), \"max\", tgt.max().item()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49c214a4-1b26-49a7-b308-e16a000f0ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❷ Train/Validation DataLoader split train : val = 6 : 4\n",
    "\n",
    "masked_train_ds = MaskedChannelSeqDataset(dataset[:split_idx], seq_len=seq_len)\n",
    "masked_val_ds   = MaskedChannelSeqDataset(dataset[split_idx:], seq_len=seq_len)\n",
    "\n",
    "# iterate over train_ds to compute min and max of features/targets\n",
    "\n",
    "batch_size   = 32\n",
    "masked_train_loader = DataLoader(masked_train_ds, batch_size=batch_size, shuffle=False)\n",
    "masked_val_loader   = DataLoader(masked_val_ds,   batch_size=batch_size, shuffle=False)\n",
    "# ─────────────────────────────────────────────\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2f2f795-7848-420a-84c9-4b53a3933deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq: 0.0 ~ 0.6701632738113403\n",
      "tgt: 0.329348623752594 ~ 0.666236400604248\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(masked_train_loader))\n",
    "seq, mpos, tgt = batch\n",
    "print(\"seq:\", seq.min().item(), \"~\", seq.max().item())\n",
    "print(\"tgt:\", tgt.min().item(), \"~\", tgt.max().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b29d65-387d-4326-b989-afe274279c38",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0383c46a-7e6c-4907-aaa4-e6b1902a770d",
   "metadata": {},
   "source": [
    "LWMWithHead: A wrapper class that uses a pre-trained LWM (Transformer encoder) as the backbone,\n",
    "             and attaches a new fully-connected (FC) head for downstream tasks\n",
    "             (regression, classification, etc.).\n",
    "\n",
    "Changes:\n",
    "- input_dim: Dimension of the actual input data (e.g., 64)\n",
    "- patch_length: Patch length expected by the backbone (e.g., 16)\n",
    "- Replaces the original element_length parameter with these two distinct parameters\n",
    "- Applies a projection layer (self.input_proj) in forward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80c33fca-38fa-4842-b886-69972e5d28a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from lwm_model import lwm\n",
    "\n",
    "class LWMWithHead(nn.Module):\n",
    "    \"\"\"\n",
    "    LWMWithHead: A wrapper class that uses a pre-trained LWM (Transformer encoder) as the backbone,\n",
    "                 and attaches a new fully-connected (FC) head for downstream tasks\n",
    "                 (regression, classification, etc.).\n",
    "\n",
    "    Changes:\n",
    "    - input_dim: Dimension of the actual input data (e.g., 64)\n",
    "    - patch_length: Patch length expected by the backbone (e.g., 16)\n",
    "    - Replaces the original element_length parameter with these two distinct parameters\n",
    "    - Applies a projection layer (self.input_proj) in forward()\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,                 # Dimension of the actual input data (e.g., 64)\n",
    "        patch_length: int,              # Patch length expected by the backbone (e.g., 16)\n",
    "        d_model: int = 64,              # LWM hidden size\n",
    "        max_len: int = 129,             # Positional encoding max length\n",
    "        n_layers: int = 12,             # Number of Transformer encoder layers\n",
    "        hidden_dim: int = 256,          # FC head hidden dimension\n",
    "        out_dim: int = 64,              # FC head output dimension\n",
    "        freeze_backbone: bool = True,   # Whether to freeze the backbone\n",
    "        checkpoint_path: str | None = \"./model_weights.pth\",\n",
    "        device: str = \"cuda\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # apply a projection layer to match backbone's expected patch_length\n",
    "        self.input_proj = nn.Linear(input_dim, patch_length)\n",
    "\n",
    "        # initialize backbone\n",
    "        if checkpoint_path is None:\n",
    "            # randomly initialized backbone\n",
    "            self.backbone = lwm(\n",
    "                element_length=patch_length,\n",
    "                d_model=d_model,\n",
    "                max_len=max_len,\n",
    "                n_layers=n_layers\n",
    "            ).to(device)\n",
    "        else:\n",
    "            # load pre-trained weights\n",
    "            self.backbone = lwm.from_pretrained(\n",
    "                ckpt_name=checkpoint_path,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "        # freeze backbone parameters if required\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # attach a new fully-connected head for downstream tasks\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, masked_pos: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: Tensor of shape (B, L, input_dim)\n",
    "            masked_pos: Tensor of shape (B, num_mask)\n",
    "        Returns:\n",
    "            out: Tensor of shape (B, out_dim)\n",
    "        \"\"\"\n",
    "        # project inputs to patch_length dimension\n",
    "        x = self.input_proj(input_ids)\n",
    "\n",
    "        # backbone forward: returns (logits_lm, enc_output)\n",
    "        _, enc_output = self.backbone(x, masked_pos)\n",
    "\n",
    "        # extract CLS token feature (first token)\n",
    "        feat = enc_output[:, 0, :]\n",
    "\n",
    "        # pass through FC head to get final output\n",
    "        out = self.head(feat)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2e8f0ee-6782-4eb2-b6c3-fe363ed8d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GRUWithHead(nn.Module):\n",
    "    \"\"\"\n",
    "    GRUWithHead (projected):\n",
    "      • Projects the raw feature dimension (input_dim) to a smaller patch_length\n",
    "        so every backbone receives the same patch-sized input (like LWM).\n",
    "      • Stacks N GRU layers, then an FC head for downstream tasks.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int    = 64,   # raw feature dimension coming from the DataLoader\n",
    "        patch_length: int = 16,   # target dimension fed to the GRU backbone\n",
    "        d_model: int      = 64,   # GRU hidden size\n",
    "        n_layers: int     = 12,   # number of stacked GRU layers\n",
    "        bidirectional: bool = True,\n",
    "        dropout: float      = 0.1,\n",
    "        hidden_dim: int     = 256, # FC-head hidden size\n",
    "        out_dim: int        = 64,  # FC-head output size\n",
    "        freeze_backbone: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 0) Project raw_dim → patch_length (64 → 16)\n",
    "        self.input_proj = nn.Linear(input_dim, patch_length)\n",
    "\n",
    "        # 1) GRU backbone that expects 'patch_length' features per time step\n",
    "        self.backbone = nn.GRU(\n",
    "            input_size     = patch_length,\n",
    "            hidden_size    = d_model,\n",
    "            num_layers     = n_layers,\n",
    "            batch_first    = True,\n",
    "            bidirectional  = bidirectional,\n",
    "            dropout        = dropout if n_layers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # 2) Fully-connected head\n",
    "        gru_out_dim = d_model * (2 if bidirectional else 1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(gru_out_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x : Tensor of shape (batch, seq_len, input_dim) – raw features\n",
    "        Returns:\n",
    "            Tensor of shape (batch, out_dim)\n",
    "        \"\"\"\n",
    "        # project raw features to patch_length\n",
    "        x_proj = self.input_proj(x)                 # (B, seq_len, patch_length)\n",
    "\n",
    "        # sequence modelling with GRU\n",
    "        out, _ = self.backbone(x_proj)              # (B, seq_len, num_dirs*d_model)\n",
    "\n",
    "        # use the last time-step representation\n",
    "        feat = out[:, -1, :]                        # (B, gru_out_dim)\n",
    "\n",
    "        # downstream head\n",
    "        return self.head(feat)                      # (B, out_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f9d84d1-33a1-4f34-9e97-51242fc2d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # Create positional encoding matrix of shape (1, max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            Tensor: x plus positional encodings\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, feat_dim: int, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # Optional linear projection from feat_dim to d_model\n",
    "        self.proj = nn.Linear(feat_dim, d_model) if feat_dim != d_model else None\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch, seq_len, feat_dim)\n",
    "        Returns:\n",
    "            Tensor of shape (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        if self.proj is not None:\n",
    "            x = self.proj(x)\n",
    "        return self.pos_enc(x)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dim_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        # Multi-Head Self-Attention\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        # Position-wise Feed-Forward Network\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_ff),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_ff, d_model)\n",
    "        )\n",
    "        # Layer Normalization and Dropout for residual connections\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        src_mask: torch.Tensor = None,\n",
    "        src_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (seq_len, batch, d_model)\n",
    "            src_mask: Optional Tensor of shape (seq_len, seq_len)\n",
    "            src_key_padding_mask: Optional Tensor of shape (batch, seq_len)\n",
    "        Returns:\n",
    "            Tensor of shape (seq_len, batch, d_model)\n",
    "        \"\"\"\n",
    "        # Self-attention sublayer\n",
    "        attn_out, _ = self.self_attn(x, x, x, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)\n",
    "        x = x + self.dropout1(attn_out)\n",
    "        x = self.norm1(x)\n",
    "        # Feed-forward sublayer\n",
    "        ff_out = self.ff(x)\n",
    "        x = x + self.dropout2(ff_out)\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderCustom(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feat_dim: int,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        dim_ff: int,\n",
    "        n_layers: int,\n",
    "        dropout: float = 0.1,\n",
    "        max_len: int = 5000\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Input embedding: feature projection + positional encoding\n",
    "        self.input_embedding = InputEmbedding(feat_dim, d_model, max_len)\n",
    "        # Stack of N encoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, n_heads, dim_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        src_mask: torch.Tensor = None,\n",
    "        src_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch, seq_len, feat_dim)\n",
    "        Returns:\n",
    "            Tensor of shape (seq_len, batch, d_model)\n",
    "        \"\"\"\n",
    "        x = self.input_embedding(x)       # (batch, seq_len, d_model)\n",
    "        x = x.transpose(0, 1)             # (seq_len, batch, d_model)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dim_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        # Masked Self-Attention\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        # Encoder-Decoder Attention\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        # Position-wise Feed-Forward Network\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_ff),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_ff, d_model)\n",
    "        )\n",
    "        # Layer Normalizations and Dropouts\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt: torch.Tensor,\n",
    "        memory: torch.Tensor,\n",
    "        tgt_mask: torch.Tensor = None,\n",
    "        memory_mask: torch.Tensor = None,\n",
    "        tgt_key_padding_mask: torch.Tensor = None,\n",
    "        memory_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tgt: Tensor of shape (tgt_len, batch, d_model)\n",
    "            memory: Tensor of shape (src_len, batch, d_model)\n",
    "        Returns:\n",
    "            Tensor of shape (tgt_len, batch, d_model)\n",
    "        \"\"\"\n",
    "        # Masked self-attention sublayer\n",
    "        attn1, _ = self.self_attn(\n",
    "            tgt, tgt, tgt,\n",
    "            attn_mask=tgt_mask,\n",
    "            key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "        tgt = tgt + self.dropout1(attn1)\n",
    "        tgt = self.norm1(tgt)\n",
    "        # Encoder-decoder attention sublayer\n",
    "        attn2, _ = self.multihead_attn(\n",
    "            tgt, memory, memory,\n",
    "            attn_mask=memory_mask,\n",
    "            key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "        tgt = tgt + self.dropout2(attn2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        # Feed-forward sublayer\n",
    "        ff_out = self.ff(tgt)\n",
    "        tgt = tgt + self.dropout3(ff_out)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "class TransformerDecoderCustom(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feat_dim: int,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        dim_ff: int,\n",
    "        n_layers: int,\n",
    "        dropout: float = 0.1,\n",
    "        max_len: int = 5000\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Input embedding for target sequence\n",
    "        self.input_embedding = InputEmbedding(feat_dim, d_model, max_len)\n",
    "        # Stack of N decoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, n_heads, dim_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        # Final projection back to feature dimension\n",
    "        self.output_linear = nn.Linear(d_model, feat_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt: torch.Tensor,\n",
    "        memory: torch.Tensor,\n",
    "        tgt_mask: torch.Tensor = None,\n",
    "        memory_mask: torch.Tensor = None,\n",
    "        tgt_key_padding_mask: torch.Tensor = None,\n",
    "        memory_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tgt: Tensor of shape (batch, tgt_len, feat_dim)\n",
    "            memory: Tensor of shape (src_len, batch, d_model)\n",
    "        Returns:\n",
    "            Tensor of shape (batch, tgt_len, feat_dim)\n",
    "        \"\"\"\n",
    "        x = self.input_embedding(tgt)       # (batch, tgt_len, d_model)\n",
    "        x = x.transpose(0, 1)               # (tgt_len, batch, d_model)\n",
    "        for layer in self.layers:\n",
    "            x = layer(\n",
    "                x,\n",
    "                memory,\n",
    "                tgt_mask=tgt_mask,\n",
    "                memory_mask=memory_mask,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                memory_key_padding_mask=memory_key_padding_mask\n",
    "            )\n",
    "        x = x.transpose(0, 1)               # (batch, tgt_len, d_model)\n",
    "        return self.output_linear(x)        # project back to feat_dim\n",
    "\n",
    "        \n",
    "class TransformerWithHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int    = 64,   # raw feature dimension\n",
    "        patch_length: int = 16,   # what the encoder consumes\n",
    "        d_model: int      = 64,   # hidden size inside transformer\n",
    "        n_heads: int      = 4,\n",
    "        dim_ff: int       = 256,\n",
    "        n_layers: int     = 12,\n",
    "        dropout: float    = 0.1,\n",
    "        hidden_dim: int   = 256,\n",
    "        out_dim: int      = 64,\n",
    "        max_len: int      = 5000,\n",
    "        freeze_backbone: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 0) project raw 64-dim → 16-dim patch\n",
    "        self.input_proj = nn.Linear(input_dim, patch_length)\n",
    "\n",
    "        # 1) encoder expects patch_length features\n",
    "        self.encoder = TransformerEncoderCustom(\n",
    "            feat_dim = patch_length,\n",
    "            d_model  = d_model,\n",
    "            n_heads  = n_heads,\n",
    "            dim_ff   = dim_ff,\n",
    "            n_layers = n_layers,\n",
    "            dropout  = dropout,\n",
    "            max_len  = max_len,\n",
    "        )\n",
    "        if freeze_backbone:\n",
    "            for p in self.encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # 2) task head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,                 # (batch, seq_len, 64)\n",
    "        src_mask: torch.Tensor = None,\n",
    "        src_key_padding_mask: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        # raw → patch_length\n",
    "        x = self.input_proj(x)           # (batch, seq_len, 16)\n",
    "\n",
    "        # encode (InputEmbedding inside encoder adds 16→64 + positional encoding)\n",
    "        enc_out = self.encoder(\n",
    "            x, src_mask=src_mask, src_key_padding_mask=src_key_padding_mask\n",
    "        )                                # (seq_len, batch, d_model)\n",
    "\n",
    "        last_token = enc_out[-1]         # (batch, d_model)\n",
    "        return self.head(last_token)     # (batch, out_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2503278-03dd-4b6d-8497-e9f9d83e10fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNNWithHead(nn.Module):\n",
    "    \"\"\"\n",
    "    RNNWithHead (projected):\n",
    "      • Projects raw feature vectors from `input_dim` to `patch_length`\n",
    "      • Feeds the projected sequence to an RNN backbone\n",
    "      • Maps the last hidden state through an FC head\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int    = 64,   # raw feature dimension coming from DataLoader\n",
    "        patch_length: int = 16,   # dimension consumed by the RNN backbone\n",
    "        hidden_size: int  = 64,   # RNN hidden size\n",
    "        num_layers: int   = 12,   # number of stacked RNN layers\n",
    "        bidirectional: bool = True,\n",
    "        dropout: float      = 0.1,\n",
    "        hidden_dim: int     = 256, # FC-head hidden size\n",
    "        out_dim: int        = 64,  # FC-head output size\n",
    "        freeze_backbone: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 0) project raw 64-dim → 16-dim\n",
    "        self.input_proj = nn.Linear(input_dim, patch_length)\n",
    "\n",
    "        # 1) RNN backbone\n",
    "        self.backbone = nn.RNN(\n",
    "            input_size     = patch_length,\n",
    "            hidden_size    = hidden_size,\n",
    "            num_layers     = num_layers,\n",
    "            batch_first    = True,\n",
    "            bidirectional  = bidirectional,\n",
    "            dropout        = dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # 2) FC head\n",
    "        rnn_out_dim = hidden_size * (2 if bidirectional else 1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(rnn_out_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, input_dim=64)\n",
    "        returns: (batch, out_dim)\n",
    "        \"\"\"\n",
    "        x_proj = self.input_proj(x)           # (batch, seq_len, 16)\n",
    "        out, _ = self.backbone(x_proj)        # (batch, seq_len, rnn_out_dim)\n",
    "        feat   = out[:, -1, :]                # take last time step\n",
    "        return self.head(feat)                # (batch, out_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b210558c-7f51-4e6b-ad3a-8efa5237a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMWithHead(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTMWithHead (projected):\n",
    "      • Projects raw feature vectors from `input_dim` to a compact `patch_length`\n",
    "      • Feeds the projected sequence to an LSTM backbone\n",
    "      • Uses the last hidden state to drive an FC head for the downstream task\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int    = 64,   # raw feature dimension (e.g., 64)\n",
    "        patch_length: int = 16,   # dimension consumed by the LSTM backbone\n",
    "        hidden_size: int  = 64,   # LSTM hidden size\n",
    "        num_layers: int   = 12,   # number of stacked LSTM layers\n",
    "        bidirectional: bool = True,\n",
    "        dropout: float      = 0.1,\n",
    "        hidden_dim: int     = 256, # FC-head hidden size\n",
    "        out_dim: int        = 64,  # FC-head output size\n",
    "        freeze_backbone: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 0) Raw 64-dim → 16-dim patch projection\n",
    "        self.input_proj = nn.Linear(input_dim, patch_length)\n",
    "\n",
    "        # 1) LSTM backbone that expects `patch_length` features\n",
    "        self.backbone = nn.LSTM(\n",
    "            input_size     = patch_length,\n",
    "            hidden_size    = hidden_size,\n",
    "            num_layers     = num_layers,\n",
    "            batch_first    = True,\n",
    "            bidirectional  = bidirectional,\n",
    "            dropout        = dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # 2) FC head\n",
    "        lstm_out_dim = hidden_size * (2 if bidirectional else 1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(lstm_out_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, input_dim=64)\n",
    "        returns: (batch, out_dim)\n",
    "        \"\"\"\n",
    "        # project raw features to patch_length\n",
    "        x_proj = self.input_proj(x)             # (B, seq_len, 16)\n",
    "\n",
    "        # sequence modeling with LSTM\n",
    "        out, _ = self.backbone(x_proj)          # (B, seq_len, lstm_out_dim)\n",
    "\n",
    "        # take the last time-step representation\n",
    "        feat = out[:, -1, :]                    # (B, lstm_out_dim)\n",
    "\n",
    "        # downstream head\n",
    "        return self.head(feat)                  # (B, out_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4766f28-d102-4ed0-8272-7d5098ad9f15",
   "metadata": {},
   "source": [
    "## fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e104da9-d8e8-42ea-a810-3567b9ae412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "# ──────────────────────────\n",
    "# Shared hyper-parameters\n",
    "# ──────────────────────────\n",
    "INPUT_DIM     = 64     # raw feature dimension\n",
    "PATCH_LENGTH  = 16     # dimension fed to every backbone\n",
    "D_MODEL       = 64     # internal hidden size (GRU/LSTM/Transformer)\n",
    "N_LAYERS      = 12     # stacked layers\n",
    "HIDDEN_DIM    = 256    # head hidden dimension\n",
    "OUT_DIM       = 64     # head output dimension\n",
    "DROPOUT       = 0.1    # dropout for recurrent / transformer blocks\n",
    "BIDIRECTIONAL = True   # use bidirectional RNNs\n",
    "DEVICE        = \"cuda\"\n",
    "\n",
    "# ──────────────────────────\n",
    "# Model class catalog\n",
    "# ──────────────────────────\n",
    "MODEL_CATALOG = {\n",
    "    \"LWM_freeze_backbone\"     : LWMWithHead,\n",
    "    \"LWM_pretrained_Fine_tune\": LWMWithHead,\n",
    "    \"LWM_Fine_tune\"           : LWMWithHead,\n",
    "    \"gru\"                     : GRUWithHead,\n",
    "    \"RNN\"                     : RNNWithHead,\n",
    "    \"LSTM\"                    : LSTMWithHead,\n",
    "    \"Transformer\"             : TransformerWithHead,\n",
    "}\n",
    "\n",
    "# ──────────────────────────\n",
    "# Per-model constructor kwargs\n",
    "# ──────────────────────────\n",
    "MODEL_PARAMS = {\n",
    "    # ── LWM variants ─────────────────────────────\n",
    "    \"LWM_freeze_backbone\": {\n",
    "        \"input_dim\"       : INPUT_DIM,\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"d_model\"         : D_MODEL,\n",
    "        \"max_len\"         : PATCH_LENGTH + 1,\n",
    "        \"n_layers\"        : N_LAYERS,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"freeze_backbone\" : True,\n",
    "        \"checkpoint_path\" : \"./model_weights.pth\",\n",
    "        \"device\"          : DEVICE,\n",
    "    },\n",
    "    \"LWM_pretrained_Fine_tune\": {\n",
    "        \"input_dim\"       : INPUT_DIM,\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"d_model\"         : D_MODEL,\n",
    "        \"max_len\"         : PATCH_LENGTH + 1,\n",
    "        \"n_layers\"        : N_LAYERS,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"freeze_backbone\" : False,\n",
    "        \"checkpoint_path\" : \"./model_weights.pth\",\n",
    "        \"device\"          : DEVICE,\n",
    "    },\n",
    "    \"LWM_Fine_tune\": {\n",
    "        \"input_dim\"       : INPUT_DIM,\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"d_model\"         : D_MODEL,\n",
    "        \"max_len\"         : PATCH_LENGTH + 1,\n",
    "        \"n_layers\"        : N_LAYERS,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"freeze_backbone\" : False,\n",
    "        \"checkpoint_path\" : None,\n",
    "        \"device\"          : DEVICE,\n",
    "    },\n",
    "\n",
    "    # ── GRU (projected) ──────────────────────────\n",
    "    \"gru\": {\n",
    "        \"input_dim\"       : INPUT_DIM,     # 64 → project → 16\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"d_model\"         : D_MODEL,\n",
    "        \"n_layers\"        : N_LAYERS,\n",
    "        \"bidirectional\"   : BIDIRECTIONAL,\n",
    "        \"dropout\"         : DROPOUT,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"freeze_backbone\" : False,\n",
    "    },\n",
    "\n",
    "    # ── Vanilla RNN (projected) ──────────────────\n",
    "    \"RNN\": {\n",
    "        \"input_dim\"       : INPUT_DIM,\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"hidden_size\"     : D_MODEL,\n",
    "        \"num_layers\"      : N_LAYERS,\n",
    "        \"bidirectional\"   : BIDIRECTIONAL,\n",
    "        \"dropout\"         : 0.0,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"freeze_backbone\" : False,\n",
    "    },\n",
    "\n",
    "    # ── LSTM (projected) ─────────────────────────\n",
    "    \"LSTM\": {\n",
    "        \"input_dim\"       : INPUT_DIM,\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"hidden_size\"     : D_MODEL,\n",
    "        \"num_layers\"      : N_LAYERS,\n",
    "        \"bidirectional\"   : BIDIRECTIONAL,\n",
    "        \"dropout\"         : DROPOUT,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"freeze_backbone\" : False,\n",
    "    },\n",
    "\n",
    "    # ── Transformer (projected) ──────────────────\n",
    "    \"Transformer\": {\n",
    "        \"input_dim\"       : INPUT_DIM,\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"d_model\"         : D_MODEL,\n",
    "        \"n_heads\"         : 4,\n",
    "        \"dim_ff\"          : 256,\n",
    "        \"n_layers\"        : N_LAYERS,\n",
    "        \"dropout\"         : DROPOUT,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"max_len\"         : PATCH_LENGTH + 1,\n",
    "        \"freeze_backbone\" : False,\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c37de2-4bc1-42aa-9a9e-cfa43bf0c3fd",
   "metadata": {},
   "source": [
    "## model evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e9e706d-4875-4ee2-93d9-f792571fe2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def rmse(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Root-Mean-Squared Error\n",
    "    \"\"\"\n",
    "    return torch.sqrt(F.mse_loss(pred, target, reduction=\"mean\"))   # √MSE\n",
    "\n",
    "def nmse(pred: torch.Tensor, target: torch.Tensor, eps : float = 1e-12) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Normalized MSE  =  E[‖ŷ − y‖²] / E[‖y‖²]\n",
    "    \"\"\"\n",
    "    # (B, …) → (B,)  \n",
    "    mse_per_sample   = ((pred - target)**2).view(pred.size(0), -1).sum(dim=1)\n",
    "    power_per_sample = (target**2).view(target.size(0), -1).sum(dim=1) + eps\n",
    "    return (mse_per_sample / power_per_sample).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3930b75e-0895-4fb1-8039-744c86730b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_evaluate(model, loader, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Validation loop for IterableDataset.\n",
    "    Returns average RMSE and NMSE over all samples.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_rmse, total_nmse, total_samples = 0.0, 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, masked_pos, target in loader:\n",
    "            # Move to device\n",
    "            input_ids, masked_pos, target = (\n",
    "                input_ids.to(device),\n",
    "                masked_pos.to(device),\n",
    "                target.to(device),\n",
    "            )\n",
    "            # Batch size\n",
    "            bs = input_ids.size(0)\n",
    "\n",
    "            # Forward\n",
    "            pred = model(input_ids, masked_pos)\n",
    "\n",
    "            # Accumulate batch metrics\n",
    "            total_rmse    += rmse(pred, target).item() * bs\n",
    "            total_nmse    += nmse(pred, target).item() * bs\n",
    "            total_samples += bs\n",
    "\n",
    "    # Compute averages\n",
    "    return {\n",
    "        \"RMSE\": total_rmse / total_samples,\n",
    "        \"NMSE\": total_nmse / total_samples\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92d4ceda-b090-4b01-91a4-698c319420f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unmasked_evaluate(model, loader, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Validation loop for IterableDataset.\n",
    "    Returns average RMSE and NMSE over all samples.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_rmse, total_nmse, total_samples = 0.0, 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, target in loader:\n",
    "            # Move to device\n",
    "            input_ids,target = (\n",
    "                input_ids.to(device),\n",
    "                target.to(device),\n",
    "            )\n",
    "            # Batch size\n",
    "            bs = input_ids.size(0)\n",
    "\n",
    "            # Forward\n",
    "            pred = model(input_ids)\n",
    "\n",
    "            # Accumulate batch metrics\n",
    "            total_rmse    += rmse(pred, target).item() * bs\n",
    "            total_nmse    += nmse(pred, target).item() * bs\n",
    "            total_samples += bs\n",
    "\n",
    "    # Compute averages\n",
    "    return {\n",
    "        \"RMSE\": total_rmse / total_samples,\n",
    "        \"NMSE\": total_nmse / total_samples\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c0dcd-d4af-4d7b-87cd-154d81d210fb",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "317fde92-dd4d-4092-b93c-7b1b83674c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training LWM_freeze_backbone ===\n",
      "Model loaded successfully from ./model_weights.pth to cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/10] TrainLoss: 0.0157  Val RMSE: 0.0937  Val NMSE: 4.6457e-02  Val NMSE_dB: -13.3 dB  TrainTime: 206.49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/10] TrainLoss: 0.0100  Val RMSE: 0.0926  Val NMSE: 4.2636e-02  Val NMSE_dB: -13.7 dB  TrainTime: 178.40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/10] TrainLoss: 0.0086  Val RMSE: 0.0907  Val NMSE: 3.8790e-02  Val NMSE_dB: -14.1 dB  TrainTime: 179.94s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04/10] TrainLoss: 0.0076  Val RMSE: 0.0888  Val NMSE: 3.6792e-02  Val NMSE_dB: -14.3 dB  TrainTime: 202.28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/10] TrainLoss: 0.0072  Val RMSE: 0.0875  Val NMSE: 3.5754e-02  Val NMSE_dB: -14.5 dB  TrainTime: 203.23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/10] TrainLoss: 0.0070  Val RMSE: 0.0870  Val NMSE: 3.5194e-02  Val NMSE_dB: -14.5 dB  TrainTime: 208.71s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/10] TrainLoss: 0.0068  Val RMSE: 0.0858  Val NMSE: 3.4261e-02  Val NMSE_dB: -14.7 dB  TrainTime: 211.54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/10] TrainLoss: 0.0066  Val RMSE: 0.0849  Val NMSE: 3.3518e-02  Val NMSE_dB: -14.7 dB  TrainTime: 205.50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/10] TrainLoss: 0.0064  Val RMSE: 0.0841  Val NMSE: 3.2760e-02  Val NMSE_dB: -14.8 dB  TrainTime: 199.86s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/10] TrainLoss: 0.0062  Val RMSE: 0.0828  Val NMSE: 3.1742e-02  Val NMSE_dB: -15.0 dB  TrainTime: 219.54s\n",
      "🕒 LWM_freeze_backbone - average train time / epoch: 201.55s\n",
      "\n",
      "=== Training LWM_pretrained_Fine_tune ===\n",
      "Model loaded successfully from ./model_weights.pth to cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/10] TrainLoss: 0.0115  Val RMSE: 0.0855  Val NMSE: 3.6221e-02  Val NMSE_dB: -14.4 dB  TrainTime: 276.97s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/10] TrainLoss: 0.0069  Val RMSE: 0.0834  Val NMSE: 3.2972e-02  Val NMSE_dB: -14.8 dB  TrainTime: 245.39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/10] TrainLoss: 0.0061  Val RMSE: 0.0825  Val NMSE: 3.1170e-02  Val NMSE_dB: -15.1 dB  TrainTime: 242.71s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04/10] TrainLoss: 0.0050  Val RMSE: 0.0863  Val NMSE: 3.1857e-02  Val NMSE_dB: -15.0 dB  TrainTime: 255.61s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/10] TrainLoss: 0.0042  Val RMSE: 0.0819  Val NMSE: 2.9014e-02  Val NMSE_dB: -15.4 dB  TrainTime: 250.56s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/10] TrainLoss: 0.0038  Val RMSE: 0.0789  Val NMSE: 2.6393e-02  Val NMSE_dB: -15.8 dB  TrainTime: 260.04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/10] TrainLoss: 0.0033  Val RMSE: 0.0757  Val NMSE: 2.4076e-02  Val NMSE_dB: -16.2 dB  TrainTime: 271.34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/10] TrainLoss: 0.0030  Val RMSE: 0.0755  Val NMSE: 2.3930e-02  Val NMSE_dB: -16.2 dB  TrainTime: 267.70s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/10] TrainLoss: 0.0028  Val RMSE: 0.0768  Val NMSE: 2.4643e-02  Val NMSE_dB: -16.1 dB  TrainTime: 283.97s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/10] TrainLoss: 0.0027  Val RMSE: 0.0765  Val NMSE: 2.4444e-02  Val NMSE_dB: -16.1 dB  TrainTime: 263.95s\n",
      "🕒 LWM_pretrained_Fine_tune - average train time / epoch: 261.82s\n",
      "\n",
      "=== Training LWM_Fine_tune ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/10] TrainLoss: 0.0103  Val RMSE: 0.0879  Val NMSE: 3.4356e-02  Val NMSE_dB: -14.6 dB  TrainTime: 256.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/10] TrainLoss: 0.0052  Val RMSE: 0.0824  Val NMSE: 2.9784e-02  Val NMSE_dB: -15.3 dB  TrainTime: 278.70s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/10] TrainLoss: 0.0041  Val RMSE: 0.0797  Val NMSE: 2.7388e-02  Val NMSE_dB: -15.6 dB  TrainTime: 269.49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04/10] TrainLoss: 0.0035  Val RMSE: 0.0792  Val NMSE: 2.6363e-02  Val NMSE_dB: -15.8 dB  TrainTime: 263.49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/10] TrainLoss: 0.0032  Val RMSE: 0.0790  Val NMSE: 2.6550e-02  Val NMSE_dB: -15.8 dB  TrainTime: 264.37s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/10] TrainLoss: 0.0030  Val RMSE: 0.0759  Val NMSE: 2.4467e-02  Val NMSE_dB: -16.1 dB  TrainTime: 290.80s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/10] TrainLoss: 0.0028  Val RMSE: 0.0767  Val NMSE: 2.5073e-02  Val NMSE_dB: -16.0 dB  TrainTime: 283.41s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/10] TrainLoss: 0.0027  Val RMSE: 0.0764  Val NMSE: 2.4780e-02  Val NMSE_dB: -16.1 dB  TrainTime: 252.23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/10] TrainLoss: 0.0026  Val RMSE: 0.0760  Val NMSE: 2.4948e-02  Val NMSE_dB: -16.0 dB  TrainTime: 258.61s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/10] TrainLoss: 0.0025  Val RMSE: 0.0768  Val NMSE: 2.5728e-02  Val NMSE_dB: -15.9 dB  TrainTime: 282.60s\n",
      "🕒 LWM_Fine_tune - average train time / epoch: 269.97s\n",
      "\n",
      "=== Training gru ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/10] TrainLoss: 0.0132  Val RMSE: 0.0957  Val NMSE: 4.5185e-02  Val NMSE_dB: -13.5 dB  TrainTime: 104.55s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/10] TrainLoss: 0.0095  Val RMSE: 0.0971  Val NMSE: 4.2676e-02  Val NMSE_dB: -13.7 dB  TrainTime: 108.50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/10] TrainLoss: 0.0078  Val RMSE: 0.0903  Val NMSE: 3.7457e-02  Val NMSE_dB: -14.3 dB  TrainTime: 106.34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04/10] TrainLoss: 0.0070  Val RMSE: 0.0904  Val NMSE: 3.6410e-02  Val NMSE_dB: -14.4 dB  TrainTime: 98.97s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/10] TrainLoss: 0.0060  Val RMSE: 0.0846  Val NMSE: 3.1739e-02  Val NMSE_dB: -15.0 dB  TrainTime: 98.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/10] TrainLoss: 0.0047  Val RMSE: 0.0818  Val NMSE: 2.8369e-02  Val NMSE_dB: -15.5 dB  TrainTime: 98.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/10] TrainLoss: 0.0044  Val RMSE: 0.0827  Val NMSE: 2.8442e-02  Val NMSE_dB: -15.5 dB  TrainTime: 102.88s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/10] TrainLoss: 0.0042  Val RMSE: 0.0829  Val NMSE: 2.8391e-02  Val NMSE_dB: -15.5 dB  TrainTime: 98.36s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/10] TrainLoss: 0.0041  Val RMSE: 0.0809  Val NMSE: 2.7217e-02  Val NMSE_dB: -15.7 dB  TrainTime: 97.93s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/10] TrainLoss: 0.0040  Val RMSE: 0.0808  Val NMSE: 2.7079e-02  Val NMSE_dB: -15.7 dB  TrainTime: 98.96s\n",
      "🕒 gru - average train time / epoch: 101.25s\n",
      "\n",
      "=== Training RNN ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/10] TrainLoss: 0.0105  Val RMSE: 0.0879  Val NMSE: 3.6127e-02  Val NMSE_dB: -14.4 dB  TrainTime: 74.73s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/10] TrainLoss: 0.0057  Val RMSE: 0.0817  Val NMSE: 2.9307e-02  Val NMSE_dB: -15.3 dB  TrainTime: 75.23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/10] TrainLoss: 0.0042  Val RMSE: 0.0745  Val NMSE: 2.4948e-02  Val NMSE_dB: -16.0 dB  TrainTime: 75.87s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04/10] TrainLoss: 0.0036  Val RMSE: 0.0707  Val NMSE: 2.2895e-02  Val NMSE_dB: -16.4 dB  TrainTime: 74.30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/10] TrainLoss: 0.0033  Val RMSE: 0.0686  Val NMSE: 2.1485e-02  Val NMSE_dB: -16.7 dB  TrainTime: 84.11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/10] TrainLoss: 0.0030  Val RMSE: 0.0681  Val NMSE: 2.1101e-02  Val NMSE_dB: -16.8 dB  TrainTime: 78.63s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/10] TrainLoss: 0.0028  Val RMSE: 0.0673  Val NMSE: 2.0503e-02  Val NMSE_dB: -16.9 dB  TrainTime: 73.38s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/10] TrainLoss: 0.0026  Val RMSE: 0.0671  Val NMSE: 2.0288e-02  Val NMSE_dB: -16.9 dB  TrainTime: 70.05s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/10] TrainLoss: 0.0025  Val RMSE: 0.0675  Val NMSE: 2.0327e-02  Val NMSE_dB: -16.9 dB  TrainTime: 73.61s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/10] TrainLoss: 0.0025  Val RMSE: 0.0676  Val NMSE: 2.0284e-02  Val NMSE_dB: -16.9 dB  TrainTime: 68.00s\n",
      "🕒 RNN - average train time / epoch: 74.79s\n",
      "\n",
      "=== Training LSTM ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/10] TrainLoss: 0.0141  Val RMSE: 0.0988  Val NMSE: 4.6593e-02  Val NMSE_dB: -13.3 dB  TrainTime: 95.89s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/10] TrainLoss: 0.0098  Val RMSE: 0.1003  Val NMSE: 4.7084e-02  Val NMSE_dB: -13.3 dB  TrainTime: 101.55s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/10] TrainLoss: 0.0094  Val RMSE: 0.0925  Val NMSE: 4.2195e-02  Val NMSE_dB: -13.7 dB  TrainTime: 94.13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04/10] TrainLoss: 0.0090  Val RMSE: 0.0907  Val NMSE: 4.1144e-02  Val NMSE_dB: -13.9 dB  TrainTime: 97.70s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/10] TrainLoss: 0.0088  Val RMSE: 0.0904  Val NMSE: 4.0404e-02  Val NMSE_dB: -13.9 dB  TrainTime: 97.71s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/10] TrainLoss: 0.0086  Val RMSE: 0.0910  Val NMSE: 4.0542e-02  Val NMSE_dB: -13.9 dB  TrainTime: 101.97s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/10] TrainLoss: 0.0085  Val RMSE: 0.0974  Val NMSE: 4.4175e-02  Val NMSE_dB: -13.5 dB  TrainTime: 93.94s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/10] TrainLoss: 0.0080  Val RMSE: 0.0958  Val NMSE: 4.3562e-02  Val NMSE_dB: -13.6 dB  TrainTime: 104.29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/10] TrainLoss: 0.0078  Val RMSE: 0.0884  Val NMSE: 3.8428e-02  Val NMSE_dB: -14.2 dB  TrainTime: 94.67s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/10] TrainLoss: 0.0076  Val RMSE: 0.0877  Val NMSE: 3.7055e-02  Val NMSE_dB: -14.3 dB  TrainTime: 94.77s\n",
      "🕒 LSTM - average train time / epoch: 97.66s\n",
      "\n",
      "=== Training Transformer ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/10] TrainLoss: 0.0102  Val RMSE: 0.0822  Val NMSE: 2.9728e-02  Val NMSE_dB: -15.3 dB  TrainTime: 190.83s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/10] TrainLoss: 0.0044  Val RMSE: 0.0771  Val NMSE: 2.5726e-02  Val NMSE_dB: -15.9 dB  TrainTime: 172.56s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/10] TrainLoss: 0.0035  Val RMSE: 0.0740  Val NMSE: 2.3903e-02  Val NMSE_dB: -16.2 dB  TrainTime: 177.54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04/10] TrainLoss: 0.0031  Val RMSE: 0.0738  Val NMSE: 2.4129e-02  Val NMSE_dB: -16.2 dB  TrainTime: 184.68s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/10] TrainLoss: 0.0029  Val RMSE: 0.0733  Val NMSE: 2.3701e-02  Val NMSE_dB: -16.3 dB  TrainTime: 189.74s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/10] TrainLoss: 0.0028  Val RMSE: 0.0735  Val NMSE: 2.3474e-02  Val NMSE_dB: -16.3 dB  TrainTime: 178.84s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/10] TrainLoss: 0.0026  Val RMSE: 0.0714  Val NMSE: 2.2195e-02  Val NMSE_dB: -16.5 dB  TrainTime: 187.54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/10] TrainLoss: 0.0025  Val RMSE: 0.0704  Val NMSE: 2.1493e-02  Val NMSE_dB: -16.7 dB  TrainTime: 175.68s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/10] TrainLoss: 0.0024  Val RMSE: 0.0707  Val NMSE: 2.1681e-02  Val NMSE_dB: -16.6 dB  TrainTime: 171.46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/10] TrainLoss: 0.0024  Val RMSE: 0.0704  Val NMSE: 2.1441e-02  Val NMSE_dB: -16.7 dB  TrainTime: 180.34s\n",
      "🕒 Transformer - average train time / epoch: 180.92s\n",
      "\n",
      "=== Summary of NMSE(dB) by model ===\n",
      "LWM_freeze_backbone      : -14.983714818954468\n",
      "LWM_pretrained_Fine_tune : -16.11828923225403\n",
      "LWM_Fine_tune            : -15.895888805389404\n",
      "gru                      : -15.673699378967285\n",
      "RNN                      : -16.92837953567505\n",
      "LSTM                     : -14.311479330062866\n",
      "Transformer              : -16.68756127357483\n",
      "\n",
      "Total training time for all models: 13304.01s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Unified training / validation script\n",
    "------------------------------------\n",
    "* Handles multiple models listed in MODEL_CATALOG\n",
    "* Uses separate masked / un-masked DataLoaders depending on model type\n",
    "* Measures epoch-level train time and prints per-epoch + average timings\n",
    "\"\"\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time, math, torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 0) Globals and hyper-parameters\n",
    "# ─────────────────────────────────────────────\n",
    "device            = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion         = nn.MSELoss().to(device)\n",
    "\n",
    "NUM_EPOCHS        = 10\n",
    "LR                = 1e-4                      # learning-rate\n",
    "total_start_time  = time.time()               # wall-clock timer for all models\n",
    "results           = {}                        # holds best-epoch NMSE_dB for each model\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 1) Model training / validation loop\n",
    "# ─────────────────────────────────────────────\n",
    "for model_name, ModelCls in MODEL_CATALOG.items():\n",
    "\n",
    "    print(f\"\\n=== Training {model_name} ===\")\n",
    "\n",
    "    # 1-A) instantiate model\n",
    "    model_args = MODEL_PARAMS[model_name]\n",
    "    model      = ModelCls(**model_args).to(device)\n",
    "\n",
    "    # 1-B) collect trainable parameters\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    if len(trainable_params) == 0:\n",
    "        print(f\"⚠️  '{model_name}' has no trainable parameters — skipping training.\")\n",
    "        results[model_name] = float(\"nan\")\n",
    "        continue\n",
    "\n",
    "    optimizer   = torch.optim.Adam(trainable_params, lr=LR)\n",
    "    epoch_times = []                       # store per-epoch train times\n",
    "\n",
    "    # choose loaders depending on model family\n",
    "    uses_mask   = model_name.startswith(\"LWM_\")\n",
    "    tr_loader   = masked_train_loader   if uses_mask else unmasked_train_loader\n",
    "    val_loader  = masked_val_loader    if uses_mask else unmasked_val_loader\n",
    "    eval_fn     = masked_evaluate      if uses_mask else unmasked_evaluate\n",
    "\n",
    "    # 1-C) epoch loop\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "\n",
    "        # ── TRAIN ───────────────────────────────────────\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(tr_loader,\n",
    "                    desc=f\"[{model_name} {epoch:02d}/{NUM_EPOCHS}] train\",\n",
    "                    leave=False)\n",
    "\n",
    "        for b, batch in enumerate(pbar, 1):\n",
    "            # unpack batch depending on masked / unmasked loader\n",
    "            if uses_mask:\n",
    "                inp, mpos, tgt = [x.to(device) for x in batch]\n",
    "                pred = model(inp, mpos).squeeze(-1)\n",
    "            else:\n",
    "                inp, tgt      = [x.to(device) for x in batch]\n",
    "                pred          = model(inp)\n",
    "\n",
    "            loss = criterion(pred, tgt)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if b % 100 == 0:\n",
    "                pbar.set_postfix(train_loss=running_loss / b)\n",
    "\n",
    "        epoch_train_time = time.time() - t0\n",
    "        epoch_times.append(epoch_train_time)\n",
    "        avg_train_loss = running_loss / b\n",
    "\n",
    "        # ── VALID ───────────────────────────────────────\n",
    "        metrics      = eval_fn(model, val_loader, device)\n",
    "        val_rmse     = metrics[\"RMSE\"]\n",
    "        val_nmse     = metrics[\"NMSE\"]\n",
    "        val_nmse_db  = 10 * torch.log10(torch.tensor(val_nmse)).item()\n",
    "\n",
    "        print(\n",
    "            f\"[{epoch:02d}/{NUM_EPOCHS}] \"\n",
    "            f\"TrainLoss: {avg_train_loss:.4f}  \"\n",
    "            f\"Val RMSE: {val_rmse:.4f}  \"\n",
    "            f\"Val NMSE: {val_nmse:.4e}  \"\n",
    "            f\"Val NMSE_dB: {val_nmse_db:.1f} dB  \"\n",
    "            f\"TrainTime: {epoch_train_time:.2f}s\"\n",
    "        )\n",
    "\n",
    "    # 1-D) epoch-time statistics and final log\n",
    "    avg_epoch_time = sum(epoch_times) / len(epoch_times)\n",
    "    print(f\"🕒 {model_name} - average train time / epoch: {avg_epoch_time:.2f}s\")\n",
    "    results[model_name] = val_nmse_db\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 2) summary\n",
    "# ─────────────────────────────────────────────\n",
    "print(\"\\n=== Summary of NMSE(dB) by model ===\")\n",
    "for name, nmse_db in results.items():\n",
    "    print(f\"{name:25s}: {nmse_db if not math.isnan(nmse_db) else 'skipped':>6}\")\n",
    "\n",
    "print(f\"\\nTotal training time for all models: {time.time() - total_start_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac929b23-8f2c-4b46-b746-d450e45bbe3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5ad5758-e0d4-4a80-a591-4b82af28765d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq: min 0.0 max 0.6701632738113403    tgt: min 0.329348623752594 max 0.666236400604248\n"
     ]
    }
   ],
   "source": [
    "# after you (re)create masked_train_loader\n",
    "batch = next(iter(masked_train_loader))\n",
    "seq, mpos, tgt = batch   # seq: (B, L, D), tgt: (B, D)\n",
    "print(\n",
    "    \"seq: min\", seq.min().item(), \"max\", seq.max().item(),\n",
    "    \"   tgt: min\", tgt.min().item(), \"max\", tgt.max().item()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a90966-4bb9-40c9-a8a6-6d2435ceec06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff42f6d-3ed7-43b4-b35e-16e0d7345bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f748d3-0edc-4e77-bbf6-be96d7d8d99e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a224fa-03a3-4c41-989a-47e2327c5c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bea2d4-1027-495f-a2e3-efbc703aff5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dd53c6-5322-4744-95d6-90cbe8da1d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9f299d-07db-4e49-aac5-ebddf12c7538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
