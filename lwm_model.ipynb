{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ELEMENT_LENGTH = 16\n",
    "D_MODEL = 64\n",
    "MAX_LEN = 129\n",
    "N_LAYERS = 12\n",
    "N_HEADS = 12\n",
    "D_FF = D_MODEL * 4\n",
    "D_K = D_MODEL // N_HEADS\n",
    "D_V = D_MODEL // N_HEADS\n",
    "DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(d_model))\n",
    "        self.bias = nn.Parameter(torch.zeros(d_model))\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, element_length, d_model, max_len):\n",
    "        super().__init__()\n",
    "        self.element_length = element_length\n",
    "        self.d_model = d_model\n",
    "        self.proj = nn.Linear(element_length, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
    "        self.norm = LayerNormalization(d_model)\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long, device=x.device)\n",
    "        pos = pos.unsqueeze(0).expand_as(x[:, :, 0])\n",
    "        tok_emb = self.proj(x.float())  \n",
    "        embedding = tok_emb + self.pos_embed(pos)\n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, Q, K, V):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(D_K)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Linear(D_MODEL, D_K * N_HEADS)\n",
    "        self.W_K = nn.Linear(D_MODEL, D_K * N_HEADS)\n",
    "        self.W_V = nn.Linear(D_MODEL, D_V * N_HEADS)\n",
    "        self.linear = nn.Linear(N_HEADS * D_V, D_MODEL)\n",
    "        self.norm = LayerNormalization(D_MODEL)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        \n",
    "    def forward(self, Q, K, V):\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, N_HEADS, D_K).transpose(1, 2)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, N_HEADS, D_K).transpose(1, 2)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, N_HEADS, D_V).transpose(1, 2)\n",
    "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s)\n",
    "        output = context.transpose(1, 2).contiguous().view(batch_size, -1, N_HEADS * D_V)\n",
    "        output = self.linear(output)\n",
    "        return residual + self.dropout(output), attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(D_MODEL, D_FF)\n",
    "        self.fc2 = nn.Linear(D_FF, D_MODEL)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        self.norm = LayerNormalization(D_MODEL)\n",
    "    def forward(self, x):\n",
    "        output = self.fc2(self.dropout(F.relu(self.fc1(x))))\n",
    "        return x + self.dropout(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "        self.norm = LayerNormalization(D_MODEL)\n",
    "    def forward(self, enc_inputs):\n",
    "        attn_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs)\n",
    "        attn_outputs = self.norm(attn_outputs)\n",
    "        enc_outputs = self.pos_ffn(attn_outputs)\n",
    "        return enc_outputs, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
