{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0939df15-abc7-40e7-83bf-f12820dd42f2",
   "metadata": {},
   "source": [
    "# All Model saves here\n",
    "- option 1: all random separate train / test dataset\n",
    "- solve mpos part and transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cbf7fb-304f-432d-a3f9-580052d5a375",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e94396-1d80-4df3-8648-19368f82c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import DeepMIMOv3\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import IterableDataset\n",
    "import numpy as np\n",
    "import time, gc\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from lwm_model import lwm\n",
    "from torch.optim import Adam\n",
    "from pathlib import Path\n",
    "import torch, time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb2d8b4-7d3d-43ae-b997-2a4e6eab9a1a",
   "metadata": {},
   "source": [
    "## GPU Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a6479f7-d2b2-4632-b3a8-ffe3c49a6608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# GPU \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea87b9c9-a4b1-4442-b26f-606828c4dc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.6\n",
      "90501\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)                   \n",
    "print(torch.backends.cudnn.version())       \n",
    "print(\"CUDA available:\", torch.cuda.is_available())  # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dc1a47-3bc0-4236-813e-c7be4c21bd41",
   "metadata": {},
   "source": [
    "## DeepMIMOv3 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "821eb69b-806e-4694-adf9-fd0efcbf5e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = DeepMIMOv3.default_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1d1f75d-54c0-4198-990f-15eaeec28e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change parameters for the setup\n",
    "# Scenario O1_60 extracted at the dataset_folder\n",
    "#LWM dynamic senario\n",
    "# parameters['dataset_folder'] = r'/content/drive/MyDrive/Colab Notebooks/LWM'\n",
    "scene = 15 # scene 15\n",
    "# change my linux route\n",
    "parameters['dataset_folder'] = '/home/dlghdbs200/LWM'\n",
    "\n",
    "# scnario = 02_dyn_3p5 <- download file\n",
    "parameters['scenario'] = 'O2_dyn_3p5'\n",
    "parameters['dynamic_scenario_scenes'] = np.arange(scene) #scene 0~9\n",
    "\n",
    "# Up to 10 multipath paths per user-to-base station channel\n",
    "parameters['num_paths'] = 10\n",
    "\n",
    "# User rows 1-100\n",
    "parameters['user_rows'] = np.arange(100)\n",
    "# User subsampling\n",
    "parameters['user_subsampling'] = 0.01\n",
    "\n",
    "# Activate only the first basestation\n",
    "parameters['active_BS'] = np.array([1])\n",
    "\n",
    "parameters['activate_OFDM'] = 1\n",
    "\n",
    "parameters['OFDM']['bandwidth'] = 0.05 # 50 MHz\n",
    "parameters['OFDM']['subcarriers'] = 512 # OFDM with 512 subcarriers\n",
    "parameters['OFDM']['selected_subcarriers'] = np.arange(0, 64, 1)\n",
    "#parameters['OFDM']['subcarriers_limit'] = 64 # Keep only first 64 subcarriers\n",
    "\n",
    "parameters['ue_antenna']['shape'] = np.array([1, 1]) # Single antenna\n",
    "parameters['bs_antenna']['shape'] = np.array([1, 32]) # ULA of 32 elements\n",
    "#parameters['bs_antenna']['rotation'] = np.array([0, 30, 90]) # ULA of 32 elements\n",
    "#parameters['ue_antenna']['rotation'] = np.array([[0, 30], [30, 60], [60, 90]]) # ULA of 32 elements\n",
    "#parameters['ue_antenna']['radiation_pattern'] = 'isotropic'\n",
    "#parameters['bs_antenna']['radiation_pattern'] = 'halfwave-dipole'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a9e7bb3-7cb7-4742-a949-9dd84ca31a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                             | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following parameters seem unnecessary:\n",
      "{'activate_OFDM'}\n",
      "\n",
      "Scene 1/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  38%|███████████████████▌                                | 25884/69006 [00:00<00:00, 258776.70it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 260655.04it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 5893.13it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4088.02it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 612.93it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 2/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  37%|███████████████████▍                                | 25789/69006 [00:00<00:00, 257869.34it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 262110.76it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 6194.75it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5053.38it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 340.81it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 3/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  38%|███████████████████▋                                | 26127/69006 [00:00<00:00, 261257.17it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 266320.09it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 5988.53it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4905.62it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 269.99it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 4/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  39%|████████████████████▎                               | 26913/69006 [00:00<00:00, 269107.80it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 267566.60it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 6196.46it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4843.31it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 343.94it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 5/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  38%|████████████████████                                | 26564/69006 [00:00<00:00, 265618.09it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 263929.43it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 5877.76it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4373.62it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 249.85it/s]\u001b[A\n",
      " 33%|████████████████████████████▎                                                        | 1/3 [00:08<00:17,  8.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenes 0–4 generation time: 8.39s\n",
      "The following parameters seem unnecessary:\n",
      "{'activate_OFDM'}\n",
      "\n",
      "Scene 1/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  37%|███████████████████▍                                | 25726/69006 [00:00<00:00, 257242.46it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 260431.53it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 6051.81it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4843.31it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 345.81it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 2/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  39%|████████████████████▎                               | 26973/69006 [00:00<00:00, 269709.68it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 268063.71it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 5884.29it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4514.86it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 246.85it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 3/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  39%|████████████████████▎                               | 26901/69006 [00:00<00:00, 268994.22it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 269571.21it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 5942.60it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5184.55it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 280.11it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 4/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  40%|████████████████████▋                               | 27382/69006 [00:00<00:00, 273803.29it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 273943.38it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 5907.32it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4573.94it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 611.50it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 5/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  39%|████████████████████▏                               | 26719/69006 [00:00<00:00, 267170.51it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 267155.64it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 5954.92it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4922.89it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 725.41it/s]\u001b[A\n",
      " 67%|████████████████████████████████████████████████████████▋                            | 2/3 [00:15<00:07,  7.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenes 5–9 generation time: 7.19s\n",
      "The following parameters seem unnecessary:\n",
      "{'activate_OFDM'}\n",
      "\n",
      "Scene 1/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  40%|████████████████████▌                               | 27326/69006 [00:00<00:00, 273236.81it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 272757.91it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 5716.99it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4718.00it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 615.36it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 2/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  40%|████████████████████▌                               | 27340/69006 [00:00<00:00, 273377.45it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 270589.89it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 5828.07it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4660.34it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 605.06it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 3/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  38%|███████████████████▋                                | 26176/69006 [00:00<00:00, 261736.54it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 264139.95it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 5810.88it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4568.96it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 800.44it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 4/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  39%|████████████████████▏                               | 26744/69006 [00:00<00:00, 267417.94it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 263638.30it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 5761.10it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4922.89it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 652.61it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scene 5/5\n",
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing:   0%|                                                                    | 0/69006 [00:00<?, ?it/s]\u001b[A\n",
      "Reading ray-tracing:  38%|████████████████████                                | 26544/69006 [00:00<00:00, 265419.37it/s]\u001b[A\n",
      "Reading ray-tracing: 100%|████████████████████████████████████████████████████| 69006/69006 [00:00<00:00, 263396.22it/s]\u001b[A\n",
      "\n",
      "Generating channels:   0%|                                                                      | 0/727 [00:00<?, ?it/s]\u001b[A\n",
      "Generating channels: 100%|██████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 5677.59it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BS-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading ray-tracing: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4017.53it/s]\u001b[A\n",
      "\n",
      "Generating channels: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 708.86it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:23<00:00,  7.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenes 10–14 generation time: 7.25s\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## dataset setting (chunked on‑the‑fly generation)\n",
    "import time, gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 0~999 scene index , process 50 at that time\n",
    "scene_indices = np.arange(scene)\n",
    "chunk_size   = 5\n",
    "all_data     = []\n",
    "\n",
    "# Call generate_data for each scene chunk\n",
    "for i in tqdm(range(0, len(scene_indices), chunk_size)):\n",
    "    chunk = scene_indices[i : i+chunk_size].tolist()\n",
    "    parameters['dynamic_scenario_scenes'] = chunk\n",
    "\n",
    "    start = time.time()\n",
    "    data_chunk = DeepMIMOv3.generate_data(parameters)\n",
    "    print(f\"Scenes {chunk[0]}–{chunk[-1]} generation time: {time.time() - start:.2f}s\")\n",
    "\n",
    "    # combine all_data or save in the Disk\n",
    "    all_data.extend(data_chunk)\n",
    "\n",
    "    # free memory \n",
    "    del data_chunk\n",
    "    gc.collect()\n",
    "\n",
    "# comvine Dataset\n",
    "dataset = all_data\n",
    "\n",
    "\n",
    "print(parameters['user_rows'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc75e14-9605-49b0-8034-2a1dc367f97e",
   "metadata": {},
   "source": [
    "## About Information\n",
    "User : 737\n",
    "UE antenna : 1\n",
    "BS antenna : 32  Shape(a+bj)\n",
    "subcarrier : 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48148074-4682-4ab8-acbe-71ef042a5020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unmasked Data Model(gru\n",
    "# separate maksed data and unmasked data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe56d11-0ebb-4c13-b4c2-61fadd9c714b",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ba0a2d3-ee35-43dc-b457-bd400e5b75c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnMaskedChannelSeqDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    IterableDataset (un-masked version)\n",
    "\n",
    "    * Task : predict the next-step channel vector from the past `seq_len` steps.\n",
    "    * Pipeline\n",
    "        1. **Power-normalise** each complex channel vector → real + imag concat.\n",
    "        2. **Min–Max scale** inputs and targets with *one* shared scaler\n",
    "           (fitted on the same power-normalised data).\n",
    "        3. Yield `(sequence, target)` pairs as `torch.FloatTensor`.\n",
    "    * Optionally reuse externally provided scalers (train/val split consistency).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        scenes,\n",
    "        seq_len: int = 5,\n",
    "        eps: float   = 1e-9,\n",
    "        scalers: tuple[MinMaxScaler, MinMaxScaler] | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scenes  = scenes\n",
    "        self.seq_len = seq_len\n",
    "        self.eps     = eps\n",
    "\n",
    "        # --- channel tensor dimensions --------------------------------------\n",
    "        ch0          = scenes[0][0]['user']['channel']        # (U, 1, A, S)\n",
    "        self.U       = ch0.shape[0]                           # users\n",
    "        self.A       = ch0.shape[2]                           # BS antennas\n",
    "        self.S       = ch0.shape[3]                           # sub-carriers\n",
    "        self.vec_len = 2 * self.A                            # real + imag\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # 1) Fit / reuse MinMax scalers on power-normalised data\n",
    "        # --------------------------------------------------------------------\n",
    "        if scalers is None:\n",
    "            X_list, y_list = [], []\n",
    "            T = len(scenes)\n",
    "            for t in range(self.seq_len, T):\n",
    "                past  = scenes[t - self.seq_len : t]\n",
    "                s_tgt = scenes[t]\n",
    "\n",
    "                for u in range(self.U):\n",
    "                    for s in range(self.S):\n",
    "                        # power-normalised sequence (seq_len, vec_len)\n",
    "                        seq_np = np.stack([\n",
    "                            self._power_norm(p[0]['user']['channel'][u, 0, :, s])\n",
    "                            for p in past\n",
    "                        ], axis=0).astype(np.float32)\n",
    "\n",
    "                        # power-normalised target   (vec_len,)\n",
    "                        tgt_np = self._power_norm(\n",
    "                            s_tgt[0]['user']['channel'][u, 0, :, s]\n",
    "                        ).astype(np.float32)\n",
    "\n",
    "                        # skip if empty (all zeros)\n",
    "                        if not np.any(seq_np) or not np.any(tgt_np):\n",
    "                            continue\n",
    "\n",
    "                        X_list.append(seq_np.reshape(-1, self.vec_len))\n",
    "                        y_list.append(tgt_np)\n",
    "\n",
    "            X_all = np.vstack(X_list)          # (N*seq_len, vec_len)\n",
    "            y_all = np.stack(y_list)           # (N, vec_len)\n",
    "            self.scaler_x = MinMaxScaler().fit(X_all)\n",
    "            self.scaler_y = MinMaxScaler().fit(y_all)\n",
    "        else:\n",
    "            # use pre-computed scalers (train/val share the same)\n",
    "            self.scaler_x, self.scaler_y = scalers\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # iterator\n",
    "    # ------------------------------------------------------------------------\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Yields:\n",
    "            seq_tensor   : FloatTensor (seq_len, vec_len)\n",
    "            target_tensor: FloatTensor (vec_len,)\n",
    "        Both tensors are power-normalised **and** Min–Max scaled.\n",
    "        \"\"\"\n",
    "        T = len(self.scenes)\n",
    "        for t in range(self.seq_len, T):\n",
    "            past  = self.scenes[t - self.seq_len : t]\n",
    "            s_tgt = self.scenes[t]\n",
    "\n",
    "            for u in range(self.U):\n",
    "                for s in range(self.S):\n",
    "                    seq_np = np.stack([\n",
    "                        self._power_norm(p[0]['user']['channel'][u, 0, :, s])\n",
    "                        for p in past\n",
    "                    ], axis=0)\n",
    "                    tgt_np = self._power_norm(\n",
    "                        s_tgt[0]['user']['channel'][u, 0, :, s]\n",
    "                    )\n",
    "\n",
    "                    if not np.any(seq_np) or not np.any(tgt_np):\n",
    "                        continue\n",
    "\n",
    "                    # identical scalers for train / val\n",
    "                    N, D = seq_np.shape\n",
    "                    seq_np = self.scaler_x.transform(seq_np.reshape(-1, D)).reshape(N, D)\n",
    "                    tgt_np = self.scaler_y.transform(tgt_np.reshape(1, -1)).reshape(-1,)\n",
    "\n",
    "                    yield torch.from_numpy(seq_np), torch.from_numpy(tgt_np)\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # helpers\n",
    "    # ------------------------------------------------------------------------\n",
    "    def _power_norm(self, h: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert complex vector → real|imag concatenation\n",
    "        and force average power to 1.\n",
    "        \"\"\"\n",
    "        v      = np.concatenate([h.real, h.imag]).astype(np.float32)\n",
    "        power  = np.mean(v * v) + self.eps\n",
    "        return v / np.sqrt(power)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Rough size estimate (not used by IterableDataset).\"\"\"\n",
    "        return (len(self.scenes) - self.seq_len) * self.U * self.S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97d365ac-69a6-494c-bfc7-198bdeb29582",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedChannelSeqDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    IterableDataset for masked channel sequence data.\n",
    "\n",
    "    - Predicts the next-step channel vector from a sequence of past vectors.\n",
    "    - Applies power normalization and MinMax scaling to both inputs and targets.\n",
    "    - Masks 15% of the patches according to:\n",
    "        * 80% chance: replace selected patch with zeros\n",
    "        * 10% chance: replace selected patch with Gaussian noise\n",
    "        * 10% chance: leave the selected patch unchanged\n",
    "      The other 85% of samples are returned unmasked.\n",
    "    \"\"\"\n",
    "    def __init__(self, scenes, seq_len=5, eps=1e-9, noise_std=1.0):\n",
    "        super().__init__()\n",
    "        self.scenes    = scenes\n",
    "        self.seq_len   = seq_len\n",
    "        self.eps       = eps\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        # Determine dimensions: U=users, A=antennas, S=subcarriers\n",
    "        ch0 = scenes[0][0]['user']['channel']   # shape: (U, 1, A, S)\n",
    "        self.U       = ch0.shape[0]\n",
    "        self.A       = ch0.shape[2]\n",
    "        self.S       = ch0.shape[3]\n",
    "        self.vec_len = 2 * self.A               # real+imag concatenated\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # 1) PRECOMPUTE power-norm → fit MinMax scalers on exactly the same data\n",
    "        # ----------------------------------------------------------------------\n",
    "        X_list, y_list = [], []\n",
    "        T = len(scenes)\n",
    "        for t in range(self.seq_len, T):\n",
    "            past         = scenes[t - self.seq_len : t]\n",
    "            target_scene = scenes[t]\n",
    "            for u in range(self.U):\n",
    "                for s in range(self.S):\n",
    "                    # power-normalize each time-slice in the sequence\n",
    "                    seq_np = np.stack([\n",
    "                        self._power_norm(ps[0]['user']['channel'][u,0,:,s])\n",
    "                        for ps in past\n",
    "                    ], axis=0).astype(np.float32)  # (seq_len, vec_len)\n",
    "\n",
    "                    # power-normalize target\n",
    "                    tgt_np = self._power_norm(\n",
    "                        target_scene[0]['user']['channel'][u,0,:,s]\n",
    "                    ).astype(np.float32)            # (vec_len,)\n",
    "\n",
    "                    # skip if invalid\n",
    "                    if not np.any(seq_np) or not np.any(tgt_np):\n",
    "                        continue\n",
    "\n",
    "                    # flatten sequence for scaler\n",
    "                    X_list.append(seq_np.reshape(-1, self.vec_len))\n",
    "                    y_list.append(tgt_np)\n",
    "\n",
    "        # fit scalers on the exact same power-normalized data\n",
    "        X_all = np.vstack(X_list)  # (num_samples*seq_len, vec_len)\n",
    "        y_all = np.stack(y_list)   # (num_samples, vec_len)\n",
    "        self.scaler_x = MinMaxScaler().fit(X_all)\n",
    "        self.scaler_y = MinMaxScaler().fit(y_all)\n",
    "\n",
    "        # prepare a zero-mask vector\n",
    "        self.mask_value = torch.zeros(self.vec_len, dtype=torch.float32)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Yields:\n",
    "            seq_tensor      (seq_len, vec_len) : normalized & scaled sequence\n",
    "            masked_pos      (1,) tensor long   : index of masked time step\n",
    "            target_tensor   (vec_len,)          : normalized & scaled target\n",
    "        \"\"\"\n",
    "        T = len(self.scenes)\n",
    "        # it is not necessary mask because already pretrained about LWM \n",
    "        # so Change mask_prob = 0 \n",
    "        mask_prob  = 0\n",
    "        zero_prob  = mask_prob * 0.8\n",
    "        noise_prob = mask_prob * 0.1\n",
    "\n",
    "        for t in range(self.seq_len, T):\n",
    "            past       = self.scenes[t - self.seq_len : t]\n",
    "            scene_dict = self.scenes[t]\n",
    "\n",
    "            for u in range(self.U):\n",
    "                for s in range(self.S):\n",
    "                    # power-normalize each slice\n",
    "                    seq_np = np.stack([\n",
    "                        self._power_norm(ps[0]['user']['channel'][u,0,:,s])\n",
    "                        for ps in past\n",
    "                    ], axis=0)\n",
    "                    tgt_np = self._power_norm(\n",
    "                        scene_dict[0]['user']['channel'][u,0,:,s]\n",
    "                    )\n",
    "\n",
    "                    if not np.any(seq_np) or not np.any(tgt_np):\n",
    "                        continue\n",
    "\n",
    "                    # MinMax transform (using the same scalers)\n",
    "                    N, D = seq_np.shape\n",
    "                    seq_np = self.scaler_x.transform(seq_np.reshape(-1, D)).reshape(N, D)\n",
    "                    tgt_np = self.scaler_y.transform(tgt_np.reshape(1, -1)).reshape(-1,)\n",
    "\n",
    "                    seq_tensor   = torch.from_numpy(seq_np)\n",
    "                    target_tensor= torch.from_numpy(tgt_np)\n",
    "\n",
    "                    # select mask position\n",
    "                    mpos = random.randrange(self.seq_len)\n",
    "\n",
    "                    r = random.random()\n",
    "                    if r < zero_prob:\n",
    "                        masked_seq = seq_tensor.clone()\n",
    "                        masked_seq[mpos] = self.mask_value\n",
    "                        yield masked_seq, torch.tensor([mpos]), target_tensor\n",
    "\n",
    "                    elif r < zero_prob + noise_prob:\n",
    "                        masked_seq = seq_tensor.clone()\n",
    "                        masked_seq[mpos] = torch.randn(self.vec_len) * self.noise_std\n",
    "                        yield masked_seq, torch.tensor([mpos]), target_tensor\n",
    "\n",
    "                    elif r < mask_prob:\n",
    "                        # masked-but-unchanged\n",
    "                        yield seq_tensor, torch.tensor([mpos]), target_tensor\n",
    "\n",
    "                    else:\n",
    "                        # unmasked\n",
    "                        yield seq_tensor, torch.tensor([mpos]), target_tensor\n",
    "\n",
    "    def _power_norm(self, h: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert complex vector to real+imag concat and normalize power to 1.\n",
    "        \"\"\"\n",
    "        v = np.concatenate([h.real, h.imag]).astype(np.float32)\n",
    "        power = np.mean(v * v) + self.eps\n",
    "        return v / np.sqrt(power)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.scenes) - self.seq_len) * self.U * self.S\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b0cc00-cfa9-49f3-b981-87e55e3d9e20",
   "metadata": {},
   "source": [
    "## define trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fc802ee-c248-4351-9d38-bc8386b76424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_params(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e1373a-f803-49f3-b891-f9d6f59e5dc9",
   "metadata": {},
   "source": [
    "## Split Train/Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29548f28-3188-4562-bb96-634ed3bac496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❷ Train/Validation DataLoader split train : val = 6 : 4\n",
    "seq_len      = 5\n",
    "split_ratio  = 0.6\n",
    "split_idx    = int(len(dataset) * split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3fd809a5-3720-4ffd-90ce-b2d5ccf541f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasked_train_ds = UnMaskedChannelSeqDataset(dataset[:split_idx], seq_len=seq_len)\n",
    "unmasked_val_ds   = UnMaskedChannelSeqDataset(dataset[split_idx:], seq_len=seq_len)\n",
    "\n",
    "# iterate over train_ds to compute min and max of features/targets\n",
    "\n",
    "batch_size   = 32\n",
    "unmasked_train_loader = DataLoader(unmasked_train_ds, batch_size=batch_size, shuffle=False)\n",
    "unmasked_val_loader   = DataLoader(unmasked_val_ds,   batch_size=batch_size, shuffle=False)\n",
    "# ─────────────────────────────────────────────\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49c214a4-1b26-49a7-b308-e16a000f0ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❷ Train/Validation DataLoader split train : val = 6 : 4\n",
    "\n",
    "masked_train_ds = MaskedChannelSeqDataset(dataset[:split_idx], seq_len=seq_len)\n",
    "masked_val_ds   = MaskedChannelSeqDataset(dataset[split_idx:], seq_len=seq_len)\n",
    "\n",
    "# iterate over train_ds to compute min and max of features/targets\n",
    "\n",
    "batch_size   = 32\n",
    "masked_train_loader = DataLoader(masked_train_ds, batch_size=batch_size, shuffle=False)\n",
    "masked_val_loader   = DataLoader(masked_val_ds,   batch_size=batch_size, shuffle=False)\n",
    "# ─────────────────────────────────────────────\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b29d65-387d-4326-b989-afe274279c38",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0383c46a-7e6c-4907-aaa4-e6b1902a770d",
   "metadata": {},
   "source": [
    "LWMWithHead: A wrapper class that uses a pre-trained LWM (Transformer encoder) as the backbone,\n",
    "             and attaches a new fully-connected (FC) head for downstream tasks\n",
    "             (regression, classification, etc.).\n",
    "\n",
    "Changes:\n",
    "- input_dim: Dimension of the actual input data (e.g., 64)\n",
    "- patch_length: Patch length expected by the backbone (e.g., 16)\n",
    "- Replaces the original element_length parameter with these two distinct parameters\n",
    "- Applies a projection layer (self.input_proj) in forward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80c33fca-38fa-4842-b886-69972e5d28a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LWMWithHead(nn.Module):\n",
    "    \"\"\"\n",
    "    LWMWithHead: A wrapper class that uses a pre-trained LWM (Transformer encoder) as the backbone,\n",
    "                 and attaches a new fully-connected (FC) head for downstream tasks\n",
    "                 (regression, classification, etc.).\n",
    "\n",
    "    Changes:\n",
    "    - input_dim: Dimension of the actual input data (e.g., 64)\n",
    "    - patch_length: Patch length expected by the backbone (e.g., 16)\n",
    "    - Replaces the original element_length parameter with these two distinct parameters\n",
    "    - Applies a projection layer (self.input_proj) in forward()\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,                 # Dimension of the actual input data (e.g., 64)\n",
    "        patch_length: int,              # Patch length expected by the backbone (e.g., 16)\n",
    "        d_model: int = 64,              # LWM hidden size\n",
    "        max_len: int = 129,             # Positional encoding max length\n",
    "        n_layers: int = 12,             # Number of Transformer encoder layers\n",
    "        hidden_dim: int = 256,          # FC head hidden dimension\n",
    "        out_dim: int = 64,              # FC head output dimension\n",
    "        freeze_backbone: bool = True,   # Whether to freeze the backbone\n",
    "        checkpoint_path: str | None = \"./model_weights.pth\",\n",
    "        device: str = \"cuda\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # apply a projection layer to match backbone's expected patch_length\n",
    "        self.input_proj = nn.Linear(input_dim, patch_length)\n",
    "\n",
    "        # initialize backbone\n",
    "        if checkpoint_path is None:\n",
    "            # randomly initialized backbone\n",
    "            self.backbone = lwm(\n",
    "                element_length=patch_length,\n",
    "                d_model=d_model,\n",
    "                max_len=max_len,\n",
    "                n_layers=n_layers\n",
    "            ).to(device)\n",
    "        else:\n",
    "            # load pre-trained weights\n",
    "            self.backbone = lwm.from_pretrained(\n",
    "                ckpt_name=checkpoint_path,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "        # freeze backbone parameters if required\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # attach a new fully-connected head for downstream tasks\n",
    "        self.head = nn.Sequential(\n",
    "            # change 2 layer -> 1 layer\n",
    "            nn.Linear(d_model, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, masked_pos: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: Tensor of shape (B, L, input_dim)\n",
    "            masked_pos: Tensor of shape (B, num_mask)\n",
    "        Returns:\n",
    "            out: Tensor of shape (B, out_dim)\n",
    "        \"\"\"\n",
    "        # project inputs to patch_length dimension\n",
    "        x = self.input_proj(input_ids)\n",
    "\n",
    "        # backbone forward: returns (logits_lm, enc_output)\n",
    "        _, enc_output = self.backbone(x, masked_pos)\n",
    "\n",
    "        # extract CLS token feature (first token)\n",
    "        feat = enc_output[:, 0, :]\n",
    "\n",
    "        # pass through FC head to get final output\n",
    "        out = self.head(feat)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2e8f0ee-6782-4eb2-b6c3-fe363ed8d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GRUWithHead(nn.Module):\n",
    "    \"\"\"\n",
    "    GRUWithHead (projected):\n",
    "      • Projects the raw feature dimension (input_dim) to a smaller patch_length\n",
    "        so every backbone receives the same patch-sized input (like LWM).\n",
    "      • Stacks N GRU layers, then an FC head for downstream tasks.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int    = 64,   # raw feature dimension coming from the DataLoader\n",
    "        patch_length: int = 16,   # target dimension fed to the GRU backbone\n",
    "        d_model: int      = 64,   # GRU hidden size\n",
    "        n_layers: int     = 12,   # number of stacked GRU layers\n",
    "        bidirectional: bool = True,\n",
    "        dropout: float      = 0.1,\n",
    "        hidden_dim: int     = 256, # FC-head hidden size\n",
    "        out_dim: int        = 64,  # FC-head output size\n",
    "        freeze_backbone: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 0) Project raw_dim → patch_length (64 → 16)\n",
    "        self.input_proj = nn.Linear(input_dim, patch_length)\n",
    "\n",
    "        # 1) GRU backbone that expects 'patch_length' features per time step\n",
    "        self.backbone = nn.GRU(\n",
    "            input_size     = patch_length,\n",
    "            hidden_size    = d_model,\n",
    "            num_layers     = n_layers,\n",
    "            batch_first    = True,\n",
    "            bidirectional  = bidirectional,\n",
    "            dropout        = dropout if n_layers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # 2) Fully-connected head\n",
    "        gru_out_dim = d_model * (2 if bidirectional else 1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(gru_out_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x : Tensor of shape (batch, seq_len, input_dim) – raw features\n",
    "        Returns:\n",
    "            Tensor of shape (batch, out_dim)\n",
    "        \"\"\"\n",
    "        # project raw features to patch_length\n",
    "        x_proj = self.input_proj(x)                 # (B, seq_len, patch_length)\n",
    "\n",
    "        # sequence modelling with GRU\n",
    "        out, _ = self.backbone(x_proj)              # (B, seq_len, num_dirs*d_model)\n",
    "\n",
    "        # use the last time-step representation\n",
    "        feat = out[:, -1, :]                        # (B, gru_out_dim)\n",
    "\n",
    "        # downstream head\n",
    "        return self.head(feat)                      # (B, out_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f9d84d1-33a1-4f34-9e97-51242fc2d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # Create positional encoding matrix of shape (1, max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            Tensor: x plus positional encodings\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, feat_dim: int, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # Optional linear projection from feat_dim to d_model\n",
    "        self.proj = nn.Linear(feat_dim, d_model) if feat_dim != d_model else None\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch, seq_len, feat_dim)\n",
    "        Returns:\n",
    "            Tensor of shape (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        if self.proj is not None:\n",
    "            x = self.proj(x)\n",
    "        return self.pos_enc(x)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dim_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        # Multi-Head Self-Attention\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        # Position-wise Feed-Forward Network\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_ff),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_ff, d_model)\n",
    "        )\n",
    "        # Layer Normalization and Dropout for residual connections\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        src_mask: torch.Tensor = None,\n",
    "        src_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (seq_len, batch, d_model)\n",
    "            src_mask: Optional Tensor of shape (seq_len, seq_len)\n",
    "            src_key_padding_mask: Optional Tensor of shape (batch, seq_len)\n",
    "        Returns:\n",
    "            Tensor of shape (seq_len, batch, d_model)\n",
    "        \"\"\"\n",
    "        # Self-attention sublayer\n",
    "        attn_out, _ = self.self_attn(x, x, x, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)\n",
    "        x = x + self.dropout1(attn_out)\n",
    "        x = self.norm1(x)\n",
    "        # Feed-forward sublayer\n",
    "        ff_out = self.ff(x)\n",
    "        x = x + self.dropout2(ff_out)\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderCustom(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feat_dim: int,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        dim_ff: int,\n",
    "        n_layers: int,\n",
    "        dropout: float = 0.1,\n",
    "        max_len: int = 5000\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Input embedding: feature projection + positional encoding\n",
    "        self.input_embedding = InputEmbedding(feat_dim, d_model, max_len)\n",
    "        # Stack of N encoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, n_heads, dim_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        src_mask: torch.Tensor = None,\n",
    "        src_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch, seq_len, feat_dim)\n",
    "        Returns:\n",
    "            Tensor of shape (seq_len, batch, d_model)\n",
    "        \"\"\"\n",
    "        x = self.input_embedding(x)       # (batch, seq_len, d_model)\n",
    "        x = x.transpose(0, 1)             # (seq_len, batch, d_model)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dim_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        # Masked Self-Attention\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        # Encoder-Decoder Attention\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        # Position-wise Feed-Forward Network\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_ff),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_ff, d_model)\n",
    "        )\n",
    "        # Layer Normalizations and Dropouts\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt: torch.Tensor,\n",
    "        memory: torch.Tensor,\n",
    "        tgt_mask: torch.Tensor = None,\n",
    "        memory_mask: torch.Tensor = None,\n",
    "        tgt_key_padding_mask: torch.Tensor = None,\n",
    "        memory_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tgt: Tensor of shape (tgt_len, batch, d_model)\n",
    "            memory: Tensor of shape (src_len, batch, d_model)\n",
    "        Returns:\n",
    "            Tensor of shape (tgt_len, batch, d_model)\n",
    "        \"\"\"\n",
    "        # Masked self-attention sublayer\n",
    "        attn1, _ = self.self_attn(\n",
    "            tgt, tgt, tgt,\n",
    "            attn_mask=tgt_mask,\n",
    "            key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "        tgt = tgt + self.dropout1(attn1)\n",
    "        tgt = self.norm1(tgt)\n",
    "        # Encoder-decoder attention sublayer\n",
    "        attn2, _ = self.multihead_attn(\n",
    "            tgt, memory, memory,\n",
    "            attn_mask=memory_mask,\n",
    "            key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "        tgt = tgt + self.dropout2(attn2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        # Feed-forward sublayer\n",
    "        ff_out = self.ff(tgt)\n",
    "        tgt = tgt + self.dropout3(ff_out)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "class TransformerDecoderCustom(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feat_dim: int,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        dim_ff: int,\n",
    "        n_layers: int,\n",
    "        dropout: float = 0.1,\n",
    "        max_len: int = 5000\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Input embedding for target sequence\n",
    "        self.input_embedding = InputEmbedding(feat_dim, d_model, max_len)\n",
    "        # Stack of N decoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, n_heads, dim_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        # Final projection back to feature dimension\n",
    "        # self.output_linear = nn.Linear(d_model, feat_dim)\n",
    "        self.output_linear = nn.Identity()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt: torch.Tensor,\n",
    "        memory: torch.Tensor,\n",
    "        tgt_mask: torch.Tensor = None,\n",
    "        memory_mask: torch.Tensor = None,\n",
    "        tgt_key_padding_mask: torch.Tensor = None,\n",
    "        memory_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tgt: Tensor of shape (batch, tgt_len, feat_dim)\n",
    "            memory: Tensor of shape (src_len, batch, d_model)\n",
    "        Returns:\n",
    "            Tensor of shape (batch, tgt_len, feat_dim)\n",
    "        \"\"\"\n",
    "        x = self.input_embedding(tgt)       # (batch, tgt_len, d_model)\n",
    "        x = x.transpose(0, 1)               # (tgt_len, batch, d_model)\n",
    "        for layer in self.layers:\n",
    "            x = layer(\n",
    "                x,\n",
    "                memory,\n",
    "                tgt_mask=tgt_mask,\n",
    "                memory_mask=memory_mask,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                memory_key_padding_mask=memory_key_padding_mask\n",
    "            )\n",
    "        x = x.transpose(0, 1)               # (batch, tgt_len, d_model)\n",
    "        return self.output_linear(x)        # project back to feat_dim\n",
    "\n",
    "        \n",
    "\n",
    "class TransformerWithHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int    = 64,   # raw feature dimension\n",
    "        patch_length: int = 16,   # sequence length consumed by encoder/decoder\n",
    "        d_model: int      = 64,   # hidden size inside the transformer\n",
    "        n_heads: int      = 4,\n",
    "        dim_ff: int       = 256,\n",
    "        n_layers: int     = 12,\n",
    "        dropout: float    = 0.1,\n",
    "        hidden_dim: int   = 256,\n",
    "        out_dim: int      = 64,\n",
    "        max_len: int      = 5000,\n",
    "        freeze_backbone: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 0) Project raw input dimension to patch length\n",
    "        self.input_proj = nn.Linear(input_dim, patch_length)\n",
    "\n",
    "        # 1) Encoder: processes the source sequence\n",
    "        self.encoder = TransformerEncoderCustom(\n",
    "            feat_dim = patch_length,\n",
    "            d_model  = d_model,\n",
    "            n_heads  = n_heads,\n",
    "            dim_ff   = dim_ff,\n",
    "            n_layers = n_layers,\n",
    "            dropout  = dropout,\n",
    "            max_len  = max_len,\n",
    "        )\n",
    "        if freeze_backbone:\n",
    "            for p in self.encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # 2) Decoder: generates target sequence using encoder memory\n",
    "        self.decoder = TransformerDecoderCustom(\n",
    "            feat_dim = patch_length,\n",
    "            d_model  = d_model,\n",
    "            n_heads  = n_heads,\n",
    "            dim_ff   = dim_ff,\n",
    "            n_layers = n_layers,\n",
    "            dropout  = dropout,\n",
    "            max_len  = max_len,\n",
    "        )\n",
    "\n",
    "        # 3) Task head: maps final decoder output to desired output dimension\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,                # (batch, src_len, input_dim)\n",
    "        tgt: torch.Tensor,                # (batch, tgt_len, input_dim)\n",
    "        src_mask: torch.Tensor = None,\n",
    "        src_key_padding_mask: torch.Tensor = None,\n",
    "        tgt_mask: torch.Tensor = None,\n",
    "        tgt_key_padding_mask: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        # 1) Encode source sequence to produce memory\n",
    "        src_patch = self.input_proj(src)  # (batch, src_len, patch_length)\n",
    "        memory = self.encoder(\n",
    "            src_patch,\n",
    "            src_mask=src_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask\n",
    "        )  # (src_len, batch, d_model)\n",
    "\n",
    "        # 2) Decode target sequence using encoder memory\n",
    "        tgt_patch = self.input_proj(tgt)  # (batch, tgt_len, patch_length)\n",
    "        dec_out = self.decoder(\n",
    "            tgt_patch,\n",
    "            memory,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=None,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=src_key_padding_mask\n",
    "        )  # (batch, tgt_len, d_model)\n",
    "\n",
    "        # 3) Use last time-step output from decoder for prediction\n",
    "        last_step = dec_out[:, -1, :]      # (batch, d_model)\n",
    "        return self.head(last_step)        # (batch, out_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e2503278-03dd-4b6d-8497-e9f9d83e10fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNWithHead(nn.Module):\n",
    "    \"\"\"\n",
    "    RNNWithHead (projected):\n",
    "      • Projects raw feature vectors from `input_dim` to `patch_length`\n",
    "      • Feeds the projected sequence to an RNN backbone\n",
    "      • Maps the last hidden state through an FC head\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int    = 64,   # raw feature dimension coming from DataLoader\n",
    "        patch_length: int = 16,   # dimension consumed by the RNN backbone\n",
    "        hidden_size: int  = 64,   # RNN hidden size\n",
    "        num_layers: int   = 12,   # number of stacked RNN layers\n",
    "        bidirectional: bool = True,\n",
    "        dropout: float      = 0.1,\n",
    "        hidden_dim: int     = 256, # FC-head hidden size\n",
    "        out_dim: int        = 64,  # FC-head output size\n",
    "        freeze_backbone: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 0) project raw 64-dim → 16-dim\n",
    "        self.input_proj = nn.Linear(input_dim, patch_length)\n",
    "\n",
    "        # 1) RNN backbone\n",
    "        self.backbone = nn.RNN(\n",
    "            input_size     = patch_length,\n",
    "            hidden_size    = hidden_size,\n",
    "            num_layers     = num_layers,\n",
    "            batch_first    = True,\n",
    "            bidirectional  = bidirectional,\n",
    "            dropout        = dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # 2) FC head\n",
    "        rnn_out_dim = hidden_size * (2 if bidirectional else 1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(rnn_out_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, input_dim=64)\n",
    "        returns: (batch, out_dim)\n",
    "        \"\"\"\n",
    "        x_proj = self.input_proj(x)           # (batch, seq_len, 16)\n",
    "        out, _ = self.backbone(x_proj)        # (batch, seq_len, rnn_out_dim)\n",
    "        feat   = out[:, -1, :]                # take last time step\n",
    "        return self.head(feat)                # (batch, out_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b210558c-7f51-4e6b-ad3a-8efa5237a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMWithHead(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTMWithHead (projected):\n",
    "      • Projects raw feature vectors from `input_dim` to a compact `patch_length`\n",
    "      • Feeds the projected sequence to an LSTM backbone\n",
    "      • Uses the last hidden state to drive an FC head for the downstream task\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int    = 64,   # raw feature dimension (e.g., 64)\n",
    "        patch_length: int = 16,   # dimension consumed by the LSTM backbone\n",
    "        hidden_size: int  = 64,   # LSTM hidden size\n",
    "        num_layers: int   = 12,   # number of stacked LSTM layers\n",
    "        bidirectional: bool = True,\n",
    "        dropout: float      = 0.1,\n",
    "        hidden_dim: int     = 256, # FC-head hidden size\n",
    "        out_dim: int        = 64,  # FC-head output size\n",
    "        freeze_backbone: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 0) Raw 64-dim → 16-dim patch projection\n",
    "        self.input_proj = nn.Linear(input_dim, patch_length)\n",
    "\n",
    "        # 1) LSTM backbone that expects `patch_length` features\n",
    "        self.backbone = nn.LSTM(\n",
    "            input_size     = patch_length,\n",
    "            hidden_size    = hidden_size,\n",
    "            num_layers     = num_layers,\n",
    "            batch_first    = True,\n",
    "            bidirectional  = bidirectional,\n",
    "            dropout        = dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # 2) FC head\n",
    "        lstm_out_dim = hidden_size * (2 if bidirectional else 1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(lstm_out_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, input_dim=64)\n",
    "        returns: (batch, out_dim)\n",
    "        \"\"\"\n",
    "        # project raw features to patch_length\n",
    "        x_proj = self.input_proj(x)             # (B, seq_len, 16)\n",
    "\n",
    "        # sequence modeling with LSTM\n",
    "        out, _ = self.backbone(x_proj)          # (B, seq_len, lstm_out_dim)\n",
    "\n",
    "        # take the last time-step representation\n",
    "        feat = out[:, -1, :]                    # (B, lstm_out_dim)\n",
    "\n",
    "        # downstream head\n",
    "        return self.head(feat)                  # (B, out_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4766f28-d102-4ed0-8272-7d5098ad9f15",
   "metadata": {},
   "source": [
    "## fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e104da9-d8e8-42ea-a810-3567b9ae412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────\n",
    "# Shared hyper-parameters\n",
    "# ──────────────────────────\n",
    "INPUT_DIM     = 64     # raw feature dimension\n",
    "PATCH_LENGTH  = 16     # dimension fed to every backbone\n",
    "D_MODEL       = 64     # internal hidden size (GRU/LSTM/Transformer)\n",
    "N_LAYERS      = 12     # stacked layers\n",
    "HIDDEN_DIM    = 256    # head hidden dimension\n",
    "OUT_DIM       = 64     # head output dimension\n",
    "DROPOUT       = 0.1    # dropout for recurrent / transformer blocks\n",
    "BIDIRECTIONAL = True   # use bidirectional RNNs\n",
    "DEVICE        = \"cuda\"\n",
    "\n",
    "# ──────────────────────────\n",
    "# Model class catalog\n",
    "# ──────────────────────────\n",
    "MODEL_CATALOG = {\n",
    "    \"LWM_freeze_backbone\"     : LWMWithHead,\n",
    "    \"LWM_pretrained_Fine_tune\": LWMWithHead,\n",
    "    \"LWM_Fine_tune\"           : LWMWithHead,\n",
    "    \"gru\"                     : GRUWithHead,\n",
    "    \"RNN\"                     : RNNWithHead,\n",
    "    \"LSTM\"                    : LSTMWithHead,\n",
    "    \"Transformer\"             : TransformerWithHead,\n",
    "}\n",
    "\n",
    "# ──────────────────────────\n",
    "# Per-model constructor kwargs\n",
    "# ──────────────────────────\n",
    "MODEL_PARAMS = {\n",
    "    # ── LWM variants ─────────────────────────────\n",
    "    \"LWM_freeze_backbone\": {\n",
    "        \"input_dim\"       : INPUT_DIM,\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"d_model\"         : D_MODEL,\n",
    "        \"max_len\"         : PATCH_LENGTH + 1,\n",
    "        \"n_layers\"        : N_LAYERS,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"freeze_backbone\" : True,\n",
    "        \"checkpoint_path\" : \"./model_weights.pth\",\n",
    "        \"device\"          : DEVICE,\n",
    "    },\n",
    "    \"LWM_pretrained_Fine_tune\": {\n",
    "        \"input_dim\"       : INPUT_DIM,\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"d_model\"         : D_MODEL,\n",
    "        \"max_len\"         : PATCH_LENGTH + 1,\n",
    "        \"n_layers\"        : N_LAYERS,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"freeze_backbone\" : False,\n",
    "        \"checkpoint_path\" : \"./model_weights.pth\",\n",
    "        \"device\"          : DEVICE,\n",
    "    },\n",
    "    \"LWM_Fine_tune\": {\n",
    "        \"input_dim\"       : INPUT_DIM,\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"d_model\"         : D_MODEL,\n",
    "        \"max_len\"         : PATCH_LENGTH + 1,\n",
    "        \"n_layers\"        : N_LAYERS,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"freeze_backbone\" : False,\n",
    "        \"checkpoint_path\" : None,\n",
    "        \"device\"          : DEVICE,\n",
    "    },\n",
    "\n",
    "    # ── GRU (projected) ──────────────────────────\n",
    "    \"gru\": {\n",
    "        \"input_dim\"       : INPUT_DIM,     # 64 → project → 16\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"d_model\"         : D_MODEL,\n",
    "        \"n_layers\"        : N_LAYERS,\n",
    "        \"bidirectional\"   : BIDIRECTIONAL,\n",
    "        \"dropout\"         : DROPOUT,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"freeze_backbone\" : False,\n",
    "    },\n",
    "\n",
    "    # ── Vanilla RNN (projected) ──────────────────\n",
    "    \"RNN\": {\n",
    "        \"input_dim\"       : INPUT_DIM,\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"hidden_size\"     : D_MODEL,\n",
    "        \"num_layers\"      : N_LAYERS,\n",
    "        \"bidirectional\"   : BIDIRECTIONAL,\n",
    "        \"dropout\"         : 0.0,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"freeze_backbone\" : False,\n",
    "    },\n",
    "\n",
    "    # ── LSTM (projected) ─────────────────────────\n",
    "    \"LSTM\": {\n",
    "        \"input_dim\"       : INPUT_DIM,\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"hidden_size\"     : D_MODEL,\n",
    "        \"num_layers\"      : N_LAYERS,\n",
    "        \"bidirectional\"   : BIDIRECTIONAL,\n",
    "        \"dropout\"         : DROPOUT,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"freeze_backbone\" : False,\n",
    "    },\n",
    "\n",
    "    # ── Transformer (projected) ──────────────────\n",
    "    \"Transformer\": {\n",
    "        \"input_dim\"       : INPUT_DIM,\n",
    "        \"patch_length\"    : PATCH_LENGTH,\n",
    "        \"d_model\"         : D_MODEL,\n",
    "        \"n_heads\"         : 4,\n",
    "        \"dim_ff\"          : 256,\n",
    "        \"n_layers\"        : N_LAYERS,\n",
    "        \"dropout\"         : DROPOUT,\n",
    "        \"hidden_dim\"      : HIDDEN_DIM,\n",
    "        \"out_dim\"         : OUT_DIM,\n",
    "        \"max_len\"         : PATCH_LENGTH + 1,\n",
    "        \"freeze_backbone\" : False,\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c37de2-4bc1-42aa-9a9e-cfa43bf0c3fd",
   "metadata": {},
   "source": [
    "## model evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e9e706d-4875-4ee2-93d9-f792571fe2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def rmse(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Root-Mean-Squared Error\n",
    "    \"\"\"\n",
    "    return torch.sqrt(F.mse_loss(pred, target, reduction=\"mean\"))   # √MSE\n",
    "\n",
    "def nmse(pred: torch.Tensor, target: torch.Tensor, eps : float = 1e-12) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Normalized MSE  =  E[‖ŷ − y‖²] / E[‖y‖²]\n",
    "    \"\"\"\n",
    "    # (B, …) → (B,)  \n",
    "    mse_per_sample   = ((pred - target)**2).view(pred.size(0), -1).sum(dim=1)\n",
    "    power_per_sample = (target**2).view(target.size(0), -1).sum(dim=1) + eps\n",
    "    return (mse_per_sample / power_per_sample).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3930b75e-0895-4fb1-8039-744c86730b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_evaluate(model, loader, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Validation loop for IterableDataset.\n",
    "    Returns average RMSE and NMSE over all samples.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_rmse, total_nmse, total_samples = 0.0, 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, masked_pos, target in loader:\n",
    "            # Move to device\n",
    "            input_ids, masked_pos, target = (\n",
    "                input_ids.to(device),\n",
    "                masked_pos.to(device),\n",
    "                target.to(device),\n",
    "            )\n",
    "            # Batch size\n",
    "            bs = input_ids.size(0)\n",
    "\n",
    "            # Forward\n",
    "            pred = model(input_ids, masked_pos)\n",
    "\n",
    "            # Accumulate batch metrics\n",
    "            total_rmse    += rmse(pred, target).item() * bs\n",
    "            total_nmse    += nmse(pred, target).item() * bs\n",
    "            total_samples += bs\n",
    "\n",
    "    # Compute averages\n",
    "    return {\n",
    "        \"RMSE\": total_rmse / total_samples,\n",
    "        \"NMSE\": total_nmse / total_samples\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "92d4ceda-b090-4b01-91a4-698c319420f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def unmasked_evaluate(model, loader, device, patch_length=4):\n",
    "    \"\"\"\n",
    "    Validation loop for IterableDataset.\n",
    "    Computes and returns the average RMSE and NMSE over the dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_rmse, total_nmse, total_samples = 0.0, 0.0, 0\n",
    "\n",
    "    # Inspect the model's forward signature to determine if it requires a decoder input\n",
    "    sig = inspect.signature(model.forward)\n",
    "    needs_tgt = len(sig.parameters) >= 3  # True if forward(self, src, tgt, ...) exists\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, target in loader:\n",
    "            # Move input and target tensors to the specified device\n",
    "            input_ids = input_ids.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            if needs_tgt:\n",
    "                # Transformer models: use the last `patch_length` time steps as decoder input\n",
    "                tgt = input_ids[:, -patch_length:, :]\n",
    "                pred = model(input_ids, tgt)\n",
    "            else:\n",
    "                # Single-input models (e.g., GRU, LSTM): only the source sequence is needed\n",
    "                pred = model(input_ids)\n",
    "\n",
    "            # Accumulate weighted metrics\n",
    "            batch_size = input_ids.size(0)\n",
    "            total_rmse += rmse(pred, target).item() * batch_size\n",
    "            total_nmse += nmse(pred, target).item() * batch_size\n",
    "            total_samples += batch_size\n",
    "\n",
    "    # Calculate average RMSE and NMSE over all samples\n",
    "    avg_rmse = total_rmse / total_samples\n",
    "    avg_nmse = total_nmse / total_samples\n",
    "\n",
    "    return {\n",
    "        \"RMSE\": avg_rmse,\n",
    "        \"NMSE\": avg_nmse\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c0dcd-d4af-4d7b-87cd-154d81d210fb",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "317fde92-dd4d-4092-b93c-7b1b83674c52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training LWM_freeze_backbone ===\n",
      "Model loaded successfully from ./model_weights.pth to cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/1] TrainLoss: 0.0000  Val RMSE: 0.4974  Val NMSE: 9.2923e-01  Val NMSE_dB: -0.3 dB  TrainTime: 37.61s\n",
      "🕒 LWM_freeze_backbone – avg train time / epoch: 37.61s\n",
      "\n",
      "=== Training LWM_pretrained_Fine_tune ===\n",
      "Model loaded successfully from ./model_weights.pth to cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/1] TrainLoss: 0.0000  Val RMSE: 0.5305  Val NMSE: 1.0576e+00  Val NMSE_dB: 0.2 dB  TrainTime: 39.28s\n",
      "🕒 LWM_pretrained_Fine_tune – avg train time / epoch: 39.28s\n",
      "\n",
      "=== Training LWM_Fine_tune ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/1] TrainLoss: 0.0000  Val RMSE: 0.7648  Val NMSE: 2.2081e+00  Val NMSE_dB: 3.4 dB  TrainTime: 39.48s\n",
      "🕒 LWM_Fine_tune – avg train time / epoch: 39.48s\n",
      "\n",
      "=== Training gru ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/1] TrainLoss: 0.0000  Val RMSE: 0.5146  Val NMSE: 9.9495e-01  Val NMSE_dB: -0.0 dB  TrainTime: 40.41s\n",
      "🕒 gru – avg train time / epoch: 40.41s\n",
      "\n",
      "=== Training RNN ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/1] TrainLoss: 0.0000  Val RMSE: 0.5060  Val NMSE: 9.6191e-01  Val NMSE_dB: -0.2 dB  TrainTime: 39.77s\n",
      "🕒 RNN – avg train time / epoch: 39.77s\n",
      "\n",
      "=== Training LSTM ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/1] TrainLoss: 0.0000  Val RMSE: 0.5166  Val NMSE: 1.0030e+00  Val NMSE_dB: 0.0 dB  TrainTime: 40.00s\n",
      "🕒 LSTM – avg train time / epoch: 40.00s\n",
      "\n",
      "=== Training Transformer ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     71\u001b[0m     tgt \u001b[38;5;241m=\u001b[39m xb[:,\u001b[38;5;241m4\u001b[39m:,:]\n\u001b[0;32m---> 72\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# pred   = model(xb)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[33], line 304\u001b[0m, in \u001b[0;36mTransformerWithHead.forward\u001b[0;34m(self, src, tgt, src_mask, src_key_padding_mask, tgt_mask, tgt_key_padding_mask)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# 2) Decode target sequence using encoder memory\u001b[39;00m\n\u001b[1;32m    303\u001b[0m tgt_patch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_proj(tgt)  \u001b[38;5;66;03m# (batch, tgt_len, patch_length)\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_patch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch, tgt_len, d_model)\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# 3) Use last time-step output from decoder for prediction\u001b[39;00m\n\u001b[1;32m    314\u001b[0m last_step \u001b[38;5;241m=\u001b[39m dec_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]      \u001b[38;5;66;03m# (batch, d_model)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[33], line 222\u001b[0m, in \u001b[0;36mTransformerDecoderCustom.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    220\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)               \u001b[38;5;66;03m# (tgt_len, batch, d_model)\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 222\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)               \u001b[38;5;66;03m# (batch, tgt_len, d_model)\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_linear(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[33], line 159\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m    tgt: Tensor of shape (tgt_len, batch, d_model)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    Tensor of shape (tgt_len, batch, d_model)\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# Masked self-attention sublayer\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m attn1, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m tgt \u001b[38;5;241m=\u001b[39m tgt \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(attn1)\n\u001b[1;32m    165\u001b[0m tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(tgt)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/activation.py:1373\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1347\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1348\u001b[0m         query,\n\u001b[1;32m   1349\u001b[0m         key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m   1371\u001b[0m     )\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1373\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1393\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:6230\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[1;32m   6227\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m   6228\u001b[0m         in_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   6229\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 6230\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43m_in_projection_packed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6232\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m   6233\u001b[0m         q_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   6234\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:5617\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   5614\u001b[0m     proj \u001b[38;5;241m=\u001b[39m linear(q, w, b)\n\u001b[1;32m   5615\u001b[0m     \u001b[38;5;66;03m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[39;00m\n\u001b[1;32m   5616\u001b[0m     proj \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 5617\u001b[0m         \u001b[43mproj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5618\u001b[0m         \u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   5619\u001b[0m         \u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   5620\u001b[0m         \u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   5621\u001b[0m         \u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m   5622\u001b[0m     )\n\u001b[1;32m   5623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m proj[\u001b[38;5;241m0\u001b[39m], proj[\u001b[38;5;241m1\u001b[39m], proj[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   5624\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5625\u001b[0m     \u001b[38;5;66;03m# encoder-decoder attention\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:1432\u001b[0m, in \u001b[0;36mTensor.unflatten\u001b[0;34m(self, dim, sizes)\u001b[0m\n\u001b[1;32m   1430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39munflatten(dim, sizes, names)\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msizes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Unified training / validation script\n",
    "------------------------------------\n",
    "* Trains every architecture listed in MODEL_CATALOG\n",
    "* Chooses masked / un-masked DataLoader automatically\n",
    "* Reports per-epoch speed & validation scores\n",
    "* Saves **best** and **last** checkpoints under ./checkpoints/\n",
    "\"\"\"\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 0) Globals and hyper-parameters\n",
    "# ─────────────────────────────────────────────\n",
    "device      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion   = nn.MSELoss().to(device)\n",
    "\n",
    "NUM_EPOCHS  = 10\n",
    "LR          = 1e-4                         # learning-rate\n",
    "CKPT_DIR    = Path(\"checkpoints\")          # where *.pth files will be stored\n",
    "CKPT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "total_start = time.time()                  # wall-clock timer for *all* models\n",
    "results     = {}                           # best-epoch NMSE(dB) for every model\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 1) Train / validate each model\n",
    "# ─────────────────────────────────────────────\n",
    "for model_name, ModelCls in MODEL_CATALOG.items():\n",
    "\n",
    "    print(f\"\\n=== Training {model_name} ===\")\n",
    "    model_args = MODEL_PARAMS[model_name]\n",
    "    model      = ModelCls(**model_args).to(device)\n",
    "\n",
    "    # collect only trainable parameters\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    if len(trainable_params) == 0:\n",
    "        print(f\"⚠️  '{model_name}' has no trainable parameters — skipping.\")\n",
    "        results[model_name] = float(\"nan\")\n",
    "        continue\n",
    "\n",
    "    optimizer   = torch.optim.Adam(trainable_params, lr=LR)\n",
    "    epoch_times = []                       # per-epoch training duration\n",
    "    best_nmse   = float(\"inf\")             # track the best val-NMSE\n",
    "\n",
    "    # pick loaders / evaluation fn based on model family\n",
    "    uses_mask  = model_name.startswith(\"LWM_\") \n",
    "    tr_loader  = masked_train_loader if uses_mask else unmasked_train_loader\n",
    "    val_loader = masked_val_loader  if uses_mask else unmasked_val_loader\n",
    "    eval_fn    = masked_evaluate    if uses_mask else unmasked_evaluate\n",
    "\n",
    "    # ── EPOCH LOOP ──────────────────────────\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "\n",
    "        # ---------- TRAIN ----------\n",
    "        t0 = time.time()\n",
    "        # model.train()\n",
    "        run_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(tr_loader,\n",
    "                    desc=f\"[{model_name} {epoch:02d}/{NUM_EPOCHS}] train\",\n",
    "                    leave=False)\n",
    "\n",
    "        for b, batch in enumerate(pbar, 1):\n",
    "            if uses_mask:\n",
    "                xb, mpos, yb = [x.to(device) for x in batch]\n",
    "                pred = model(xb, mpos).squeeze(-1)\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                xb, yb = [x.to(device) for x in batch]\n",
    "                if model_name == \"Transformer\":\n",
    "                    tgt = xb[:,4:,:]\n",
    "                    pred = model(xb, tgt)\n",
    "                else:\n",
    "                    pred   = model(xb)\n",
    "                    \n",
    "                    \n",
    "\n",
    "            loss = criterion(pred, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            run_loss += loss.item()\n",
    "            if b % 100 == 0:\n",
    "                pbar.set_postfix(train_loss=run_loss / b)\n",
    "\n",
    "        epoch_times.append(time.time() - t0)\n",
    "        avg_train_loss = run_loss / b\n",
    "\n",
    "        # ---------- VALID ----------\n",
    "        metrics   = eval_fn(model, val_loader, device)\n",
    "        val_rmse  = metrics[\"RMSE\"]\n",
    "        val_nmse  = metrics[\"NMSE\"]\n",
    "        val_nmse_db = 10 * torch.log10(torch.tensor(val_nmse)).item()\n",
    "\n",
    "        # save best checkpoint\n",
    "        if val_nmse < best_nmse:\n",
    "            best_nmse = val_nmse\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                CKPT_DIR / f\"{model_name}_best.pth\"\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            f\"[{epoch:02d}/{NUM_EPOCHS}] \"\n",
    "            f\"TrainLoss: {avg_train_loss:.4f}  \"\n",
    "            f\"Val RMSE: {val_rmse:.4f}  \"\n",
    "            f\"Val NMSE: {val_nmse:.4e}  \"\n",
    "            f\"Val NMSE_dB: {val_nmse_db:.1f} dB  \"\n",
    "            f\"TrainTime: {epoch_times[-1]:.2f}s\"\n",
    "        )\n",
    "\n",
    "    # after all epochs – save *last* weights\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        CKPT_DIR / f\"{model_name}_last.pth\"\n",
    "    )\n",
    "\n",
    "    avg_ep_time = sum(epoch_times) / len(epoch_times)\n",
    "    print(f\"🕒 {model_name} – avg train time / epoch: {avg_ep_time:.2f}s\")\n",
    "\n",
    "    # store best NMSE_dB for the summary\n",
    "    results[model_name] = 10 * math.log10(best_nmse)\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 2) Summary\n",
    "# ─────────────────────────────────────────────\n",
    "print(\"\\n=== Summary of best NMSE(dB) by model ===\")\n",
    "for name, nmse_db in results.items():\n",
    "    print(f\"{name:25s}: {nmse_db if not math.isnan(nmse_db) else 'skipped':>6}\")\n",
    "\n",
    "print(f\"\\nTotal training time for all models: {time.time() - total_start:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e41b5c9-77dd-4300-ac44-925dd560ccfd",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac929b23-8f2c-4b46-b746-d450e45bbe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 0)  Load the *best* checkpoints into `trained_models`\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "CKPT_DIR = Path(\"checkpoints\")                 # folder with *.pth files\n",
    "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "trained_models = {}\n",
    "for name, ModelCls in MODEL_CATALOG.items():\n",
    "    ckpt_path = CKPT_DIR / f\"{name}_best.pth\"\n",
    "    if ckpt_path.exists():\n",
    "        model = ModelCls(**MODEL_PARAMS[name])\n",
    "        model.load_state_dict(torch.load(ckpt_path, map_location=\"cpu\"))\n",
    "        trained_models[name] = model          # keep on CPU for now\n",
    "    else:\n",
    "        print(f\"⚠️  {ckpt_path} not found — skipping this model.\")\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 1)  Pure-inference timing loop (no loss / labels)\n",
    "# ─────────────────────────────────────────────\n",
    "torch.backends.cudnn.benchmark = True          # let cuDNN pick fastest kernels\n",
    "INFER_TIME = {}                                # {model: (total, per_batch, per_sample)}\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    uses_mask = name.startswith(\"LWM_\")\n",
    "    v_loader  = masked_val_loader if uses_mask else unmasked_val_loader\n",
    "\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    # ― Warm-up one batch to ramp GPU clocks and cache kernels\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(v_loader))\n",
    "        if uses_mask:\n",
    "            seq, mpos, _ = [x.to(device) for x in batch]\n",
    "            _ = model(seq, mpos)\n",
    "        else:\n",
    "            seq, _ = [x.to(device) for x in batch]\n",
    "            _ = model(seq)\n",
    "\n",
    "    # ― Timed inference pass over the entire loader\n",
    "    torch.cuda.synchronize()\n",
    "    t0        = time.time()\n",
    "    n_batches = 0\n",
    "    n_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in v_loader:\n",
    "            if uses_mask:\n",
    "                seq, mpos, _ = [x.to(device) for x in batch]\n",
    "                _  = model(seq, mpos)\n",
    "                bs = seq.size(0)\n",
    "            else:\n",
    "                seq, _ = [x.to(device) for x in batch]\n",
    "                _  = model(seq)\n",
    "                bs = seq.size(0)\n",
    "\n",
    "            n_batches += 1\n",
    "            n_samples += bs\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    INFER_TIME[name] = (\n",
    "        elapsed,                # total seconds\n",
    "        elapsed / n_batches,    # seconds per batch\n",
    "        elapsed / n_samples     # seconds per sample\n",
    "    )\n",
    "\n",
    "    print(f\"⏱ {name:25s} | total {elapsed:6.2f}s  \"\n",
    "          f\"| /batch {elapsed/n_batches*1e3:6.2f} ms  \"\n",
    "          f\"| /sample {elapsed/n_samples*1e3:6.2f} ms\")\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 2)  Pretty summary table\n",
    "# ─────────────────────────────────────────────\n",
    "print(\"\\n=== Inference-time summary ===\")\n",
    "header = f\"{'model':25s} | {'total [s]':>9} | {'/batch [ms]':>12} | {'/sample [ms]':>13}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "for n, (tot, pb, ps) in INFER_TIME.items():\n",
    "    print(f\"{n:25s} | {tot:9.2f} | {pb*1e3:12.2f} | {ps*1e3:13.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b547a829-25e4-446b-a962-6da55cc44176",
   "metadata": {},
   "source": [
    "## Compare trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a90966-4bb9-40c9-a8a6-6d2435ceec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Report trainable parameters for every model\n",
    "# ─────────────────────────────────────────────\n",
    "print(\"\\n=== Trainable parameters per model ===\")\n",
    "for name, ModelCls in MODEL_CATALOG.items():\n",
    "    # instantiate model with its params (on CPU is fine for counting)\n",
    "    model = ModelCls(**MODEL_PARAMS[name])\n",
    "    count = count_trainable_params(model)\n",
    "    print(f\"{name:25s}: {count:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff42f6d-3ed7-43b4-b35e-16e0d7345bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f748d3-0edc-4e77-bbf6-be96d7d8d99e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a224fa-03a3-4c41-989a-47e2327c5c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bea2d4-1027-495f-a2e3-efbc703aff5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dd53c6-5322-4744-95d6-90cbe8da1d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9f299d-07db-4e49-aac5-ebddf12c7538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
