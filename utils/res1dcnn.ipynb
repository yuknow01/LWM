{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "from torch.optim import Adam\n", "from torch.optim.lr_scheduler import MultiStepLR"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class ResidualBlock(nn.Module):\n", "    def __init__(self, in_channels, out_channels):\n", "        super(ResidualBlock, self).__init__()\n", "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n", "        self.bn1 = nn.BatchNorm1d(out_channels)\n", "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n", "        self.bn2 = nn.BatchNorm1d(out_channels)\n\n", "        # Shortcut connection to match dimensions when needed\n", "        self.shortcut = nn.Sequential()\n", "        if in_channels != out_channels:\n", "            self.shortcut = nn.Sequential(\n", "                nn.Conv1d(in_channels, out_channels, kernel_size=1),\n", "                nn.BatchNorm1d(out_channels)\n", "            )\n", "    \n", "    def forward(self, x):\n", "        residual = x\n", "        x = F.relu(self.bn1(self.conv1(x)))\n", "        x = self.bn2(self.conv2(x))\n", "        x += self.shortcut(residual)\n", "        x = F.relu(x)\n", "        return x"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class ResNet1DCNN(nn.Module):\n", "    def __init__(self, input_channels, sequence_length, num_classes):\n", "        super(ResNet1DCNN, self).__init__()\n", "        \n", "        # Initial convolution layer\n", "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=7, stride=2, padding=3)\n", "        self.bn1 = nn.BatchNorm1d(32)\n", "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n", "        \n", "        # Residual layers\n", "        self.layer1 = self._make_layer(32, 32, 2)\n", "        self.layer2 = self._make_layer(32, 64, 3)\n", "        self.layer3 = self._make_layer(64, 128, 4)\n", "        \n", "        # Calculate the size of the flattened features\n", "        with torch.no_grad():\n", "            dummy_input = torch.zeros(1, input_channels, sequence_length)\n", "            dummy_output = self.compute_conv_output(dummy_input)\n", "            self.flatten_size = dummy_output.numel()\n", "        \n", "        # Fully connected layers\n", "        self.fc1 = nn.Linear(self.flatten_size, 128)\n", "        self.bn_fc1 = nn.BatchNorm1d(128)\n", "        self.fc2 = nn.Linear(128, num_classes)\n", "        \n", "        self.dropout = nn.Dropout(0.5)\n", "        \n", "    def _make_layer(self, in_channels, out_channels, num_blocks):\n", "        layers = [ResidualBlock(in_channels, out_channels)]\n", "        for _ in range(1, num_blocks):\n", "            layers.append(ResidualBlock(out_channels, out_channels))\n", "        return nn.Sequential(*layers)\n", "    \n", "    def compute_conv_output(self, x):\n", "        x = self.maxpool(F.relu(self.bn1(self.conv1(x))))\n", "        x = self.layer1(x)\n", "        x = self.layer2(x)\n", "        x = self.layer3(x)\n", "        x = F.adaptive_avg_pool1d(x, 1)\n", "        return x\n", "    \n", "    def forward(self, x):\n", "        \n", "        x = x.transpose(1, 2)\n", "        \n", "        x = self.compute_conv_output(x)\n", "        \n", "        x = x.view(x.size(0), -1)\n", "        x = F.relu(self.bn_fc1(self.fc1(x)))\n", "        x = self.dropout(x)\n", "        x = self.fc2(x)\n", "        \n", "        return x"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}